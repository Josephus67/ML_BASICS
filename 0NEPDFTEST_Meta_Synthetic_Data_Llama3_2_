{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"0365c9b5e9a44318a70109069135b7fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0cf2ea32d7e42d2b6cb00ba5b128e6d","placeholder":"​","style":"IPY_MODEL_5856dc5484e54deaae2c0ea0703885cb","value":"model.safetensors: 100%"}},"079705095aa14fa0bd5af53459ad5bfb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e5ffbbd4d954281af45f8d0f2488de7","placeholder":"​","style":"IPY_MODEL_6751a852357447259e610430fdbb444b","value":" 35/35 [00:00&lt;00:00, 435.88 examples/s]"}},"09b1ea46e7d4437bac1539aeb1a47fbe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ecfbc357537f493e83f9a84606581f4c","IPY_MODEL_425f26b7325541569144cd46eaefcdc0","IPY_MODEL_1739bb52ba624c67804927dd372649e7"],"layout":"IPY_MODEL_33262ee83efa4523b434a4c456067d18"}},"0d66ab586e7f4f588b3e70bb0d6191a3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1082d64463a94f6282c1754720c702cd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"122c08b183f0425f881acef17f5daf08":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"137ca939b43b4bc7b41da9005e28b646":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14b6b0568a074f8eb20cb57ffbcbb0f3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"15c3082a57e04b0198a8232f135290ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a87d29f844c449119a95f5548c0af0c7","placeholder":"​","style":"IPY_MODEL_abe93b4cbe524be08d08ff75f22d0c08","value":"generation_config.json: 100%"}},"1739bb52ba624c67804927dd372649e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed0ba5d3437b492dbdb9980ae677c35c","placeholder":"​","style":"IPY_MODEL_122c08b183f0425f881acef17f5daf08","value":" 54.7k/54.7k [00:00&lt;00:00, 5.38MB/s]"}},"1a155e85dd674f468d8b240481413722":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1b032fc19dfa46a5add673aa9c6aeda5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d95662d1741434581d501e83b99c778":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1e5ffbbd4d954281af45f8d0f2488de7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f65e57e365e47eeb83cb259efbd698b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21ba911bfeb2439aa0ca135c299de1ba":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27be088be2de4517964a8e5217641fd9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2b15c1d19cf84911bee54a6d113eed1a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d0a363ce9c54665811b8cd0a8fdb42b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2eee87d6fc0445038e9ccf896cfa3d77":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f5974e37f7847efa368f06b96ae9481","max":454,"min":0,"orientation":"horizontal","style":"IPY_MODEL_97ff30809ee4484988b6d0de7acd5c04","value":454}},"2f5974e37f7847efa368f06b96ae9481":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f822636140c45e8a9713b711d057005":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f88451af2cd48aa93da32431e5de139":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_47d0de835e7d48e39161a7f038167653","placeholder":"​","style":"IPY_MODEL_6956008e9def426b82617536b015dd38","value":" 17.2M/17.2M [00:00&lt;00:00, 34.6MB/s]"}},"2fb3b417c2404e8a9ad83b1a4114174c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"315f16beade4418390ad511458013b81":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33262ee83efa4523b434a4c456067d18":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"390ed406aa524d0c91e11ef74d01d700":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3ca561741e5f4b59b796b074cac64d80":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_15c3082a57e04b0198a8232f135290ef","IPY_MODEL_7cd6c8ef7ec9451c90560208277975c5","IPY_MODEL_f631738fd3014d13a7e6a50a66eaf92a"],"layout":"IPY_MODEL_f8b4855c3f0146dd8d3d0876f3a12c08"}},"3fdc25451ed543d288d89df799883b3c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_21ba911bfeb2439aa0ca135c299de1ba","max":35,"min":0,"orientation":"horizontal","style":"IPY_MODEL_91ae4592c2d545bf9df31983257b6907","value":35}},"425f26b7325541569144cd46eaefcdc0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f246d9fe6e9415596645f736027d246","max":54670,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1d95662d1741434581d501e83b99c778","value":54670}},"442f7ccc30314a58b1aabc91cf4dbe48":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47d0de835e7d48e39161a7f038167653":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4952048f852a4593af48573a163a052f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a6e9f1533f83416c8b512f31b099254c","IPY_MODEL_937a90e4c4dc41b195d24fa1694c9157","IPY_MODEL_b82aae8041fa462d88467a02fa252af7"],"layout":"IPY_MODEL_e7d6be17a7bb4d5e8b248d7f5dee9647"}},"50066adf2c2d4f6abe16456cb3cee737":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"50f52e4c2e5e4513b0d1fb23c39657f8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5454423e6291484bb63b310699f5d645":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7e23ee6479ee484e90b93c572848a2b8","IPY_MODEL_cea072caa7694eeb8b858ea0624e88fa","IPY_MODEL_812390f2da734ba9b516af73ddb8f452"],"layout":"IPY_MODEL_f83ddfe2510447d0a6c08b109cf42dbf"}},"55e82538888940c1b754fa02d29e965c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5738348f2e70424babbd91c44680b87a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5759ecdb02714a9f82892e12c6b9ae46":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f229d574614945029666403a10bb761e","max":2354805470,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b6da3300ba7e42888add16741bafd614","value":2354805470}},"5856dc5484e54deaae2c0ea0703885cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5c022f8ebf4c44baa2195ca706b687e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0c12a5f8a9a4a7c8a5584b54acb6adb","placeholder":"​","style":"IPY_MODEL_5738348f2e70424babbd91c44680b87a","value":" 454/454 [00:00&lt;00:00, 34.4kB/s]"}},"5d6cb85f619b439db3544414b732607c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60c5e826831c4afbb869a1429d3a372e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_137ca939b43b4bc7b41da9005e28b646","placeholder":"​","style":"IPY_MODEL_390ed406aa524d0c91e11ef74d01d700","value":"Map: 100%"}},"62e6190f733844319221ce6eb7a7928c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_acd616b0a9574855a3d4525e72a6513a","IPY_MODEL_9b27fcca9ec64b81952560f38080abc6","IPY_MODEL_2f88451af2cd48aa93da32431e5de139"],"layout":"IPY_MODEL_55e82538888940c1b754fa02d29e965c"}},"644e720f44d24d159133bfa515482054":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6751a852357447259e610430fdbb444b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6956008e9def426b82617536b015dd38":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6b9895bccc364862a39b1c70d9612ba5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4c156d84ae041339c5fef3c9ce6ba30","placeholder":"​","style":"IPY_MODEL_6b9d7dd7465a4fbfbfbca7cc5dfc2c23","value":"special_tokens_map.json: 100%"}},"6b9d7dd7465a4fbfbfbca7cc5dfc2c23":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"734ba48df6c34b8fb20e92f99c4e4aa2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d66ab586e7f4f588b3e70bb0d6191a3","max":890,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d527203d8b414db08f40dd6be3235c5c","value":890}},"73d0263833c147fc99cc5be1ce5357e9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"74806db2db6e4085a28dceca4fbeee28":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"772f1cc7afea484bbb3cb1cc6aa7576f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ba927b6ec4e41c39cb445443c301c57":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_81c79f2262d04a8aba854e679f5e1dac","IPY_MODEL_734ba48df6c34b8fb20e92f99c4e4aa2","IPY_MODEL_fde296d28c5d4c61b384a4553b499d69"],"layout":"IPY_MODEL_442f7ccc30314a58b1aabc91cf4dbe48"}},"7be62d010b564972924fea1d8343398c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c05acca6aed46d49663afe10ebf6d9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_644e720f44d24d159133bfa515482054","placeholder":"​","style":"IPY_MODEL_50066adf2c2d4f6abe16456cb3cee737","value":" 35/35 [00:02&lt;00:00, 19.01 examples/s]"}},"7cd6c8ef7ec9451c90560208277975c5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f60d2b4aa054b82bcef0b49cd4014f3","max":234,"min":0,"orientation":"horizontal","style":"IPY_MODEL_27be088be2de4517964a8e5217641fd9","value":234}},"7e23ee6479ee484e90b93c572848a2b8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cab7a4bf98cf42c69f81b9c4b7b4b21d","placeholder":"​","style":"IPY_MODEL_1a155e85dd674f468d8b240481413722","value":"tokenizer.json: 100%"}},"7f246d9fe6e9415596645f736027d246":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f616bf83de54cfeaabda52b3f592e4c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"812390f2da734ba9b516af73ddb8f452":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf983c3e807d4e25ba8c2a079aea34bf","placeholder":"​","style":"IPY_MODEL_315f16beade4418390ad511458013b81","value":" 17.2M/17.2M [00:00&lt;00:00, 23.6MB/s]"}},"81c79f2262d04a8aba854e679f5e1dac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f65e57e365e47eeb83cb259efbd698b","placeholder":"​","style":"IPY_MODEL_772f1cc7afea484bbb3cb1cc6aa7576f","value":"config.json: 100%"}},"834de6e5d5504ba794c43d186d6de8a7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83657ceb17f1454ca7e33a5fcc5ad5f7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bbc163a35ec54087bf0c56cc332df0b4","placeholder":"​","style":"IPY_MODEL_c901f4517b474511b6dd880e63f25b58","value":"tokenizer_config.json: 100%"}},"85ff0a11606e466ebd7f7b3b6ed3048e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d6cb85f619b439db3544414b732607c","placeholder":"​","style":"IPY_MODEL_9f083cc2ddb64b6196cc2d170affabae","value":" 2.35G/2.35G [02:28&lt;00:00, 48.6MB/s]"}},"8c6845e9bd81418e8c02d66c32046129":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d38463677324bd79c193798157a1568":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e20e88da0f34b6fadf118a0443674c6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91ae4592c2d545bf9df31983257b6907":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"937a90e4c4dc41b195d24fa1694c9157":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c6845e9bd81418e8c02d66c32046129","max":454,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2fb3b417c2404e8a9ad83b1a4114174c","value":454}},"943c7280c19948999aa663ca6736d698":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"97ff30809ee4484988b6d0de7acd5c04":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9a18c25e8d694fff905917e7ce2931dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9a62a83412df4453840f06c97ebcc216":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6b9895bccc364862a39b1c70d9612ba5","IPY_MODEL_2eee87d6fc0445038e9ccf896cfa3d77","IPY_MODEL_5c022f8ebf4c44baa2195ca706b687e7"],"layout":"IPY_MODEL_834de6e5d5504ba794c43d186d6de8a7"}},"9b27fcca9ec64b81952560f38080abc6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3ab144489504058b7c9009d867bef95","max":17209920,"min":0,"orientation":"horizontal","style":"IPY_MODEL_943c7280c19948999aa663ca6736d698","value":17209920}},"9f083cc2ddb64b6196cc2d170affabae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f60d2b4aa054b82bcef0b49cd4014f3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6e9f1533f83416c8b512f31b099254c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc915d911ee144bea96b1885f5eb7191","placeholder":"​","style":"IPY_MODEL_caed1189f10140549fe2b445dd7dbd16","value":"special_tokens_map.json: 100%"}},"a87d29f844c449119a95f5548c0af0c7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab2bdd97242243d7aadfe932c72c39a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"abe93b4cbe524be08d08ff75f22d0c08":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"acd616b0a9574855a3d4525e72a6513a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b15c1d19cf84911bee54a6d113eed1a","placeholder":"​","style":"IPY_MODEL_cfdfb65ccad04899881976d93945bf0d","value":"tokenizer.json: 100%"}},"b0c12a5f8a9a4a7c8a5584b54acb6adb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b554950c61ae436dbb7772e7de173a81":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6da3300ba7e42888add16741bafd614":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b82aae8041fa462d88467a02fa252af7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e22b473dd97446298b42987e36d945ec","placeholder":"​","style":"IPY_MODEL_9a18c25e8d694fff905917e7ce2931dd","value":" 454/454 [00:00&lt;00:00, 29.8kB/s]"}},"bbc163a35ec54087bf0c56cc332df0b4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf983c3e807d4e25ba8c2a079aea34bf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0cf2ea32d7e42d2b6cb00ba5b128e6d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6f86936f86347819002d6bbf50ec484":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_74806db2db6e4085a28dceca4fbeee28","max":54674,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e2f989df5f494351a5b242d2430c2917","value":54674}},"c7787082fecf4df0ae58e67f46e42589":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_60c5e826831c4afbb869a1429d3a372e","IPY_MODEL_3fdc25451ed543d288d89df799883b3c","IPY_MODEL_079705095aa14fa0bd5af53459ad5bfb"],"layout":"IPY_MODEL_1082d64463a94f6282c1754720c702cd"}},"c901f4517b474511b6dd880e63f25b58":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cab7a4bf98cf42c69f81b9c4b7b4b21d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cae045b72aca476a9c16c39778be53a0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"caed1189f10140549fe2b445dd7dbd16":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd3f2a8158124c6b87be72e959a455d5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cea072caa7694eeb8b858ea0624e88fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b032fc19dfa46a5add673aa9c6aeda5","max":17209920,"min":0,"orientation":"horizontal","style":"IPY_MODEL_df624458fe5a4a21b58602af80fa0a1c","value":17209920}},"cf6e3f6267cb49d5ac4575d1d1e41d10":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef715f56f3824a64949be1f2bb05296b","placeholder":"​","style":"IPY_MODEL_cd3f2a8158124c6b87be72e959a455d5","value":" 54.7k/54.7k [00:00&lt;00:00, 5.79MB/s]"}},"cfd9f69db23f4b86a42179ac0ac91a8c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e624589b592d43239e812e02fd3ab244","IPY_MODEL_e0311985f098497e821f7c8720457384","IPY_MODEL_7c05acca6aed46d49663afe10ebf6d9b"],"layout":"IPY_MODEL_8d38463677324bd79c193798157a1568"}},"cfdfb65ccad04899881976d93945bf0d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d527203d8b414db08f40dd6be3235c5c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d5500717a6534c768be6a07913b6a339":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d944a4b11b614896bdb6af583faed134":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_83657ceb17f1454ca7e33a5fcc5ad5f7","IPY_MODEL_c6f86936f86347819002d6bbf50ec484","IPY_MODEL_cf6e3f6267cb49d5ac4575d1d1e41d10"],"layout":"IPY_MODEL_2d0a363ce9c54665811b8cd0a8fdb42b"}},"df624458fe5a4a21b58602af80fa0a1c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e0311985f098497e821f7c8720457384":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7be62d010b564972924fea1d8343398c","max":35,"min":0,"orientation":"horizontal","style":"IPY_MODEL_14b6b0568a074f8eb20cb57ffbcbb0f3","value":35}},"e22b473dd97446298b42987e36d945ec":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2f989df5f494351a5b242d2430c2917":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e3ab144489504058b7c9009d867bef95":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e624589b592d43239e812e02fd3ab244":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b554950c61ae436dbb7772e7de173a81","placeholder":"​","style":"IPY_MODEL_ab2bdd97242243d7aadfe932c72c39a2","value":"Unsloth: Tokenizing [&quot;text&quot;] (num_proc=2): 100%"}},"e7d6be17a7bb4d5e8b248d7f5dee9647":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ecfbc357537f493e83f9a84606581f4c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e20e88da0f34b6fadf118a0443674c6","placeholder":"​","style":"IPY_MODEL_d5500717a6534c768be6a07913b6a339","value":"tokenizer_config.json: 100%"}},"ed0ba5d3437b492dbdb9980ae677c35c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef715f56f3824a64949be1f2bb05296b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f229d574614945029666403a10bb761e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4c156d84ae041339c5fef3c9ce6ba30":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f631738fd3014d13a7e6a50a66eaf92a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f616bf83de54cfeaabda52b3f592e4c","placeholder":"​","style":"IPY_MODEL_2f822636140c45e8a9713b711d057005","value":" 234/234 [00:00&lt;00:00, 25.2kB/s]"}},"f83ddfe2510447d0a6c08b109cf42dbf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8b4855c3f0146dd8d3d0876f3a12c08":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc915d911ee144bea96b1885f5eb7191":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc9c4d66a2fa41a393525a04ced011f3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0365c9b5e9a44318a70109069135b7fe","IPY_MODEL_5759ecdb02714a9f82892e12c6b9ae46","IPY_MODEL_85ff0a11606e466ebd7f7b3b6ed3048e"],"layout":"IPY_MODEL_cae045b72aca476a9c16c39778be53a0"}},"fde296d28c5d4c61b384a4553b499d69":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_50f52e4c2e5e4513b0d1fb23c39657f8","placeholder":"​","style":"IPY_MODEL_73d0263833c147fc99cc5be1ce5357e9","value":" 890/890 [00:00&lt;00:00, 95.0kB/s]"}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13673006,"sourceType":"datasetVersion","datasetId":8693925},{"sourceId":13678110,"sourceType":"datasetVersion","datasetId":8697885},{"sourceId":13678209,"sourceType":"datasetVersion","datasetId":8697957}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n\n\n<a href=\"https://github.com/meta-llama/synthetic-data-kit\"><img src=\"https://raw.githubusercontent.com/unslothai/notebooks/refs/heads/main/assets/meta%20round%20logo.png\" width=\"137\"></a>\n<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n</div>\n\nTo install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n\nYou will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n","metadata":{"id":"Ja-zG68pMRmH"}},{"cell_type":"markdown","source":"### News","metadata":{"id":"Om2qjxs5PSr0"}},{"cell_type":"markdown","source":"\nUnsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n\n[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n\nIntroducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n\nUnsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n\nVisit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n","metadata":{"id":"RCa86oMuPSr0"}},{"cell_type":"markdown","source":"### Installation","metadata":{"id":"imwJhjjYPSr0"}},{"cell_type":"code","source":"%%capture\nimport os\n!pip install --upgrade -qqq uv\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    # If you're not in Colab, just use pip install!\n    !pip install unsloth vllm synthetic-data-kit==0.0.3\nelse:\n    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n    try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n    except: is_t4 = False\n    get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n    !uv pip install -qqq --upgrade         unsloth {get_vllm} {get_numpy} {get_pil} torchvision bitsandbytes xformers\n    !uv pip install -qqq {get_triton}\n    !uv pip install synthetic-data-kit==0.0.3\n!uv pip install transformers==4.56.2\n!uv pip install --no-deps trl==0.22.2","metadata":{"id":"_bPWUW0VMRmN","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T10:32:27.661965Z","iopub.execute_input":"2025-11-10T10:32:27.662520Z","iopub.status.idle":"2025-11-10T10:33:48.101816Z","shell.execute_reply.started":"2025-11-10T10:32:27.662494Z","shell.execute_reply":"2025-11-10T10:33:48.101008Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"#@title Colab Extra Install { display-mode: \"form\" }\nimport os\n!pip install --upgrade -qqq uv\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    # If you're not in Colab, just use pip install!\n    !pip install unsloth vllm\nelse:\n    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n    try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n    except: is_t4 = False\n    get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n    !uv pip install -qqq --upgrade \\\n        unsloth {get_vllm} {get_numpy} {get_pil} torchvision bitsandbytes xformers\n    !uv pip install -qqq {get_triton}\n!uv pip install transformers==4.56.2\n!uv pip install --no-deps trl==0.22.2","metadata":{"id":"N9cMHGGeMRmO","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T10:33:48.103189Z","iopub.execute_input":"2025-11-10T10:33:48.103466Z","iopub.status.idle":"2025-11-10T10:33:53.612755Z","shell.execute_reply.started":"2025-11-10T10:33:48.103427Z","shell.execute_reply":"2025-11-10T10:33:53.612043Z"}},"outputs":[{"name":"stdout","text":"\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m31 packages\u001b[0m \u001b[2min 17ms\u001b[0m\u001b[0m                                         \u001b[0m\n\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 86ms\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 78ms\u001b[0m\u001b[0m                                 \u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.56.2\u001b[0m\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m                                            \u001b[0m\n\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 8ms\u001b[0m\u001b[0m                                  \u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtrl\u001b[0m\u001b[2m==0.23.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtrl\u001b[0m\u001b[2m==0.22.2\u001b[0m\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### Unsloth","metadata":{"id":"YuJT3lyHMRmP"}},{"cell_type":"markdown","source":"## Primary Goal\nOur goal is to make Llama 3.2 3B understand the \"Byte Latent Transformer: Patches Scale Better Than Tokens\" [research paper](https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/) that was published in December 2024.\n\nWe'll use https://github.com/meta-llama/synthetic-data-kit to generate question and answer pairs **fully locally** which will be used for finetuning Llama 3.2 3B!","metadata":{"id":"O5drUeToMRmQ"}},{"cell_type":"code","source":"from unsloth.dataprep import SyntheticDataKit\n\ngenerator = SyntheticDataKit.from_pretrained(\n    # Choose any model from https://huggingface.co/unsloth\n    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n    max_seq_length = 2048, # Longer sequence lengths will be slower!\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":937,"referenced_widgets":["7ba927b6ec4e41c39cb445443c301c57","81c79f2262d04a8aba854e679f5e1dac","734ba48df6c34b8fb20e92f99c4e4aa2","fde296d28c5d4c61b384a4553b499d69","442f7ccc30314a58b1aabc91cf4dbe48","1f65e57e365e47eeb83cb259efbd698b","772f1cc7afea484bbb3cb1cc6aa7576f","0d66ab586e7f4f588b3e70bb0d6191a3","d527203d8b414db08f40dd6be3235c5c","50f52e4c2e5e4513b0d1fb23c39657f8","73d0263833c147fc99cc5be1ce5357e9","09b1ea46e7d4437bac1539aeb1a47fbe","ecfbc357537f493e83f9a84606581f4c","425f26b7325541569144cd46eaefcdc0","1739bb52ba624c67804927dd372649e7","33262ee83efa4523b434a4c456067d18","8e20e88da0f34b6fadf118a0443674c6","d5500717a6534c768be6a07913b6a339","7f246d9fe6e9415596645f736027d246","1d95662d1741434581d501e83b99c778","ed0ba5d3437b492dbdb9980ae677c35c","122c08b183f0425f881acef17f5daf08","5454423e6291484bb63b310699f5d645","7e23ee6479ee484e90b93c572848a2b8","cea072caa7694eeb8b858ea0624e88fa","812390f2da734ba9b516af73ddb8f452","f83ddfe2510447d0a6c08b109cf42dbf","cab7a4bf98cf42c69f81b9c4b7b4b21d","1a155e85dd674f468d8b240481413722","1b032fc19dfa46a5add673aa9c6aeda5","df624458fe5a4a21b58602af80fa0a1c","bf983c3e807d4e25ba8c2a079aea34bf","315f16beade4418390ad511458013b81","9a62a83412df4453840f06c97ebcc216","6b9895bccc364862a39b1c70d9612ba5","2eee87d6fc0445038e9ccf896cfa3d77","5c022f8ebf4c44baa2195ca706b687e7","834de6e5d5504ba794c43d186d6de8a7","f4c156d84ae041339c5fef3c9ce6ba30","6b9d7dd7465a4fbfbfbca7cc5dfc2c23","2f5974e37f7847efa368f06b96ae9481","97ff30809ee4484988b6d0de7acd5c04","b0c12a5f8a9a4a7c8a5584b54acb6adb","5738348f2e70424babbd91c44680b87a"]},"id":"Ym8UA1PRiXsa","outputId":"1dc83bf1-adf6-47d8-fdea-6a4c2f26ffb2","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T10:33:53.613738Z","iopub.execute_input":"2025-11-10T10:33:53.614042Z","iopub.status.idle":"2025-11-10T10:37:00.601277Z","shell.execute_reply.started":"2025-11-10T10:33:53.614016Z","shell.execute_reply":"2025-11-10T10:37:00.600551Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-11-10 10:34:03.568431: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762770843.738193      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762770843.791657      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"INFO 11-10 10:34:30 [__init__.py:244] Automatically detected platform cuda.\nERROR 11-10 10:34:32 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n🦥 Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/890 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9aaef711af524bfd8a74ec3a54bf2626"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04cbde4d76ba4f309e2fc2362d86103f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39de279b7f1c4eeda080543684857413"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5ad1fe80d6e46159d868b1b54a0b363"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ed19eecd14f413b85abf0254e077452"}},"metadata":{}},{"name":"stdout","text":"INFO 11-10 10:34:45 [vllm_utils.py:700] Unsloth: Patching vLLM v1 graph capture\nINFO 11-10 10:34:45 [vllm_utils.py:730] Unsloth: Patching vLLM v0 graph capture\nUnsloth: Using dtype = torch.float16 for vLLM.\nUnsloth: vLLM loading unsloth/Llama-3.2-3B-Instruct with actual GPU utilization = 89.39%\nUnsloth: Your GPU has CUDA compute capability 7.5 with VRAM = 14.74 GB.\nUnsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 192.\nUnsloth: vLLM's KV Cache can use up to 7.19 GB. Also swap space = 4 GB.\nUnsloth: `cudagraph_mode` is not in `from vllm.config import CompilationConfig`\nUnsloth: Disabling `disable_cascade_attn` in vLLM to allow for better on policy RL!\nvLLM STDOUT: INFO 11-10 10:34:57 [__init__.py:244] Automatically detected platform cuda.\nvLLM STDOUT: INFO 11-10 10:34:59 [api_server.py:1395] vLLM API server version 0.9.2\nvLLM STDOUT: INFO 11-10 10:34:59 [cli_args.py:325] non-default args: {'model': 'unsloth/Llama-3.2-3B-Instruct', 'dtype': 'float16', 'seed': 0, 'max_model_len': 2048, 'max_logprobs': 0, 'disable_cascade_attn': True, 'gpu_memory_utilization': 0.8938626454842437, 'enable_prefix_caching': True, 'max_num_batched_tokens': 2048, 'max_num_seqs': 192, 'enable_chunked_prefill': True, 'disable_log_stats': True}\nvLLM STDOUT: INFO 11-10 10:35:14 [config.py:841] This model supports multiple tasks: {'generate', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.\nvLLM STDOUT: WARNING 11-10 10:35:14 [config.py:3371] Casting torch.bfloat16 to torch.float16.\nvLLM STDOUT: INFO 11-10 10:35:14 [config.py:1472] Using max model len 2048\nvLLM STDOUT: WARNING 11-10 10:35:14 [arg_utils.py:1735] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \nvLLM STDOUT: INFO 11-10 10:35:16 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=2048.\nvLLM STDOUT: INFO 11-10 10:35:16 [api_server.py:268] Started engine process with PID 394\nvLLM STDOUT: INFO 11-10 10:35:28 [__init__.py:244] Automatically detected platform cuda.\nvLLM STDOUT: INFO 11-10 10:35:30 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.2) with config: model='unsloth/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='unsloth/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":192,\"local_cache_dir\":null}, use_cached_outputs=True, \nvLLM STDOUT: INFO 11-10 10:35:31 [cuda.py:311] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nvLLM STDOUT: INFO 11-10 10:35:31 [cuda.py:360] Using XFormers backend.\nvLLM STDOUT: INFO 11-10 10:35:51 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\nvLLM STDOUT: INFO 11-10 10:35:51 [model_runner.py:1171] Starting to load model unsloth/Llama-3.2-3B-Instruct...\nvLLM STDOUT: INFO 11-10 10:35:52 [weight_utils.py:292] Using model weights format ['*.safetensors']\nvLLM STDOUT: INFO 11-10 10:36:14 [weight_utils.py:308] Time spent downloading weights for unsloth/Llama-3.2-3B-Instruct: 21.445562 seconds\nvLLM STDOUT: INFO 11-10 10:36:26 [default_loader.py:272] Loading weights took 12.65 seconds\nvLLM STDOUT: INFO 11-10 10:36:27 [model_runner.py:1203] Model loading took 6.0160 GiB and 34.565849 seconds\nvLLM STDOUT: INFO 11-10 10:36:29 [worker.py:294] Memory profiling takes 1.47 seconds\nvLLM STDOUT: INFO 11-10 10:36:29 [worker.py:294] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.89) = 13.18GiB\nvLLM STDOUT: INFO 11-10 10:36:29 [worker.py:294] model weights take 6.02GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 0.90GiB; the rest of the memory reserved for KV Cache is 6.22GiB.\nvLLM STDOUT: INFO 11-10 10:36:29 [executor_base.py:113] # cuda blocks: 3637, # CPU blocks: 2340\nvLLM STDOUT: INFO 11-10 10:36:29 [executor_base.py:118] Maximum concurrency for 2048 tokens per request: 28.41x\nvLLM STDOUT: INFO 11-10 10:36:35 [model_runner.py:1513] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nvLLM STDOUT: INFO 11-10 10:36:59 [model_runner.py:1671] Graph capturing finished in 24 secs, took 0.15 GiB\nvLLM STDOUT: INFO 11-10 10:36:59 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 31.51 seconds\nvLLM STDOUT: WARNING 11-10 10:36:59 [config.py:1392] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\nvLLM STDOUT: INFO 11-10 10:36:59 [serving_chat.py:125] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\nvLLM STDOUT: INFO 11-10 10:36:59 [serving_completion.py:72] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\nvLLM STDOUT: INFO 11-10 10:36:59 [api_server.py:1457] Starting vLLM API server 0 on http://0.0.0.0:8000\nvLLM STDOUT: INFO 11-10 10:36:59 [launcher.py:29] Available routes are:\nvLLM STDOUT: INFO 11-10 10:36:59 [launcher.py:37] Route: /openapi.json, Methods: GET, HEAD\nvLLM STDOUT: INFO 11-10 10:36:59 [launcher.py:37] Route: /docs, Methods: GET, HEAD\nvLLM STDOUT: INFO 11-10 10:36:59 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: GET, HEAD\nvLLM STDOUT: INFO 11-10 10:36:59 [launcher.py:37] Route: /redoc, Methods: GET, HEAD\nvLLM Server Ready Detected\nvLLM STDOUT: INFO 11-10 10:36:59 [launcher.py:37] Route: /health, Methods: GET\nvLLM STDOUT: INFO 11-10 10:36:59 [launcher.py:37] Route: /load, Methods: GET\nvLLM STDOUT: INFO 11-10 10:36:59 [launcher.py:37] Route: /ping, Methods: POST\nvLLM STDOUT: INFO 11-10 10:36:59 [launcher.py:37] Route: /ping, Methods: GET\nvLLM STDOUT: INFO 11-10 10:36:59 [launcher.py:37] Route: /tokenize, Methods: POST\nvLLM STDOUT: INFO 11-10 10:36:59 [launcher.py:37] Route: /detokenize, Methods: POST\nvLLM STDOUT: INFO 11-10 10:36:59 [launcher.py:37] Route: /v1/models, Methods: GET\nvLLM STDOUT: INFO 11-10 10:36:59 [launcher.py:37] Route: /version, Methods: GET\nvLLM STDOUT: INFO 11-10 10:36:59 [launcher.py:37] Route: /v1/chat/completions, Methods: POST\nvLLM STDOUT: INFO 11-10 10:36:59 [launcher.py:37] Route: /v1/completions, Methods: POST\nvLLM STDOUT: INFO 11-10 10:36:59 [launcher.py:37] Route: /v1/embeddings, Methods: POST\nvLLM STDOUT: INFO 11-10 10:36:59 [launcher.py:37] Route: /pooling, Methods: POST\nvLLM STDOUT: INFO 11-10 10:36:59 [launcher.py:37] Route: /classify, Methods: POST\nvLLM STDOUT: INFO 11-10 10:36:59 [launcher.py:37] Route: /score, Methods: POST\nvLLM STDOUT: INFO 11-10 10:36:59 [launcher.py:37] Route: /v1/score, Methods: POST\nvLLM STDOUT: INFO 11-10 10:36:59 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST\nvLLM STDOUT: INFO 11-10 10:36:59 [launcher.py:37] Route: /v1/audio/translations, Methods: POST\nvLLM STDOUT: INFO 11-10 10:36:59 [launcher.py:37] Route: /rerank, Methods: POST\nvLLM STDOUT: INFO 11-10 10:36:59 [launcher.py:37] Route: /v1/rerank, Methods: POST\nvLLM STDOUT: INFO 11-10 10:36:59 [launcher.py:37] Route: /v2/rerank, Methods: POST\nvLLM STDOUT: INFO 11-10 10:36:59 [launcher.py:37] Route: /invocations, Methods: POST\nvLLM STDOUT: INFO 11-10 10:36:59 [launcher.py:37] Route: /metrics, Methods: GET\nvLLM STDOUT: INFO:     127.0.0.1:60950 - \"GET /metrics HTTP/1.1\" 200 OK\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Generate QA Pairs + Auto clean data\nWe now use synthetic data kit for question answer pair generation:","metadata":{"id":"Ef_MnK575tr2"}},{"cell_type":"code","source":"generator.prepare_qa_generation(\n    output_folder = \"data\", # Output location of synthetic data\n    temperature = 0.7, # Higher temp makes more diverse datases\n    top_p = 0.95,\n    overlap = 64, # Overlap portion during chunking\n    max_generation_tokens = 512, # Can increase for longer QA pairs\n)","metadata":{"id":"q487TNby-nwT","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T10:42:04.365496Z","iopub.execute_input":"2025-11-10T10:42:04.366530Z","iopub.status.idle":"2025-11-10T10:42:04.370945Z","shell.execute_reply.started":"2025-11-10T10:42:04.366497Z","shell.execute_reply":"2025-11-10T10:42:04.370306Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"Check if it succeeded:","metadata":{"id":"yV7DyufR51IN"}},{"cell_type":"code","source":"!synthetic-data-kit system-check","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C2gQZcr_Wp94","outputId":"6c1fc4cb-de58-4573-a8c4-7c7d05f4f07f","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T10:42:06.526646Z","iopub.execute_input":"2025-11-10T10:42:06.527401Z","iopub.status.idle":"2025-11-10T10:42:06.961193Z","shell.execute_reply.started":"2025-11-10T10:42:06.527346Z","shell.execute_reply":"2025-11-10T10:42:06.960239Z"}},"outputs":[{"name":"stdout","text":"vLLM STDOUT: INFO:     127.0.0.1:38624 - \"GET /v1/models HTTP/1.1\" 200 OK\n\u001b[?25l\u001b[32m VLLM server is running at \u001b[0m\u001b[4;94mhttp://localhost:8000/v1\u001b[0m\n\u001b[2KAvailable models: \u001b[1m{\u001b[0m\u001b[32m'object'\u001b[0m: \u001b[32m'list'\u001b[0m, \u001b[32m'data'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \n\u001b[32m'unsloth/Llama-3.2-3B-Instruct'\u001b[0m, \u001b[32m'object'\u001b[0m: \u001b[32m'model'\u001b[0m, \u001b[32m'created'\u001b[0m: \u001b[1;36m1762771326\u001b[0m, \n\u001b[32m'owned_by'\u001b[0m: \u001b[32m'vllm'\u001b[0m, \u001b[32m'root'\u001b[0m: \u001b[32m'unsloth/Llama-3.2-3B-Instruct'\u001b[0m, \u001b[32m'parent'\u001b[0m: \u001b[3;35mNone\u001b[0m, \n\u001b[32m'max_model_len'\u001b[0m: \u001b[1;36m2048\u001b[0m, \u001b[32m'permission'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \n\u001b[32m'modelperm-abd6ef10979d416597edd4be0b1c3e49'\u001b[0m, \u001b[32m'object'\u001b[0m: \u001b[32m'model_permission'\u001b[0m, \n\u001b[32m'created'\u001b[0m: \u001b[1;36m1762771326\u001b[0m, \u001b[32m'allow_create_engine'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'allow_sampling'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \n\u001b[32m'allow_logprobs'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'allow_search_indices'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'allow_view'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \n\u001b[32m'allow_fine_tuning'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'organization'\u001b[0m: \u001b[32m'*'\u001b[0m, \u001b[32m'group'\u001b[0m: \u001b[3;35mNone\u001b[0m, \u001b[32m'is_blocking'\u001b[0m: \n\u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n\u001b[2K\u001b[32m⠋\u001b[0m Checking VLLM server at http://localhost:8000/v1...\n\u001b[1A\u001b[2K","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Document Parsing (PDF, CSV, HTML etc.)\nNow, let's take the Byte Latent Transformer: Patches Scale Better Than Tokens research paper found at https://arxiv.org/abs/2412.09871 and covert it to Q&A pairs in order to finetune Llama 3.2!","metadata":{"id":"xdl7aPFK55M1"}},{"cell_type":"code","source":"# Byte Latent Transformer: Patches Scale Better Than Tokens paper in HTML format\n!synthetic-data-kit \\\n    -c synthetic_data_kit_config.yaml \\\n    ingest \"/kaggle/input/interview/Machine_Learning_Interview_Questions.pdf\"\n\n# Truncate document\nfilenames = generator.chunk_data(\"data/output/Machine_Learning_Interview_Questions.txt\")\nprint(len(filenames), filenames[:3])\nprint(len(filenames))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8BN1yrPGmANA","outputId":"18ad218b-7977-4daf-9662-ddfb32e9972e","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T10:45:28.385855Z","iopub.execute_input":"2025-11-10T10:45:28.386761Z","iopub.status.idle":"2025-11-10T10:45:29.838288Z","shell.execute_reply.started":"2025-11-10T10:45:28.386728Z","shell.execute_reply":"2025-11-10T10:45:29.837416Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K\u001b[32m⠙\u001b[0m Processing /kaggle/input/interview/Machine_Learning_Interview_Questions.pdf.....\n\u001b[1A\u001b[2K\u001b[32m Text successfully extracted to \u001b[0m\n\u001b[1;32mdata/output/Machine_Learning_Interview_Questions.txt\u001b[0m\n7 ['data/output/Machine_Learning_Interview_Questions_0.txt', 'data/output/Machine_Learning_Interview_Questions_1.txt', 'data/output/Machine_Learning_Interview_Questions_2.txt']\n7\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"We see around 37 chunks of data. We now call synthetic-data-kit to create some pairs of data for 3 of our chunks.\n\nYou can process more chunks, but it'll be much slower!\n\nUsing `--num-pairs` will generate **approximately** that many QA pairs. However it might be shorter or longer depending on the `max_seq_length` of the loaded up model. So if you specify 100, you might only get 10 since the model's max sequence length is capped.","metadata":{"id":"nGdAXafV6S2M"}},{"cell_type":"code","source":"import time\n# Process 3 chunks for now -> can increase but slower!\nfor filename in filenames[:7]:\n    !synthetic-data-kit \\\n        -c synthetic_data_kit_config.yaml \\\n        create {filename} \\\n        --num-pairs 10 \\\n        --type \"qa\"\n    time.sleep(2) # Sleep some time to leave some room for processing","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pYYYlMJ7ZtT7","outputId":"d83c7691-944c-45ab-9fc9-19434a34cf2a","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T10:46:01.019281Z","iopub.execute_input":"2025-11-10T10:46:01.020021Z","iopub.status.idle":"2025-11-10T10:50:00.902128Z","shell.execute_reply.started":"2025-11-10T10:46:01.019990Z","shell.execute_reply":"2025-11-10T10:50:00.901092Z"}},"outputs":[{"name":"stdout","text":"vLLM STDOUT: INFO:     127.0.0.1:58788 - \"GET /v1/models HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO:     127.0.0.1:58804 - \"GET /v1/models HTTP/1.1\" 200 OK\n\u001b[?25lvLLM STDOUT: INFO 11-10 10:46:01 [logger.py:43] Received request chatcmpl-864702f5a0b0470b81dd253c3b4effcb: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n40 Interview Questions on Machine Learning\\n\\nJoin this telegram channel for Free Data Science &\\nMachine Learning Resources\\n\\nQ1. You are given a train data set having 1000 columns and 1 million rows. The data set\\nis based on a classification problem. Your manager has asked you to reduce the\\ndimension of this data so that model computation time can be reduced. Your machine\\nhas memory constraints. What would you do? (You are free to make practical\\nassumptions.)\\n\\nAnswer: Processing a high dimensional data on a limited memory machine is a strenuous\\ntask, your interviewer would be fully aware of that. Following are the methods you can use to\\ntackle such situation:\\n\\n1. Since we have lower RAM, we should close all other applications in our machine,\\n\\nincluding the web browser, so that most of the memory can be put to use.\\n\\n2. We can randomly sample the data set. This means, we can create a smaller data set,\\n\\nlet’s say, having 1000 variables and 300000 rows and do the computations.\\n\\n3. To reduce dimensionality, we can separate the numerical and categorical variables and\\nremove the correlated variables. For numerical variables, we’ll use correlation. For\\ncategorical variables, we’ll use the chi-square test.\\n\\n4. Also, we can use PCA and pick the components which can explain the maximum\\n\\nvariance in the data set.\\n\\n5. Using online learning algorithms like Vowpal Wabbit (available in Python) is a\\n\\npossible option.\\n\\n6. Building a linear model using Stochastic Gradient Descent is also helpful.\\n7. We can also apply our business understanding to estimate which all predictors can\\nfailing to\\n\\nimpact\\nidentify useful predictors might result in significant loss of information.\\n\\nthis is an intuitive approach,\\n\\nthe response variable. But,\\n\\nQ2. Is rotation necessary in PCA? If yes, Why? What will happen if you don’t rotate the\\ncomponents?\\n\\nAnswer: Yes, rotation (orthogonal) is necessary because it maximizes the difference between\\nvariance captured by the component. This makes the components easier to interpret. Not to\\nforget, that’s the motive of doing PCA where, we aim to select fewer components (than\\nfeatures) which can explain the maximum variance in the data set. By doing rotation, the\\nrelative location of the components doesn’t change, it only changes the actual coordinates of\\nthe points.\\n\\nIf we don’t rotate the components, the effect of PCA will diminish and we’ll have to select\\nmore number of components to explain variance in the data set.\\n\\n \\n\\x0cQ3. You are given a data set. The data set has missing values which spread along 1\\nstandard deviation from the median. What percentage of data would remain\\nunaffected? Why?\\n\\nAnswer: This question has enough hints for you to start thinking! Since, the data is spread\\nacross median, let’s assume it’s a normal distribution. We know, in a normal distribution,\\n~68% of the data lies in 1 standard deviation from mean (or mode, median), which leaves\\n~32% of the data unaffected. Therefore, ~32% of the data would remain unaffected by\\nmissing values.\\n\\nQ4. You are given a data set on cancer detection. You’ve build a classification model and\\nachieved an accuracy of 96%. Why shouldn’t you be happy with your model\\nperformance? What can you do about it?\\n\\nAnswer: If you have worked on enough data sets, you should deduce that cancer detection\\nresults in imbalanced data. In an imbalanced data set, accuracy should not be used as a\\nmeasure of performance because 96% (as given) might only be predicting majority class\\ncorrectly, but our class of interest is minority class (4%) which is the people who actually got\\ndiagnosed with cancer. Hence, in order to evaluate model performance, we should use\\nSensitivity (True Positive Rate), Specificity (True Negative Rate), F measure to determine\\nclass wise performance of the classifier. If the minority class performance is found to to be\\npoor, we can undertake the following steps:\\n\\n1. We can use undersampling, oversampling or SMOTE to make the data balanced.\\n2. We can alter the prediction threshold value by doing probability caliberation and\\n\\nfinding a optimal threshold using AUC-ROC curve.\\n\\n3. We can assign weight to classes such that the minority classes gets larger weight.\\n4. We can also use anomaly detection.\\n\\nQ5. Why is naive Bayes so ‘naive’?\\n\\nAnswer: naive Bayes is so ‘naive’ because it assumes that all of the features in a data set are\\nequally important and independent. As we know, these assumption are rarely true in real\\nworld scenario.\\n\\nQ6. Explain prior probability,\\nnaiveBayes algorithm?\\n\\nlikelihood and marginal\\n\\nlikelihood<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:46:01 [engine.py:317] Added request chatcmpl-864702f5a0b0470b81dd253c3b4effcb.\n\u001b[32m⠋\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_0.txt...vLLM STDOUT: INFO:     127.0.0.1:58806 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:46:05 [logger.py:43] Received request chatcmpl-7dffe849fe5a4e55ae0945d707f54447: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n40 Interview Questions on Machine Learning\\n\\nJoin this telegram channel for Free Data Science &\\nMachine Learning Resources\\n\\nQ1. You are given a train data set having 1000 columns and 1 million rows. The data set\\nis based on a classification problem. Your manager has asked you to reduce the\\ndimension of this data so that model computation time can be reduced. Your machine\\nhas memory constraints. What would you do? (You are free to make practical\\nassumptions.)\\n\\nAnswer: Processing a high dimensional data on a limited memory machine is a strenuous\\ntask, your interviewer would be fully aware of that. Following are the methods you can use to\\ntackle such situation:\\n\\n1. Since we have lower RAM, we should close all other applications in our machine,\\n\\nincluding the web browser, so that most of the memory can be put to use.\\n\\n2. We can randomly sample the data set. This means, we can create a smaller data set,\\n\\nlet’s say, having 1000 variables and 300000 rows and do the computations.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n\u001b[2K\u001b[1A\u001b[2KProcessing 8 chunks to generate QA pairs...\n\u001b[32m⠏\u001b[0m Generating qa content from \ndata/output/Machine_Learning_Interview_Questions_0.txt...vLLM STDOUT: INFO 11-10 10:46:05 [engine.py:317] Added request chatcmpl-7dffe849fe5a4e55ae0945d707f54447.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_0.txt...vLLM STDOUT: INFO:     127.0.0.1:58818 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:46:08 [logger.py:43] Received request chatcmpl-3c5c8c2834a942509eeb8d5132c10b58: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nSince we have lower RAM, we should close all other applications in our machine,\\n\\nincluding the web browser, so that most of the memory can be put to use.\\n\\n2. We can randomly sample the data set. This means, we can create a smaller data set,\\n\\nlet’s say, having 1000 variables and 300000 rows and do the computations.\\n\\n3. To reduce dimensionality, we can separate the numerical and categorical variables and\\nremove the correlated variables. For numerical variables, we’ll use correlation. For\\ncategorical variables, we’ll use the chi-square test.\\n\\n4. Also, we can use PCA and pick the components which can explain the maximum\\n\\nvariance in the data set.\\n\\n5. Using online learning algorithms like Vowpal Wabbit (available in Python) is a\\n\\npossible option.\\n\\n6. Building a linear model using Stochastic Gradient Descent is also helpful.\\n7. We can also apply our business understanding to estimate which all predictors can\\nfailing to\\n\\nimpact\\nidentify useful predictors might result in significant loss of information.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:46:08 [engine.py:317] Added request chatcmpl-3c5c8c2834a942509eeb8d5132c10b58.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_0.txt...vLLM STDOUT: INFO:     127.0.0.1:58834 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:46:10 [logger.py:43] Received request chatcmpl-ff56576a59694a71b2dbc36a3fb69c1b: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nUsing online learning algorithms like Vowpal Wabbit (available in Python) is a\\n\\npossible option.\\n\\n6. Building a linear model using Stochastic Gradient Descent is also helpful.\\n7. We can also apply our business understanding to estimate which all predictors can\\nfailing to\\n\\nimpact\\nidentify useful predictors might result in significant loss of information.\\n\\nthis is an intuitive approach,\\n\\nthe response variable. But,\\n\\nQ2. Is rotation necessary in PCA? If yes, Why? What will happen if you don’t rotate the\\ncomponents?\\n\\nAnswer: Yes, rotation (orthogonal) is necessary because it maximizes the difference between\\nvariance captured by the component. This makes the components easier to interpret. Not to\\nforget, that’s the motive of doing PCA where, we aim to select fewer components (than\\nfeatures) which can explain the maximum variance in the data set. By doing rotation, the\\nrelative location of the components doesn’t change, it only changes the actual coordinates of\\nthe points.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:46:10 [engine.py:317] Added request chatcmpl-ff56576a59694a71b2dbc36a3fb69c1b.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_0.txt...vLLM STDOUT: INFO:     127.0.0.1:35238 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:46:13 [logger.py:43] Received request chatcmpl-1a4ffb18e65245d5a2f4bd41fdb36315: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nThis makes the components easier to interpret. Not to\\nforget, that’s the motive of doing PCA where, we aim to select fewer components (than\\nfeatures) which can explain the maximum variance in the data set. By doing rotation, the\\nrelative location of the components doesn’t change, it only changes the actual coordinates of\\nthe points.\\n\\nIf we don’t rotate the components, the effect of PCA will diminish and we’ll have to select\\nmore number of components to explain variance in the data set.\\n\\n \\n\\x0cQ3. You are given a data set. The data set has missing values which spread along 1\\nstandard deviation from the median. What percentage of data would remain\\nunaffected? Why?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:46:13 [engine.py:317] Added request chatcmpl-1a4ffb18e65245d5a2f4bd41fdb36315.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_0.txt...vLLM STDOUT: INFO:     127.0.0.1:35246 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:46:17 [logger.py:43] Received request chatcmpl-098d3c230fb14cb2ab3715113fb53791: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nYou are given a data set. The data set has missing values which spread along 1\\nstandard deviation from the median. What percentage of data would remain\\nunaffected? Why?\\n\\nAnswer: This question has enough hints for you to start thinking! Since, the data is spread\\nacross median, let’s assume it’s a normal distribution. We know, in a normal distribution,\\n~68% of the data lies in 1 standard deviation from mean (or mode, median), which leaves\\n~32% of the data unaffected. Therefore, ~32% of the data would remain unaffected by\\nmissing values.\\n\\nQ4. You are given a data set on cancer detection. You’ve build a classification model and\\nachieved an accuracy of 96%. Why shouldn’t you be happy with your model\\nperformance? What can you do about it?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from \ndata/output/Machine_Learning_Interview_Questions_0.txt...vLLM STDOUT: INFO 11-10 10:46:17 [engine.py:317] Added request chatcmpl-098d3c230fb14cb2ab3715113fb53791.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_0.txt...vLLM STDOUT: INFO:     127.0.0.1:35252 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from \ndata/output/Machine_Learning_Interview_Questions_0.txt...vLLM STDOUT: INFO 11-10 10:46:20 [logger.py:43] Received request chatcmpl-955a0d34ada14932a401022f625dabff: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nYou are given a data set on cancer detection. You’ve build a classification model and\\nachieved an accuracy of 96%. Why shouldn’t you be happy with your model\\nperformance? What can you do about it?\\n\\nAnswer: If you have worked on enough data sets, you should deduce that cancer detection\\nresults in imbalanced data. In an imbalanced data set, accuracy should not be used as a\\nmeasure of performance because 96% (as given) might only be predicting majority class\\ncorrectly, but our class of interest is minority class (4%) which is the people who actually got\\ndiagnosed with cancer. Hence, in order to evaluate model performance, we should use\\nSensitivity (True Positive Rate), Specificity (True Negative Rate), F measure to determine\\nclass wise performance of the classifier. If the minority class performance is found to to be\\npoor, we can undertake the following steps:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:46:20 [engine.py:317] Added request chatcmpl-955a0d34ada14932a401022f625dabff.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_0.txt...vLLM STDOUT: INFO:     127.0.0.1:34868 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:46:23 [logger.py:43] Received request chatcmpl-9faed8aa05e641d2b99b78fa96d9a810: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nIn an imbalanced data set, accuracy should not be used as a\\nmeasure of performance because 96% (as given) might only be predicting majority class\\ncorrectly, but our class of interest is minority class (4%) which is the people who actually got\\ndiagnosed with cancer. Hence, in order to evaluate model performance, we should use\\nSensitivity (True Positive Rate), Specificity (True Negative Rate), F measure to determine\\nclass wise performance of the classifier. If the minority class performance is found to to be\\npoor, we can undertake the following steps:\\n\\n1. We can use undersampling, oversampling or SMOTE to make the data balanced.\\n2. We can alter the prediction threshold value by doing probability caliberation and\\n\\nfinding a optimal threshold using AUC-ROC curve.\\n\\n3. We can assign weight to classes such that the minority classes gets larger weight.\\n4. We can also use anomaly detection.\\n\\nQ5. Why is naive Bayes so ‘naive’?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:46:23 [engine.py:317] Added request chatcmpl-9faed8aa05e641d2b99b78fa96d9a810.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_0.txt...vLLM STDOUT: INFO:     127.0.0.1:34880 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:46:26 [logger.py:43] Received request chatcmpl-54df47ea5a3e431796f1333ad00784e6: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nWe can assign weight to classes such that the minority classes gets larger weight.\\n4. We can also use anomaly detection.\\n\\nQ5. Why is naive Bayes so ‘naive’?\\n\\nAnswer: naive Bayes is so ‘naive’ because it assumes that all of the features in a data set are\\nequally important and independent. As we know, these assumption are rarely true in real\\nworld scenario.\\n\\nQ6. Explain prior probability,\\nnaiveBayes algorithm?\\n\\nlikelihood and marginal\\n\\nlikelihood<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:46:26 [engine.py:317] Added request chatcmpl-54df47ea5a3e431796f1333ad00784e6.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_0.txt...vLLM STDOUT: INFO:     127.0.0.1:34892 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KBatch processing complete._Questions_0.txt...\n\u001b[32m⠋\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KGenerated 17 QA pairs totalQuestions_0.txt...\n\u001b[32m⠋\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2KSaving result to Interview_Questions_0.txt...\ndata/generated/Machine_Learning_Interview_Questions_0_qa_pairs.json\n\u001b[32m⠋\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n\u001b[32m⠋\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KSuccessfully wrote result to estions_0.txt...\ndata/generated/Machine_Learning_Interview_Questions_0_qa_pairs.json\n\u001b[32m⠋\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_0.txt...\n\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\n\u001b[1;32mdata/generated/Machine_Learning_Interview_Questions_0_qa_pairs.json\u001b[0m\nvLLM STDOUT: INFO:     127.0.0.1:39042 - \"GET /v1/models HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO:     127.0.0.1:39056 - \"GET /v1/models HTTP/1.1\" 200 OK\n\u001b[?25lvLLM STDOUT: INFO 11-10 10:46:31 [logger.py:43] Received request chatcmpl-a41a6b3256eb4e908dad6fb0a85415bf: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nAnswer: naive Bayes is so ‘naive’ because it assumes that all of the features in a data set are\\nequally important and independent. As we know, these assumption are rarely true in real\\nworld scenario.\\n\\nQ6. Explain prior probability,\\nnaiveBayes algorithm?\\n\\nlikelihood and marginal\\n\\nlikelihood in context of\\n\\nAnswer: Prior probability is nothing but, the proportion of dependent (binary) variable in the\\ndata set. It is the closest guess you can make about a class, without any further information.\\nFor example: In a data set, the dependent variable is binary (1 and 0). The proportion of 1\\n(spam) is 70% and 0 (not spam) is 30%. Hence, we can estimate that there are 70% chances\\nthat any new email would  be classified as spam.\\n\\n \\n \\n \\n\\x0cLikelihood is the probability of classifying a given observation as 1 in presence of some other\\nvariable. For example: The probability that\\nthe word ‘FREE’ is used in previous spam\\nmessage is likelihood. Marginal likelihood is, the probability that the word ‘FREE’ is used in\\nany message.\\n\\nQ7. You are working on a time series data set. You manager has asked you to build a\\nhigh accuracy model. You start with the decision tree algorithm, since you know it\\nworks fairly well on all kinds of data. Later, you tried a time series regression model and\\ngot higher accuracy than decision tree model. Can this happen? Why?\\n\\nAnswer: Time series data is known to posses linearity. On the other hand, a decision tree\\nalgorithm is known to work best to detect non – linear interactions. The reason why decision\\ntree failed to provide robust predictions because it couldn’t map the linear relationship as\\ngood as a regression model did. Therefore, we learned that, a linear regression model can\\nprovide robust prediction given the data set satisfies its linearity assumptions.\\n\\nQ8. You are assigned a new project which involves helping a food delivery company\\nsave more money. The problem is, company’s delivery team aren’t able to deliver food\\non time. As a result, their customers get unhappy. And, to keep them happy, they end up\\ndelivering food for free. Which machine learning algorithm can save them?\\n\\nAnswer: You might have started hopping through the list of ML algorithms in your mind.\\nBut, wait! Such questions are asked to test your machine learning fundamentals.\\n\\nThis is not a machine learning problem. This is a route optimization problem. A machine\\nlearning problem consist of three things:\\n\\n1. There exist a pattern.\\n2. You cannot solve it mathematically (even by writing exponential equations).\\n3. You have data on it.\\n\\nAlways look for these three factors to decide if machine learning is a tool to solve a particular\\nproblem.\\n\\nQ9. You came to know that your model is suffering from low bias and high variance.\\nWhich algorithm should you use to tackle it? Why?\\n\\nAnswer:  Low bias occurs when the model’s predicted values are near to actual values. In\\nother words, the model becomes flexible enough to mimic the training data distribution.\\nWhile it\\nto forget, a flexible model has\\nno generalization capabilities. It means, when this model is tested on an unseen data, it gives\\ndisappointing results.\\n\\nsounds like great achievement, but not\\n\\n \\n \\n \\n\\x0cIn such situations, we can use bagging algorithm (like random forest) to tackle high variance\\nproblem. Bagging algorithms divides a data set into subsets made with repeated randomized\\nsampling. Then, these samples are used to generate  a set of models using a single learning\\nalgorithm. Later,\\nthe model predictions are combined using voting (classification) or\\naveraging (regression).\\n\\nAlso, to combat high variance, we can:\\n\\n1. Use regularization technique, where higher model coefficients get penalized,\\n\\nhence lowering model complexity.\\n\\n2. Use top n features from variable importance chart. May be, with all the variable in the\\n\\ndata set, the algorithm is having difficulty in finding the meaningful signal.\\n\\nQ10. You are given a data set. The data set contains many variables, some of which are\\nhighly correlated and you know about it. Your manager has asked you to run PCA.\\nWould you remove correlated variables first? Why?\\n\\nAnswer: Chances are, you might be tempted to say No, but that would be incorrect.\\nDiscarding correlated variables have a substantial effect on PCA because, in presence of\\ncorrelated variables, the variance explained by a particular component gets inflated.\\n\\nFor example: You have 3 variables in a data set, of which 2 are correlated. If you run PCA on\\nthis data set, the first principal component would exhibit twice the variance than it would\\nexhibit with uncorrelated variables. Also, adding correlated variables lets PCA<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:46:31 [engine.py:317] Added request chatcmpl-a41a6b3256eb4e908dad6fb0a85415bf.\n\u001b[32m⠋\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_1.txt...vLLM STDOUT: INFO:     127.0.0.1:39066 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:46:36 [logger.py:43] Received request chatcmpl-937634248dec42f0803b4e01b262c505: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nAnswer: naive Bayes is so ‘naive’ because it assumes that all of the features in a data set are\\nequally important and independent. As we know, these assumption are rarely true in real\\nworld scenario.\\n\\nQ6. Explain prior probability,\\nnaiveBayes algorithm?\\n\\nlikelihood and marginal\\n\\nlikelihood in context of\\n\\nAnswer: Prior probability is nothing but, the proportion of dependent (binary) variable in the\\ndata set. It is the closest guess you can make about a class, without any further information.\\nFor example: In a data set, the dependent variable is binary (1 and 0). The proportion of 1\\n(spam) is 70% and 0 (not spam) is 30%. Hence, we can estimate that there are 70% chances\\nthat any new email would  be classified as spam.\\n\\n \\n \\n \\n\\x0cLikelihood is the probability of classifying a given observation as 1 in presence of some other\\nvariable. For example: The probability that\\nthe word ‘FREE’ is used in previous spam\\nmessage is likelihood. Marginal likelihood is, the probability that the word ‘FREE’ is used in\\nany message.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KProcessing 8 chunks to generate QA pairs.....\n\u001b[32m⠇\u001b[0m Generating qa content from \ndata/output/Machine_Learning_Interview_Questions_1.txt...vLLM STDOUT: INFO 11-10 10:46:36 [engine.py:317] Added request chatcmpl-937634248dec42f0803b4e01b262c505.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_1.txt...vLLM STDOUT: INFO:     127.0.0.1:39068 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:46:39 [logger.py:43] Received request chatcmpl-bee129509d7c428c93ee81ad097484c3: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nHence, we can estimate that there are 70% chances\\nthat any new email would  be classified as spam.\\n\\n \\n \\n \\n\\x0cLikelihood is the probability of classifying a given observation as 1 in presence of some other\\nvariable. For example: The probability that\\nthe word ‘FREE’ is used in previous spam\\nmessage is likelihood. Marginal likelihood is, the probability that the word ‘FREE’ is used in\\nany message.\\n\\nQ7. You are working on a time series data set. You manager has asked you to build a\\nhigh accuracy model. You start with the decision tree algorithm, since you know it\\nworks fairly well on all kinds of data. Later, you tried a time series regression model and\\ngot higher accuracy than decision tree model. Can this happen? Why?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:46:39 [engine.py:317] Added request chatcmpl-bee129509d7c428c93ee81ad097484c3.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_1.txt...vLLM STDOUT: INFO:     127.0.0.1:39082 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:46:41 [logger.py:43] Received request chatcmpl-fedfa81e8dec47439e7f783230cfe37c: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nYou start with the decision tree algorithm, since you know it\\nworks fairly well on all kinds of data. Later, you tried a time series regression model and\\ngot higher accuracy than decision tree model. Can this happen? Why?\\n\\nAnswer: Time series data is known to posses linearity. On the other hand, a decision tree\\nalgorithm is known to work best to detect non – linear interactions. The reason why decision\\ntree failed to provide robust predictions because it couldn’t map the linear relationship as\\ngood as a regression model did. Therefore, we learned that, a linear regression model can\\nprovide robust prediction given the data set satisfies its linearity assumptions.\\n\\nQ8. You are assigned a new project which involves helping a food delivery company\\nsave more money. The problem is, company’s delivery team aren’t able to deliver food\\non time. As a result, their customers get unhappy. And, to keep them happy, they end up\\ndelivering food for free. Which machine learning algorithm can save them?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:46:41 [engine.py:317] Added request chatcmpl-fedfa81e8dec47439e7f783230cfe37c.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_1.txt...vLLM STDOUT: INFO:     127.0.0.1:41368 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:46:44 [logger.py:43] Received request chatcmpl-92ad33a2dc674b16af874902bac7974e: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nAs a result, their customers get unhappy. And, to keep them happy, they end up\\ndelivering food for free. Which machine learning algorithm can save them?\\n\\nAnswer: You might have started hopping through the list of ML algorithms in your mind.\\nBut, wait! Such questions are asked to test your machine learning fundamentals.\\n\\nThis is not a machine learning problem. This is a route optimization problem. A machine\\nlearning problem consist of three things:\\n\\n1. There exist a pattern.\\n2. You cannot solve it mathematically (even by writing exponential equations).\\n3. You have data on it.\\n\\nAlways look for these three factors to decide if machine learning is a tool to solve a particular\\nproblem.\\n\\nQ9. You came to know that your model is suffering from low bias and high variance.\\nWhich algorithm should you use to tackle it? Why?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from \ndata/output/Machine_Learning_Interview_Questions_1.txt...vLLM STDOUT: INFO 11-10 10:46:44 [engine.py:317] Added request chatcmpl-92ad33a2dc674b16af874902bac7974e.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_1.txt...vLLM STDOUT: INFO:     127.0.0.1:41372 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:46:48 [logger.py:43] Received request chatcmpl-a1ea1694d98e4ac1bb608e05c469fb36: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nYou cannot solve it mathematically (even by writing exponential equations).\\n3. You have data on it.\\n\\nAlways look for these three factors to decide if machine learning is a tool to solve a particular\\nproblem.\\n\\nQ9. You came to know that your model is suffering from low bias and high variance.\\nWhich algorithm should you use to tackle it? Why?\\n\\nAnswer:  Low bias occurs when the model’s predicted values are near to actual values. In\\nother words, the model becomes flexible enough to mimic the training data distribution.\\nWhile it\\nto forget, a flexible model has\\nno generalization capabilities. It means, when this model is tested on an unseen data, it gives\\ndisappointing results.\\n\\nsounds like great achievement, but not<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:46:48 [engine.py:317] Added request chatcmpl-a1ea1694d98e4ac1bb608e05c469fb36.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_1.txt...vLLM STDOUT: INFO:     127.0.0.1:41384 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:46:51 [logger.py:43] Received request chatcmpl-85f3972220e646dcb11ccf9e9a863e3d: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nYou came to know that your model is suffering from low bias and high variance.\\nWhich algorithm should you use to tackle it? Why?\\n\\nAnswer:  Low bias occurs when the model’s predicted values are near to actual values. In\\nother words, the model becomes flexible enough to mimic the training data distribution.\\nWhile it\\nto forget, a flexible model has\\nno generalization capabilities. It means, when this model is tested on an unseen data, it gives\\ndisappointing results.\\n\\nsounds like great achievement, but not\\n\\n \\n \\n \\n\\x0cIn such situations, we can use bagging algorithm (like random forest) to tackle high variance\\nproblem. Bagging algorithms divides a data set into subsets made with repeated randomized\\nsampling. Then, these samples are used to generate  a set of models using a single learning\\nalgorithm. Later,\\nthe model predictions are combined using voting (classification) or\\naveraging (regression).\\n\\nAlso, to combat high variance, we can:\\n\\n1. Use regularization technique, where higher model coefficients get penalized,<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from \ndata/output/Machine_Learning_Interview_Questions_1.txt...vLLM STDOUT: INFO 11-10 10:46:51 [engine.py:317] Added request chatcmpl-85f3972220e646dcb11ccf9e9a863e3d.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_1.txt...vLLM STDOUT: INFO:     127.0.0.1:33536 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:46:56 [logger.py:43] Received request chatcmpl-6d5e141e483d473b95912742d8eb8ebc: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nThen, these samples are used to generate  a set of models using a single learning\\nalgorithm. Later,\\nthe model predictions are combined using voting (classification) or\\naveraging (regression).\\n\\nAlso, to combat high variance, we can:\\n\\n1. Use regularization technique, where higher model coefficients get penalized,\\n\\nhence lowering model complexity.\\n\\n2. Use top n features from variable importance chart. May be, with all the variable in the\\n\\ndata set, the algorithm is having difficulty in finding the meaningful signal.\\n\\nQ10. You are given a data set. The data set contains many variables, some of which are\\nhighly correlated and you know about it. Your manager has asked you to run PCA.\\nWould you remove correlated variables first? Why?\\n\\nAnswer: Chances are, you might be tempted to say No, but that would be incorrect.\\nDiscarding correlated variables have a substantial effect on PCA because, in presence of\\ncorrelated variables, the variance explained by a particular component gets inflated.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:46:56 [engine.py:317] Added request chatcmpl-6d5e141e483d473b95912742d8eb8ebc.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_1.txt...vLLM STDOUT: INFO:     127.0.0.1:33546 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:46:59 [logger.py:43] Received request chatcmpl-bddd26e47c0b49d4b92043ed2ab58006: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nYou are given a data set. The data set contains many variables, some of which are\\nhighly correlated and you know about it. Your manager has asked you to run PCA.\\nWould you remove correlated variables first? Why?\\n\\nAnswer: Chances are, you might be tempted to say No, but that would be incorrect.\\nDiscarding correlated variables have a substantial effect on PCA because, in presence of\\ncorrelated variables, the variance explained by a particular component gets inflated.\\n\\nFor example: You have 3 variables in a data set, of which 2 are correlated. If you run PCA on\\nthis data set, the first principal component would exhibit twice the variance than it would\\nexhibit with uncorrelated variables. Also, adding correlated variables lets PCA<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:46:59 [engine.py:317] Added request chatcmpl-bddd26e47c0b49d4b92043ed2ab58006.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_1.txt...vLLM STDOUT: INFO:     127.0.0.1:33550 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2KBatch processing complete._Questions_1.txt...\n\u001b[32m⠧\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KGenerated 16 QA pairs totalQuestions_1.txt...\n\u001b[32m⠧\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KSaving result to Interview_Questions_1.txt...\ndata/generated/Machine_Learning_Interview_Questions_1_qa_pairs.json\n\u001b[32m⠧\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n\u001b[32m⠧\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KSuccessfully wrote result to estions_1.txt...\ndata/generated/Machine_Learning_Interview_Questions_1_qa_pairs.json\n\u001b[32m⠧\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_1.txt...\n\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\n\u001b[1;32mdata/generated/Machine_Learning_Interview_Questions_1_qa_pairs.json\u001b[0m\nvLLM STDOUT: INFO:     127.0.0.1:38540 - \"GET /v1/models HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO:     127.0.0.1:38556 - \"GET /v1/models HTTP/1.1\" 200 OK\n\u001b[?25lvLLM STDOUT: INFO 11-10 10:47:05 [logger.py:43] Received request chatcmpl-2649958789be47a99a6c60000fe69786: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\na particular component gets inflated.\\n\\nFor example: You have 3 variables in a data set, of which 2 are correlated. If you run PCA on\\nthis data set, the first principal component would exhibit twice the variance than it would\\nexhibit with uncorrelated variables. Also, adding correlated variables lets PCA put more\\nimportance on those variable, which is misleading.\\n\\nQ11. After spending several hours, you are now anxious to build a high accuracy model.\\nAs a result, you build 5 GBM models, thinking a boosting algorithm would do the\\nmagic. Unfortunately, neither of models could perform better than benchmark score.\\nFinally, you decided to combine those models. Though, ensembled models are known to\\nreturn high accuracy, but you are unfortunate. Where did you miss?\\n\\nAnswer: As we know, ensemble learners are based on the idea of combining weak learners to\\ncreate strong learners. But, these learners provide superior result when the combined models\\nare uncorrelated. Since, we have used 5 GBM models and got no accuracy improvement,\\nsuggests that the models are correlated. The problem with correlated models is, all the models\\nprovide same information.\\n\\nFor example: If model 1 has classified User1122 as 1, there are high chances model 2 and\\nmodel 3 would have done the same, even if its actual value is 0. Therefore, ensemble learners\\nare built on the premise of combining weak uncorrelated models to obtain better predictions.\\n\\nQ12. How is kNN different from kmeans clustering?\\n\\n \\n \\n \\n\\x0cAnswer: Don’t get mislead by ‘k’ in their names. You should know that the fundamental\\ndifference between both these algorithms is, kmeans is unsupervised in nature and kNN is\\nsupervised in nature. kmeans is a clustering algorithm. kNN is a classification (or regression)\\nalgorithm.\\n\\nkmeans algorithm partitions a data set into clusters such that a cluster formed is homogeneous\\nand the points in each cluster are close to each other. The algorithm tries to maintain enough\\nseparability between these clusters. Due to unsupervised nature, the clusters have no labels.\\n\\nkNN algorithm tries to classify an unlabeled observation based on its k (can be any number )\\nsurrounding neighbors. It is also known as lazy learner because it involves minimal training\\nof model. Hence, it doesn’t use training data to make generalization on unseen data set.\\n\\nQ13. How is True Positive Rate and Recall related? Write the equation.\\n\\nAnswer: True Positive Rate = Recall. Yes, they are equal having the formula (TP/TP + FN).\\n\\nKnow more: Evaluation Metrics\\n\\nQ14. You have built a multiple regression model. Your model R² isn’t as good as you\\nwanted. For improvement, your remove the intercept term, your model R² becomes 0.8\\nfrom 0.3. Is it possible? How?\\n\\nAnswer: Yes, it is possible. We need to understand the significance of intercept term in a\\nregression model. The intercept\\nterm shows model prediction without any independent\\nvariable i.e. mean prediction. The formula of R² = 1 – ∑(y – y´)²/∑(y – ymean)² where y´ is\\npredicted value.   \\n\\nWhen intercept term is present, R² value evaluates your model wrt. to the mean model. In\\nabsence of intercept term (ymean), the model can make no such evaluation, with large\\ndenominator, ∑(y - y´)²/∑(y)² equation’s value becomes smaller than actual, resulting in\\nhigher R².\\n\\nQ15. After analyzing the model, your manager has informed that your regression model\\nis suffering from multicollinearity. How would you check if he’s true? Without losing\\nany information, can you still build a better model?\\n\\nuse\\n\\nAnswer: To check multicollinearity, we can create a correlation matrix to identify & remove\\nvariables having correlation above 75% (deciding a threshold is subjective). In addition, we\\ncan\\nof\\ninflation\\nmulticollinearity. VIF value <= 4 suggests no multicollinearity whereas a value of >= 10\\nserious multicollinearity. Also, we can use tolerance as an indicator of\\nimplies\\nmulticollinearity.\\n\\ncalculate VIF (variance\\n\\ncheck the\\n\\npresence\\n\\nfactor)\\n\\nto\\n\\n \\n \\n \\n\\x0cBut, removing correlated variables might lead to loss of information. In order to retain those\\nvariables, we can use penalized regression models like ridge or lasso regression. Also, we can\\nadd some random noise in correlated variable so that the variables become different from\\neach other. But, adding noise might affect the prediction accuracy, hence this approach should\\nbe<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:47:05 [engine.py:317] Added request chatcmpl-2649958789be47a99a6c60000fe69786.\n\u001b[32m⠋\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_2.txt...vLLM STDOUT: INFO:     127.0.0.1:38566 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:47:11 [logger.py:43] Received request chatcmpl-70f7c667b4e44996be4451eed69bf89d: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n a particular component gets inflated.\\n\\nFor example: You have 3 variables in a data set, of which 2 are correlated. If you run PCA on\\nthis data set, the first principal component would exhibit twice the variance than it would\\nexhibit with uncorrelated variables. Also, adding correlated variables lets PCA put more\\nimportance on those variable, which is misleading.\\n\\nQ11. After spending several hours, you are now anxious to build a high accuracy model.\\nAs a result, you build 5 GBM models, thinking a boosting algorithm would do the\\nmagic. Unfortunately, neither of models could perform better than benchmark score.\\nFinally, you decided to combine those models. Though, ensembled models are known to\\nreturn high accuracy, but you are unfortunate. Where did you miss?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KProcessing 7 chunks to generate QA pairs.....\n\u001b[32m⠦\u001b[0m Generating qa content from \ndata/output/Machine_Learning_Interview_Questions_2.txt...vLLM STDOUT: INFO 11-10 10:47:11 [engine.py:317] Added request chatcmpl-70f7c667b4e44996be4451eed69bf89d.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_2.txt...vLLM STDOUT: INFO:     127.0.0.1:41264 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:47:15 [logger.py:43] Received request chatcmpl-9f52a94d4a1c4566997a70c32177818f: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nUnfortunately, neither of models could perform better than benchmark score.\\nFinally, you decided to combine those models. Though, ensembled models are known to\\nreturn high accuracy, but you are unfortunate. Where did you miss?\\n\\nAnswer: As we know, ensemble learners are based on the idea of combining weak learners to\\ncreate strong learners. But, these learners provide superior result when the combined models\\nare uncorrelated. Since, we have used 5 GBM models and got no accuracy improvement,\\nsuggests that the models are correlated. The problem with correlated models is, all the models\\nprovide same information.\\n\\nFor example: If model 1 has classified User1122 as 1, there are high chances model 2 and\\nmodel 3 would have done the same, even if its actual value is 0. Therefore, ensemble learners\\nare built on the premise of combining weak uncorrelated models to obtain better predictions.\\n\\nQ12. How is kNN different from kmeans clustering?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from \ndata/output/Machine_Learning_Interview_Questions_2.txt...vLLM STDOUT: INFO 11-10 10:47:15 [engine.py:317] Added request chatcmpl-9f52a94d4a1c4566997a70c32177818f.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_2.txt...vLLM STDOUT: INFO:     127.0.0.1:41274 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:47:17 [logger.py:43] Received request chatcmpl-9726f420144f4f7d80e9829ac73dc712: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nThe problem with correlated models is, all the models\\nprovide same information.\\n\\nFor example: If model 1 has classified User1122 as 1, there are high chances model 2 and\\nmodel 3 would have done the same, even if its actual value is 0. Therefore, ensemble learners\\nare built on the premise of combining weak uncorrelated models to obtain better predictions.\\n\\nQ12. How is kNN different from kmeans clustering?\\n\\n \\n \\n \\n\\x0cAnswer: Don’t get mislead by ‘k’ in their names. You should know that the fundamental\\ndifference between both these algorithms is, kmeans is unsupervised in nature and kNN is\\nsupervised in nature. kmeans is a clustering algorithm. kNN is a classification (or regression)\\nalgorithm.\\n\\nkmeans algorithm partitions a data set into clusters such that a cluster formed is homogeneous\\nand the points in each cluster are close to each other. The algorithm tries to maintain enough\\nseparability between these clusters. Due to unsupervised nature, the clusters have no labels.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from \ndata/output/Machine_Learning_Interview_Questions_2.txt...vLLM STDOUT: INFO 11-10 10:47:17 [engine.py:317] Added request chatcmpl-9726f420144f4f7d80e9829ac73dc712.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_2.txt...vLLM STDOUT: INFO:     127.0.0.1:41280 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from \ndata/output/Machine_Learning_Interview_Questions_2.txt...vLLM STDOUT: INFO 11-10 10:47:20 [logger.py:43] Received request chatcmpl-5de53deb376948c9af7cb5c928539c8a: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nkNN is a classification (or regression)\\nalgorithm.\\n\\nkmeans algorithm partitions a data set into clusters such that a cluster formed is homogeneous\\nand the points in each cluster are close to each other. The algorithm tries to maintain enough\\nseparability between these clusters. Due to unsupervised nature, the clusters have no labels.\\n\\nkNN algorithm tries to classify an unlabeled observation based on its k (can be any number )\\nsurrounding neighbors. It is also known as lazy learner because it involves minimal training\\nof model. Hence, it doesn’t use training data to make generalization on unseen data set.\\n\\nQ13. How is True Positive Rate and Recall related? Write the equation.\\n\\nAnswer: True Positive Rate = Recall. Yes, they are equal having the formula (TP/TP + FN).\\n\\nKnow more: Evaluation Metrics\\n\\nQ14. You have built a multiple regression model. Your model R² isn’t as good as you\\nwanted. For improvement, your remove the intercept term, your model R² becomes 0.8\\nfrom 0.3. Is it possible? How?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:47:20 [engine.py:317] Added request chatcmpl-5de53deb376948c9af7cb5c928539c8a.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_2.txt...vLLM STDOUT: INFO:     127.0.0.1:54686 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:47:23 [logger.py:43] Received request chatcmpl-87827f4db0054e00a9d714e99107d8a8: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nYour model R² isn’t as good as you\\nwanted. For improvement, your remove the intercept term, your model R² becomes 0.8\\nfrom 0.3. Is it possible? How?\\n\\nAnswer: Yes, it is possible. We need to understand the significance of intercept term in a\\nregression model. The intercept\\nterm shows model prediction without any independent\\nvariable i.e. mean prediction. The formula of R² = 1 – ∑(y – y´)²/∑(y – ymean)² where y´ is\\npredicted value.   \\n\\nWhen intercept term is present, R² value evaluates your model wrt. to the mean model. In\\nabsence of intercept term (ymean), the model can make no such evaluation, with large\\ndenominator, ∑(y - y´)²/∑(y)² equation’s value becomes smaller than actual, resulting in\\nhigher R².\\n\\nQ15. After analyzing the model, your manager has informed that your regression model\\nis suffering from multicollinearity. How would you check if he’s true? Without losing\\nany information, can you still build a better model?\\n\\nuse<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from \ndata/output/Machine_Learning_Interview_Questions_2.txt...vLLM STDOUT: INFO 11-10 10:47:23 [engine.py:317] Added request chatcmpl-87827f4db0054e00a9d714e99107d8a8.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_2.txt...vLLM STDOUT: INFO:     127.0.0.1:54692 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:47:26 [logger.py:43] Received request chatcmpl-dba89e360dd84b8fb077bf6bfe889361: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nIn\\nabsence of intercept term (ymean), the model can make no such evaluation, with large\\ndenominator, ∑(y - y´)²/∑(y)² equation’s value becomes smaller than actual, resulting in\\nhigher R².\\n\\nQ15. After analyzing the model, your manager has informed that your regression model\\nis suffering from multicollinearity. How would you check if he’s true? Without losing\\nany information, can you still build a better model?\\n\\nuse\\n\\nAnswer: To check multicollinearity, we can create a correlation matrix to identify & remove\\nvariables having correlation above 75% (deciding a threshold is subjective). In addition, we\\ncan\\nof\\ninflation\\nmulticollinearity. VIF value <= 4 suggests no multicollinearity whereas a value of >= 10\\nserious multicollinearity. Also, we can use tolerance as an indicator of\\nimplies\\nmulticollinearity.\\n\\ncalculate VIF (variance\\n\\ncheck the\\n\\npresence\\n\\nfactor)\\n\\nto<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from \ndata/output/Machine_Learning_Interview_Questions_2.txt...vLLM STDOUT: INFO 11-10 10:47:26 [engine.py:317] Added request chatcmpl-dba89e360dd84b8fb077bf6bfe889361.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_2.txt...vLLM STDOUT: INFO:     127.0.0.1:54696 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:47:30 [logger.py:43] Received request chatcmpl-6fb1b9c5fdb94e62b14884dc1c0460af: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nIn addition, we\\ncan\\nof\\ninflation\\nmulticollinearity. VIF value <= 4 suggests no multicollinearity whereas a value of >= 10\\nserious multicollinearity. Also, we can use tolerance as an indicator of\\nimplies\\nmulticollinearity.\\n\\ncalculate VIF (variance\\n\\ncheck the\\n\\npresence\\n\\nfactor)\\n\\nto\\n\\n \\n \\n \\n\\x0cBut, removing correlated variables might lead to loss of information. In order to retain those\\nvariables, we can use penalized regression models like ridge or lasso regression. Also, we can\\nadd some random noise in correlated variable so that the variables become different from\\neach other. But, adding noise might affect the prediction accuracy, hence this approach should\\nbe<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from \ndata/output/Machine_Learning_Interview_Questions_2.txt...vLLM STDOUT: INFO 11-10 10:47:30 [engine.py:317] Added request chatcmpl-6fb1b9c5fdb94e62b14884dc1c0460af.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_2.txt...vLLM STDOUT: INFO:     127.0.0.1:45758 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2KBatch processing complete._Questions_2.txt...\n\u001b[32m⠹\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KGenerated 13 QA pairs totalQuestions_2.txt...\n\u001b[32m⠹\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KSaving result to Interview_Questions_2.txt...\ndata/generated/Machine_Learning_Interview_Questions_2_qa_pairs.json\n\u001b[32m⠹\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n\u001b[32m⠹\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KSuccessfully wrote result to estions_2.txt...\ndata/generated/Machine_Learning_Interview_Questions_2_qa_pairs.json\n\u001b[32m⠹\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_2.txt...\n\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\n\u001b[1;32mdata/generated/Machine_Learning_Interview_Questions_2_qa_pairs.json\u001b[0m\nvLLM STDOUT: INFO:     127.0.0.1:45774 - \"GET /v1/models HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO:     127.0.0.1:45788 - \"GET /v1/models HTTP/1.1\" 200 OK\n\u001b[?25lvLLM STDOUT: INFO 11-10 10:47:35 [logger.py:43] Received request chatcmpl-ee330b8320894e4e9e1e31a072da6c18: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nof information. In order to retain those\\nvariables, we can use penalized regression models like ridge or lasso regression. Also, we can\\nadd some random noise in correlated variable so that the variables become different from\\neach other. But, adding noise might affect the prediction accuracy, hence this approach should\\nbe carefully used.\\n\\nKnow more: Regression\\n\\nQ16. When is Ridge regression favorable over Lasso regression?\\n\\nAnswer: You can quote ISLR’s authors Hastie, Tibshirani who asserted that, in presence of\\nfew variables with medium / large sized effect, use lasso regression. In presence of many\\nvariables with small / medium sized effect, use ridge regression.\\n\\nConceptually, we can say, lasso regression (L1) does both variable selection and parameter\\nshrinkage, whereas Ridge regression only does parameter shrinkage and end up including all\\nthe coefficients in the model. In presence of correlated variables, ridge regression might be\\nthe preferred choice. Also, ridge regression works best in situations where the least square\\nestimates have higher variance. Therefore, it depends on our model objective.\\n\\nKnow more: Ridge and Lasso Regression\\n\\nQ17. Rise in global average temperature led to decrease in number of pirates around\\nthe world. Does that mean that decrease in number of pirates caused the climate\\nchange?\\n\\nAnswer: After reading this question, you should have understood that this is a classic case of\\n“causation and correlation”. No, we can’t conclude that decrease in number of pirates caused\\nthe climate change because there might be other factors (lurking or confounding variables)\\ninfluencing this phenomenon.\\n\\nTherefore, there might be a correlation between global average temperature and number of\\npirates, but based on this information we can’t say that pirated died because of rise in global\\naverage temperature.\\n\\nKnow more: Causation and Correlation\\n\\nQ18. While working on a data set, how do you select important variables? Explain your\\nmethods.\\n\\nAnswer: Following are the methods of variable selection you can use:\\n\\n1. Remove the correlated variables prior to selecting important variables\\n2. Use linear regression and select variables based on p values\\n\\n \\n \\n \\n\\x0c3. Use Forward Selection, Backward Selection, Stepwise Selection\\n4. Use Random Forest, Xgboost and plot variable importance chart\\n5. Use Lasso Regression\\n6. Measure information gain for the available set of features and select top n features\\n\\naccordingly.\\n\\nQ19. What is the difference between covariance and correlation?\\n\\nAnswer: Correlation is the standardized form of covariance.\\n\\nCovariances are difficult to compare. For example: if we calculate the covariances of salary\\n($) and age (years), we’ll get different covariances which can’t be compared because of\\nhaving unequal scales. To combat such situation, we calculate correlation to get a value\\nbetween -1 and 1, irrespective of their respective scale.\\n\\nQ20. Is it possible capture the correlation between continuous and categorical variable?\\nIf yes, how?\\n\\nAnswer: Yes, we can use ANCOVA (analysis of covariance) technique to capture association\\nbetween continuous and categorical variables.\\n\\nQ21. Both being tree based algorithm, how is random forest different from Gradient\\nboosting algorithm (GBM)?\\n\\nAnswer: The fundamental difference is, random forest uses bagging technique to make\\npredictions. GBM uses boosting techniques to make predictions.\\n\\nIn bagging technique, a data set is divided into n samples using randomized sampling. Then,\\nusing a single learning algorithm a model is build on all samples. Later, the resultant\\npredictions are combined using voting or averaging. Bagging is done is parallel. In boosting,\\nafter the first round of predictions, the algorithm weighs misclassified predictions higher,\\nsuch that they can be corrected in the succeeding round. This sequential process of giving\\nhigher weights to misclassified predictions continue until a stopping criterion is reached.\\n\\nRandom forest improves model accuracy by reducing variance (mainly). The trees grown are\\nuncorrelated to maximize the decrease in variance. On the other hand, GBM improves\\naccuracy my reducing both bias and variance in a model.\\n\\nQ22. Running a binary classification tree algorithm is the easy part. Do you know how\\ndoes a tree splitting takes place i.e. how does the tree decide which variable to split at\\nthe root node and succeeding nodes?\\n\\n \\n \\n \\n\\x0cAnswer: A classification trees makes decision based on Gini Index and Node Entropy. In\\nsimple words, the tree algorithm find the best possible feature which can divide the data\\nset into purest possible children nodes.\\n\\nGini index says, if we select two items from a population at random then they must be of\\nsame class and probability for this is 1 if population is pure. We can calculate Gini as\\nfollowing:\\n\\n1. Calculate Gini for sub-nodes, using formula sum of<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:47:35 [engine.py:317] Added request chatcmpl-ee330b8320894e4e9e1e31a072da6c18.\n\u001b[32m⠋\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_3.txt...vLLM STDOUT: INFO:     127.0.0.1:45790 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n\u001b[2K\u001b[1A\u001b[2KProcessing 10 chunks to generate QA pairs...\n\u001b[32m⠙\u001b[0m Generating qa content from \ndata/output/Machine_Learning_Interview_Questions_3.txt...vLLM STDOUT: INFO 11-10 10:47:40 [logger.py:43] Received request chatcmpl-c2b32b5cccad4adc808e73431bb88597: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n of information. In order to retain those\\nvariables, we can use penalized regression models like ridge or lasso regression. Also, we can\\nadd some random noise in correlated variable so that the variables become different from\\neach other. But, adding noise might affect the prediction accuracy, hence this approach should\\nbe carefully used.\\n\\nKnow more: Regression\\n\\nQ16. When is Ridge regression favorable over Lasso regression?\\n\\nAnswer: You can quote ISLR’s authors Hastie, Tibshirani who asserted that, in presence of\\nfew variables with medium / large sized effect, use lasso regression. In presence of many\\nvariables with small / medium sized effect, use ridge regression.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:47:40 [engine.py:317] Added request chatcmpl-c2b32b5cccad4adc808e73431bb88597.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_3.txt...vLLM STDOUT: INFO:     127.0.0.1:50684 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:47:42 [logger.py:43] Received request chatcmpl-82bedc16b544420f903d290b095bea74: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nBut, adding noise might affect the prediction accuracy, hence this approach should\\nbe carefully used.\\n\\nKnow more: Regression\\n\\nQ16. When is Ridge regression favorable over Lasso regression?\\n\\nAnswer: You can quote ISLR’s authors Hastie, Tibshirani who asserted that, in presence of\\nfew variables with medium / large sized effect, use lasso regression. In presence of many\\nvariables with small / medium sized effect, use ridge regression.\\n\\nConceptually, we can say, lasso regression (L1) does both variable selection and parameter\\nshrinkage, whereas Ridge regression only does parameter shrinkage and end up including all\\nthe coefficients in the model. In presence of correlated variables, ridge regression might be\\nthe preferred choice. Also, ridge regression works best in situations where the least square\\nestimates have higher variance. Therefore, it depends on our model objective.\\n\\nKnow more: Ridge and Lasso Regression<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:47:42 [engine.py:317] Added request chatcmpl-82bedc16b544420f903d290b095bea74.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_3.txt...vLLM STDOUT: INFO:     127.0.0.1:50688 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:47:45 [logger.py:43] Received request chatcmpl-4d5471e6dbb44f8da67d4937d4b3186e: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nIn presence of correlated variables, ridge regression might be\\nthe preferred choice. Also, ridge regression works best in situations where the least square\\nestimates have higher variance. Therefore, it depends on our model objective.\\n\\nKnow more: Ridge and Lasso Regression\\n\\nQ17. Rise in global average temperature led to decrease in number of pirates around\\nthe world. Does that mean that decrease in number of pirates caused the climate\\nchange?\\n\\nAnswer: After reading this question, you should have understood that this is a classic case of\\n“causation and correlation”. No, we can’t conclude that decrease in number of pirates caused\\nthe climate change because there might be other factors (lurking or confounding variables)\\ninfluencing this phenomenon.\\n\\nTherefore, there might be a correlation between global average temperature and number of\\npirates, but based on this information we can’t say that pirated died because of rise in global\\naverage temperature.\\n\\nKnow more: Causation and Correlation<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:47:45 [engine.py:317] Added request chatcmpl-4d5471e6dbb44f8da67d4937d4b3186e.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_3.txt...vLLM STDOUT: INFO:     127.0.0.1:50696 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:47:48 [logger.py:43] Received request chatcmpl-7cf32370aa48481d866081e687c2243c: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nRise in global average temperature led to decrease in number of pirates around\\nthe world. Does that mean that decrease in number of pirates caused the climate\\nchange?\\n\\nAnswer: After reading this question, you should have understood that this is a classic case of\\n“causation and correlation”. No, we can’t conclude that decrease in number of pirates caused\\nthe climate change because there might be other factors (lurking or confounding variables)\\ninfluencing this phenomenon.\\n\\nTherefore, there might be a correlation between global average temperature and number of\\npirates, but based on this information we can’t say that pirated died because of rise in global\\naverage temperature.\\n\\nKnow more: Causation and Correlation\\n\\nQ18. While working on a data set, how do you select important variables? Explain your\\nmethods.\\n\\nAnswer: Following are the methods of variable selection you can use:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:47:48 [engine.py:317] Added request chatcmpl-7cf32370aa48481d866081e687c2243c.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_3.txt...vLLM STDOUT: INFO:     127.0.0.1:50698 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:47:51 [logger.py:43] Received request chatcmpl-ff78f38a3fd34e42850c39889f7fb76c: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nDoes that mean that decrease in number of pirates caused the climate\\nchange?\\n\\nAnswer: After reading this question, you should have understood that this is a classic case of\\n“causation and correlation”. No, we can’t conclude that decrease in number of pirates caused\\nthe climate change because there might be other factors (lurking or confounding variables)\\ninfluencing this phenomenon.\\n\\nTherefore, there might be a correlation between global average temperature and number of\\npirates, but based on this information we can’t say that pirated died because of rise in global\\naverage temperature.\\n\\nKnow more: Causation and Correlation\\n\\nQ18. While working on a data set, how do you select important variables? Explain your\\nmethods.\\n\\nAnswer: Following are the methods of variable selection you can use:\\n\\n1. Remove the correlated variables prior to selecting important variables\\n2. Use linear regression and select variables based on p values<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:47:51 [engine.py:317] Added request chatcmpl-ff78f38a3fd34e42850c39889f7fb76c.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_3.txt...vLLM STDOUT: INFO:     127.0.0.1:38432 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:47:55 [logger.py:43] Received request chatcmpl-fc75461f0403488bafaa0b7fe4c7f7d7: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nWhile working on a data set, how do you select important variables? Explain your\\nmethods.\\n\\nAnswer: Following are the methods of variable selection you can use:\\n\\n1. Remove the correlated variables prior to selecting important variables\\n2. Use linear regression and select variables based on p values\\n\\n \\n \\n \\n\\x0c3. Use Forward Selection, Backward Selection, Stepwise Selection\\n4. Use Random Forest, Xgboost and plot variable importance chart\\n5. Use Lasso Regression\\n6. Measure information gain for the available set of features and select top n features\\n\\naccordingly.\\n\\nQ19. What is the difference between covariance and correlation?\\n\\nAnswer: Correlation is the standardized form of covariance.\\n\\nCovariances are difficult to compare. For example: if we calculate the covariances of salary\\n($) and age (years), we’ll get different covariances which can’t be compared because of\\nhaving unequal scales. To combat such situation, we calculate correlation to get a value\\nbetween -1 and 1, irrespective of their respective scale.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:47:55 [engine.py:317] Added request chatcmpl-fc75461f0403488bafaa0b7fe4c7f7d7.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_3.txt...vLLM STDOUT: INFO:     127.0.0.1:38448 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:47:58 [logger.py:43] Received request chatcmpl-1dce01a0421b4b4988bfcb1d231fda05: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nWhat is the difference between covariance and correlation?\\n\\nAnswer: Correlation is the standardized form of covariance.\\n\\nCovariances are difficult to compare. For example: if we calculate the covariances of salary\\n($) and age (years), we’ll get different covariances which can’t be compared because of\\nhaving unequal scales. To combat such situation, we calculate correlation to get a value\\nbetween -1 and 1, irrespective of their respective scale.\\n\\nQ20. Is it possible capture the correlation between continuous and categorical variable?\\nIf yes, how?\\n\\nAnswer: Yes, we can use ANCOVA (analysis of covariance) technique to capture association\\nbetween continuous and categorical variables.\\n\\nQ21. Both being tree based algorithm, how is random forest different from Gradient\\nboosting algorithm (GBM)?\\n\\nAnswer: The fundamental difference is, random forest uses bagging technique to make\\npredictions. GBM uses boosting techniques to make predictions.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:47:58 [engine.py:317] Added request chatcmpl-1dce01a0421b4b4988bfcb1d231fda05.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_3.txt...vLLM STDOUT: INFO:     127.0.0.1:38464 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:48:01 [logger.py:43] Received request chatcmpl-04608eb360c34734b148e1b41c404a71: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nIs it possible capture the correlation between continuous and categorical variable?\\nIf yes, how?\\n\\nAnswer: Yes, we can use ANCOVA (analysis of covariance) technique to capture association\\nbetween continuous and categorical variables.\\n\\nQ21. Both being tree based algorithm, how is random forest different from Gradient\\nboosting algorithm (GBM)?\\n\\nAnswer: The fundamental difference is, random forest uses bagging technique to make\\npredictions. GBM uses boosting techniques to make predictions.\\n\\nIn bagging technique, a data set is divided into n samples using randomized sampling. Then,\\nusing a single learning algorithm a model is build on all samples. Later, the resultant\\npredictions are combined using voting or averaging. Bagging is done is parallel. In boosting,\\nafter the first round of predictions, the algorithm weighs misclassified predictions higher,\\nsuch that they can be corrected in the succeeding round. This sequential process of giving\\nhigher weights to misclassified predictions continue until a stopping criterion is reached.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:48:01 [engine.py:317] Added request chatcmpl-04608eb360c34734b148e1b41c404a71.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_3.txt...vLLM STDOUT: INFO:     127.0.0.1:38342 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:48:04 [logger.py:43] Received request chatcmpl-2181a325457641d6b425ae898b0c3b4f: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nBagging is done is parallel. In boosting,\\nafter the first round of predictions, the algorithm weighs misclassified predictions higher,\\nsuch that they can be corrected in the succeeding round. This sequential process of giving\\nhigher weights to misclassified predictions continue until a stopping criterion is reached.\\n\\nRandom forest improves model accuracy by reducing variance (mainly). The trees grown are\\nuncorrelated to maximize the decrease in variance. On the other hand, GBM improves\\naccuracy my reducing both bias and variance in a model.\\n\\nQ22. Running a binary classification tree algorithm is the easy part. Do you know how\\ndoes a tree splitting takes place i.e. how does the tree decide which variable to split at\\nthe root node and succeeding nodes?\\n\\n \\n \\n \\n\\x0cAnswer: A classification trees makes decision based on Gini Index and Node Entropy. In\\nsimple words, the tree algorithm find the best possible feature which can divide the data\\nset into purest possible children nodes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:48:04 [engine.py:317] Added request chatcmpl-2181a325457641d6b425ae898b0c3b4f.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_3.txt...vLLM STDOUT: INFO:     127.0.0.1:38348 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:48:07 [logger.py:43] Received request chatcmpl-2b59af09c8124744875411d1142a758b: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nDo you know how\\ndoes a tree splitting takes place i.e. how does the tree decide which variable to split at\\nthe root node and succeeding nodes?\\n\\n \\n \\n \\n\\x0cAnswer: A classification trees makes decision based on Gini Index and Node Entropy. In\\nsimple words, the tree algorithm find the best possible feature which can divide the data\\nset into purest possible children nodes.\\n\\nGini index says, if we select two items from a population at random then they must be of\\nsame class and probability for this is 1 if population is pure. We can calculate Gini as\\nfollowing:\\n\\n1. Calculate Gini for sub-nodes, using formula sum of<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:48:07 [engine.py:317] Added request chatcmpl-2b59af09c8124744875411d1142a758b.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_3.txt...vLLM STDOUT: INFO:     127.0.0.1:38352 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2KBatch processing complete._Questions_3.txt...\n\u001b[32m⠴\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KGenerated 20 QA pairs totalQuestions_3.txt...\n\u001b[32m⠴\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KSaving result to Interview_Questions_3.txt...\ndata/generated/Machine_Learning_Interview_Questions_3_qa_pairs.json\n\u001b[32m⠴\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n\u001b[32m⠴\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KSuccessfully wrote result to estions_3.txt...\ndata/generated/Machine_Learning_Interview_Questions_3_qa_pairs.json\n\u001b[32m⠴\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_3.txt...\n\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\n\u001b[1;32mdata/generated/Machine_Learning_Interview_Questions_3_qa_pairs.json\u001b[0m\nvLLM STDOUT: INFO:     127.0.0.1:40372 - \"GET /v1/models HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO:     127.0.0.1:40386 - \"GET /v1/models HTTP/1.1\" 200 OK\n\u001b[?25lvLLM STDOUT: INFO 11-10 10:48:13 [logger.py:43] Received request chatcmpl-a3c4fe2a0d9b4618b90611530a653f8c: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\npurest possible children nodes.\\n\\nGini index says, if we select two items from a population at random then they must be of\\nsame class and probability for this is 1 if population is pure. We can calculate Gini as\\nfollowing:\\n\\n1. Calculate Gini for sub-nodes, using formula sum of square of probability for success\\n\\nand failure (p^2+q^2).\\n\\n2. Calculate Gini for split using weighted Gini score of each node of that split\\n\\nEntropy is the measure of impurity as given by (for binary class):\\n\\nHere p and q is probability of success and failure respectively in that node. Entropy is zero\\nwhen a node is homogeneous. It is maximum when a both the classes are present in a node at\\n50% – 50%.  Lower entropy is desirable.\\n\\nQ23. You’ve built a random forest model with 10000 trees. You got delighted after\\ngetting training error as 0.00. But, the validation error is 34.23. What is going on?\\nHaven’t you trained your model perfectly?\\n\\nAnswer: The model has overfitted. Training error 0.00 means the classifier has mimiced the\\ntraining data patterns to an extent, that they are not available in the unseen data. Hence, when\\nit couldn’t find those patterns and returned\\nthis classifier was run on unseen sample,\\nprediction with higher error. In random forest, it happens when we use larger number of trees\\nthan necessary. Hence, to avoid these situation, we should tune number of trees using cross\\nvalidation.\\n\\nQ24. You’ve got a data set to work having p (no. of variable) > n (no. of observation).\\nWhy is OLS as bad option to work with? Which techniques would be best to use? Why?\\n\\nAnswer: In such high dimensional data sets, we can’t use classical regression techniques,\\nsince their assumptions tend to fail. When p > n, we can no longer calculate a unique least\\nsquare coefficient estimate, the variances become infinite, so OLS cannot be used at all.\\n\\nTo combat this situation, we can use penalized regression methods like lasso, LARS, ridge\\nwhich can shrink the coefficients to reduce variance. Precisely, ridge regression works best in\\nsituations where the least square estimates have higher variance.\\n\\nAmong other methods include subset regression, forward stepwise regression.\\n\\n \\n \\n \\n\\x0cQ25. What is convex hull? (Hint: Think SVM)\\n\\nAnswer: In case of linearly separable data, convex hull represents the outer boundaries of the\\ntwo group of data points. Once convex hull is created, we get maximum margin hyperplane\\n(MMH) as a perpendicular bisector between two convex hulls. MMH is the line which\\nattempts to create greatest separation between two groups.\\n\\nQ26. We know that one hot encoding increasing the dimensionality of a data set. But,\\nlabel encoding doesn’t. How?\\n\\nAnswer: Don’t get baffled at this question. It’s a simple question asking the difference\\nbetween the two.\\n\\nUsing one hot encoding, the dimensionality (a.k.a features) in a data set get increased because\\nit creates a new variable for each level present in categorical variables. For example: let’s say\\nwe have a variable ‘color’. The variable has 3 levels namely Red, Blue and Green. One hot\\nencoding ‘color’ variable will generate three new variables as Color.Red, Color.Blue and\\nColor.Green containing 0 and 1 value.\\n\\nIn label encoding, the levels of a categorical variables gets encoded as 0 and 1, so no new\\nvariable is created. Label encoding is majorly used for binary variables.\\n\\nQ27. What cross validation technique would you use on time series data set? Is it k-fold\\nor LOOCV?\\n\\nAnswer: Neither.\\n\\nIn time series problem, k fold can be troublesome because there might be some pattern in\\nyear 4 or 5 which is not in year 3. Resampling the data set will separate these trends, and we\\nmight end up validation on past years, which is incorrect. Instead, we can use forward\\nchaining strategy with 5 fold as shown below:\\n\\n● fold 1 : training [1], test [2]\\n\\n \\n \\n\\x0c● fold 2 : training [1 2], test [3]\\n● fold 3 : training [1 2 3], test [4]\\n● fold 4 : training [1 2 3 4], test [5]\\n● fold 5 : training [1 2 3 4 5], test [6]\\n\\nwhere 1,2,3,4,5,6 represents “year”.\\n\\nQ28. You are given a data set consisting of variables having more than 30% missing\\nvalues? Let’s say, out<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:48:13 [engine.py:317] Added request chatcmpl-a3c4fe2a0d9b4618b90611530a653f8c.\n\u001b[32m⠋\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_4.txt...vLLM STDOUT: INFO:     127.0.0.1:40402 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:48:19 [logger.py:43] Received request chatcmpl-a56c8ca527944e43995d842705e5e890: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n purest possible children nodes.\\n\\nGini index says, if we select two items from a population at random then they must be of\\nsame class and probability for this is 1 if population is pure. We can calculate Gini as\\nfollowing:\\n\\n1. Calculate Gini for sub-nodes, using formula sum of square of probability for success\\n\\nand failure (p^2+q^2).\\n\\n2. Calculate Gini for split using weighted Gini score of each node of that split\\n\\nEntropy is the measure of impurity as given by (for binary class):\\n\\nHere p and q is probability of success and failure respectively in that node. Entropy is zero\\nwhen a node is homogeneous. It is maximum when a both the classes are present in a node at\\n50% – 50%.  Lower entropy is desirable.\\n\\nQ23. You’ve built a random forest model with 10000 trees. You got delighted after\\ngetting training error as 0.00. But, the validation error is 34.23. What is going on?\\nHaven’t you trained your model perfectly?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n\u001b[2K\u001b[1A\u001b[2KProcessing 7 chunks to generate QA pairs...\n\u001b[32m⠋\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_4.txt...vLLM STDOUT: INFO 11-10 10:48:19 [engine.py:317] Added request chatcmpl-a56c8ca527944e43995d842705e5e890.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_4.txt...vLLM STDOUT: INFO:     127.0.0.1:40410 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:48:22 [logger.py:43] Received request chatcmpl-522af5daaa7f4cf68850f1cc2ae5da47: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nYou got delighted after\\ngetting training error as 0.00. But, the validation error is 34.23. What is going on?\\nHaven’t you trained your model perfectly?\\n\\nAnswer: The model has overfitted. Training error 0.00 means the classifier has mimiced the\\ntraining data patterns to an extent, that they are not available in the unseen data. Hence, when\\nit couldn’t find those patterns and returned\\nthis classifier was run on unseen sample,\\nprediction with higher error. In random forest, it happens when we use larger number of trees\\nthan necessary. Hence, to avoid these situation, we should tune number of trees using cross\\nvalidation.\\n\\nQ24. You’ve got a data set to work having p (no. of variable) > n (no. of observation).\\nWhy is OLS as bad option to work with? Which techniques would be best to use? Why?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from \ndata/output/Machine_Learning_Interview_Questions_4.txt...vLLM STDOUT: INFO 11-10 10:48:22 [engine.py:317] Added request chatcmpl-522af5daaa7f4cf68850f1cc2ae5da47.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_4.txt...vLLM STDOUT: INFO:     127.0.0.1:53344 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:48:26 [logger.py:43] Received request chatcmpl-e7300a1a54cb4ef6a8b3beb0fa7d1e12: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nYou’ve got a data set to work having p (no. of variable) > n (no. of observation).\\nWhy is OLS as bad option to work with? Which techniques would be best to use? Why?\\n\\nAnswer: In such high dimensional data sets, we can’t use classical regression techniques,\\nsince their assumptions tend to fail. When p > n, we can no longer calculate a unique least\\nsquare coefficient estimate, the variances become infinite, so OLS cannot be used at all.\\n\\nTo combat this situation, we can use penalized regression methods like lasso, LARS, ridge\\nwhich can shrink the coefficients to reduce variance. Precisely, ridge regression works best in\\nsituations where the least square estimates have higher variance.\\n\\nAmong other methods include subset regression, forward stepwise regression.\\n\\n \\n \\n \\n\\x0cQ25. What is convex hull? (Hint: Think SVM)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:48:26 [engine.py:317] Added request chatcmpl-e7300a1a54cb4ef6a8b3beb0fa7d1e12.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_4.txt...vLLM STDOUT: INFO:     127.0.0.1:53354 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:48:33 [logger.py:43] Received request chatcmpl-f171ae7498154bf0ba09e4b5f9faf838: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nWhen p > n, we can no longer calculate a unique least\\nsquare coefficient estimate, the variances become infinite, so OLS cannot be used at all.\\n\\nTo combat this situation, we can use penalized regression methods like lasso, LARS, ridge\\nwhich can shrink the coefficients to reduce variance. Precisely, ridge regression works best in\\nsituations where the least square estimates have higher variance.\\n\\nAmong other methods include subset regression, forward stepwise regression.\\n\\n \\n \\n \\n\\x0cQ25. What is convex hull? (Hint: Think SVM)\\n\\nAnswer: In case of linearly separable data, convex hull represents the outer boundaries of the\\ntwo group of data points. Once convex hull is created, we get maximum margin hyperplane\\n(MMH) as a perpendicular bisector between two convex hulls. MMH is the line which\\nattempts to create greatest separation between two groups.\\n\\nQ26. We know that one hot encoding increasing the dimensionality of a data set. But,\\nlabel encoding doesn’t. How?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from \ndata/output/Machine_Learning_Interview_Questions_4.txt...vLLM STDOUT: INFO 11-10 10:48:33 [engine.py:317] Added request chatcmpl-f171ae7498154bf0ba09e4b5f9faf838.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_4.txt...vLLM STDOUT: INFO:     127.0.0.1:51844 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:48:40 [logger.py:43] Received request chatcmpl-45bd040e38174b47a2648bac02aa08eb: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nWe know that one hot encoding increasing the dimensionality of a data set. But,\\nlabel encoding doesn’t. How?\\n\\nAnswer: Don’t get baffled at this question. It’s a simple question asking the difference\\nbetween the two.\\n\\nUsing one hot encoding, the dimensionality (a.k.a features) in a data set get increased because\\nit creates a new variable for each level present in categorical variables. For example: let’s say\\nwe have a variable ‘color’. The variable has 3 levels namely Red, Blue and Green. One hot\\nencoding ‘color’ variable will generate three new variables as Color.Red, Color.Blue and\\nColor.Green containing 0 and 1 value.\\n\\nIn label encoding, the levels of a categorical variables gets encoded as 0 and 1, so no new\\nvariable is created. Label encoding is majorly used for binary variables.\\n\\nQ27. What cross validation technique would you use on time series data set? Is it k-fold\\nor LOOCV?\\n\\nAnswer: Neither.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from \ndata/output/Machine_Learning_Interview_Questions_4.txt...vLLM STDOUT: INFO 11-10 10:48:40 [engine.py:317] Added request chatcmpl-45bd040e38174b47a2648bac02aa08eb.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_4.txt...vLLM STDOUT: INFO:     127.0.0.1:37298 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:48:43 [logger.py:43] Received request chatcmpl-b1629f9d335e4414b91c8c83c96488a7: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nOne hot\\nencoding ‘color’ variable will generate three new variables as Color.Red, Color.Blue and\\nColor.Green containing 0 and 1 value.\\n\\nIn label encoding, the levels of a categorical variables gets encoded as 0 and 1, so no new\\nvariable is created. Label encoding is majorly used for binary variables.\\n\\nQ27. What cross validation technique would you use on time series data set? Is it k-fold\\nor LOOCV?\\n\\nAnswer: Neither.\\n\\nIn time series problem, k fold can be troublesome because there might be some pattern in\\nyear 4 or 5 which is not in year 3. Resampling the data set will separate these trends, and we\\nmight end up validation on past years, which is incorrect. Instead, we can use forward\\nchaining strategy with 5 fold as shown below:\\n\\n● fold 1 : training [1], test [2]\\n\\n \\n \\n\\x0c● fold 2 : training [1 2], test [3]\\n● fold 3 : training [1 2 3], test [4]\\n● fold 4 : training [1 2 3 4], test [5]\\n● fold 5 : training [1 2 3 4 5], test [6]\\n\\nwhere 1,2,3,4,5,6 represents “year”.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:48:43 [engine.py:317] Added request chatcmpl-b1629f9d335e4414b91c8c83c96488a7.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_4.txt...vLLM STDOUT: INFO:     127.0.0.1:37304 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:48:47 [logger.py:43] Received request chatcmpl-a10a600e807142b3b292dbae94675b11: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nWhat cross validation technique would you use on time series data set? Is it k-fold\\nor LOOCV?\\n\\nAnswer: Neither.\\n\\nIn time series problem, k fold can be troublesome because there might be some pattern in\\nyear 4 or 5 which is not in year 3. Resampling the data set will separate these trends, and we\\nmight end up validation on past years, which is incorrect. Instead, we can use forward\\nchaining strategy with 5 fold as shown below:\\n\\n● fold 1 : training [1], test [2]\\n\\n \\n \\n\\x0c● fold 2 : training [1 2], test [3]\\n● fold 3 : training [1 2 3], test [4]\\n● fold 4 : training [1 2 3 4], test [5]\\n● fold 5 : training [1 2 3 4 5], test [6]\\n\\nwhere 1,2,3,4,5,6 represents “year”.\\n\\nQ28. You are given a data set consisting of variables having more than 30% missing\\nvalues? Let’s say, out<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from dded request chatcmpl-a10a600e807142b3b292dbae94675b11.\ndata/output/Machine_Learning_Interview_Questions_4.txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_4.txt...vLLM STDOUT: INFO:     127.0.0.1:37306 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KBatch processing complete._Questions_4.txt...\n\u001b[32m⠏\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KGenerated 16 QA pairs totalQuestions_4.txt...\n\u001b[32m⠏\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KSaving result to Interview_Questions_4.txt...\ndata/generated/Machine_Learning_Interview_Questions_4_qa_pairs.json\n\u001b[32m⠏\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n\u001b[32m⠏\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KSuccessfully wrote result to estions_4.txt...\ndata/generated/Machine_Learning_Interview_Questions_4_qa_pairs.json\n\u001b[32m⠏\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_4.txt...\n\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\n\u001b[1;32mdata/generated/Machine_Learning_Interview_Questions_4_qa_pairs.json\u001b[0m\nvLLM STDOUT: INFO:     127.0.0.1:37472 - \"GET /v1/models HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO:     127.0.0.1:37480 - \"GET /v1/models HTTP/1.1\" 200 OK\n\u001b[?25lvLLM STDOUT: INFO 11-10 10:48:52 [logger.py:43] Received request chatcmpl-ab3b466783294eddb8368e76cc9b9ccb: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nfold 5 : training [1 2 3 4 5], test [6]\\n\\nwhere 1,2,3,4,5,6 represents “year”.\\n\\nQ28. You are given a data set consisting of variables having more than 30% missing\\nvalues? Let’s say, out of 50 variables, 8 variables have missing values higher than 30%.\\nHow will you deal with them?\\n\\nAnswer: We can deal with them in the following ways:\\n\\n1. Assign a unique category to missing values, who knows the missing values might\\n\\ndecipher some trend\\n\\n2. We can remove them blatantly.\\n3. Or, we can sensibly check their distribution with the target variable, and if found any\\nnew\\n\\nthose missing\\n\\nassign them a\\n\\nvalues and\\n\\nkeep\\n\\npattern we’ll\\ncategory while removing others.\\n\\n29. ‘People who bought this, also bought…’ recommendations seen on amazon is a\\nresult of which algorithm?\\n\\nAnswer: The basic idea for this kind of recommendation engine comes from collaborative\\nfiltering.\\n\\nCollaborative Filtering algorithm considers “User Behavior” for recommending items. They\\nexploit behavior of other users and items in terms of transaction history, ratings, selection and\\npurchase information. Other users behaviour and preferences over the items are used to\\nrecommend items to the new users. In this case, features of the items are not known.\\n\\nKnow more: Recommender System\\n\\nQ30. What do you understand by Type I vs Type II error?\\n\\nAnswer: Type I error is committed when the null hypothesis is true and we reject it, also\\nknown as a ‘False Positive’. Type II error is committed when the null hypothesis is false and\\nwe accept it, also known as ‘False Negative’.\\n\\nIn the context of confusion matrix, we can say Type I error occurs when we classify a value\\nas positive (1) when it is actually negative (0). Type II error occurs when we classify a value\\nas negative (0) when it is actually positive(1).\\n\\n \\n \\n \\n \\n\\x0cQ31. You are working on a classification problem. For validation purposes, you’ve\\nrandomly sampled the training data set into train and validation. You are confident that\\nyour model will work incredibly well on unseen data since your validation accuracy is\\nhigh. However, you get shocked after getting poor test accuracy. What went wrong?\\n\\nAnswer: In case of classification problem, we should always use stratified sampling instead\\nof random sampling. A random sampling doesn’t takes into consideration the proportion of\\ntarget classes. On the contrary, stratified sampling helps to maintain the distribution of target\\nvariable in the resultant distributed samples also.\\n\\nQ32. You have been asked to evaluate a regression model based on R², adjusted R² and\\ntolerance. What will be your criteria?\\n\\nAnswer: Tolerance (1 / VIF) is used as an indicator of multicollinearity. It is an indicator of\\npercent of variance in a predictor which cannot be accounted by other predictors. Large\\nvalues of tolerance is desirable.\\n\\nWe will consider adjusted R² as opposed to R² to evaluate model fit because R² increases\\nirrespective of improvement in prediction accuracy as we add more variables. But, adjusted\\nR² would only increase if an additional variable improves the accuracy of model, otherwise\\nstays same. It is difficult to commit a general threshold value for adjusted R² because it\\nvaries between data sets. For example: a gene mutation data set might result in lower adjusted\\nR² and still provide fairly good predictions, as compared to a stock market data\\nwhere lower adjusted R² implies that model is not good.\\n\\nQ33. In k-means or kNN, we use euclidean distance to calculate the distance between\\nnearest neighbors. Why not manhattan distance?\\n\\nAnswer: We don’t use manhattan distance because it calculates distance horizontally or\\nvertically only. It has dimension restrictions. On the other hand, euclidean metric can be used\\nin any space to calculate distance. Since, the data points can be present in any dimension,\\neuclidean distance is a more viable option.\\n\\nExample: Think of a chess board, the movement made by a bishop or a rook is calculated by\\nmanhattan distance because of their respective vertical & horizontal movements.\\n\\nQ34. Explain machine learning to me like a 5 year old.\\n\\nAnswer: It’s simple. It’s just like how babies learn to walk. Every time they fall down, they\\nlearn (unconsciously) & realize that their legs should be straight and not in a bend position.\\nThe next time they fall down, they feel pain. They cry. But, they learn ‘not to stand like that\\nagain’. In order to avoid that pain, they try harder. To succeed, they even seek support from\\nthe door or wall or anything near them, which helps them stand<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:48:52 [engine.py:317] Added request chatcmpl-ab3b466783294eddb8368e76cc9b9ccb.\n\u001b[32m⠋\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_5.txt...vLLM STDOUT: INFO:     127.0.0.1:37484 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:48:57 [logger.py:43] Received request chatcmpl-fe4720af6f144ffdbd1fdea02868b737: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n fold 5 : training [1 2 3 4 5], test [6]\\n\\nwhere 1,2,3,4,5,6 represents “year”.\\n\\nQ28. You are given a data set consisting of variables having more than 30% missing\\nvalues? Let’s say, out of 50 variables, 8 variables have missing values higher than 30%.\\nHow will you deal with them?\\n\\nAnswer: We can deal with them in the following ways:\\n\\n1. Assign a unique category to missing values, who knows the missing values might\\n\\ndecipher some trend\\n\\n2. We can remove them blatantly.\\n3. Or, we can sensibly check their distribution with the target variable, and if found any\\nnew\\n\\nthose missing\\n\\nassign them a\\n\\nvalues and\\n\\nkeep\\n\\npattern we’ll\\ncategory while removing others.\\n\\n29. ‘People who bought this, also bought…’ recommendations seen on amazon is a\\nresult of which algorithm?\\n\\nAnswer: The basic idea for this kind of recommendation engine comes from collaborative\\nfiltering.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:48:57 [engine.py:317] Added request chatcmpl-fe4720af6f144ffdbd1fdea02868b737.\n\u001b[2K\u001b[1A\u001b[2KProcessing 8 chunks to generate QA pairs...\n\u001b[32m⠋\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_5.txt...vLLM STDOUT: INFO:     127.0.0.1:37488 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:49:00 [logger.py:43] Received request chatcmpl-909464fceb974585adaf7eadcdd37a26: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nWe can remove them blatantly.\\n3. Or, we can sensibly check their distribution with the target variable, and if found any\\nnew\\n\\nthose missing\\n\\nassign them a\\n\\nvalues and\\n\\nkeep\\n\\npattern we’ll\\ncategory while removing others.\\n\\n29. ‘People who bought this, also bought…’ recommendations seen on amazon is a\\nresult of which algorithm?\\n\\nAnswer: The basic idea for this kind of recommendation engine comes from collaborative\\nfiltering.\\n\\nCollaborative Filtering algorithm considers “User Behavior” for recommending items. They\\nexploit behavior of other users and items in terms of transaction history, ratings, selection and\\npurchase information. Other users behaviour and preferences over the items are used to\\nrecommend items to the new users. In this case, features of the items are not known.\\n\\nKnow more: Recommender System\\n\\nQ30. What do you understand by Type I vs Type II error?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:49:00 [engine.py:317] Added request chatcmpl-909464fceb974585adaf7eadcdd37a26.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_5.txt...vLLM STDOUT: INFO:     127.0.0.1:34582 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:49:03 [logger.py:43] Received request chatcmpl-db46633f12154b6da8bda04ef36cf05c: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nOther users behaviour and preferences over the items are used to\\nrecommend items to the new users. In this case, features of the items are not known.\\n\\nKnow more: Recommender System\\n\\nQ30. What do you understand by Type I vs Type II error?\\n\\nAnswer: Type I error is committed when the null hypothesis is true and we reject it, also\\nknown as a ‘False Positive’. Type II error is committed when the null hypothesis is false and\\nwe accept it, also known as ‘False Negative’.\\n\\nIn the context of confusion matrix, we can say Type I error occurs when we classify a value\\nas positive (1) when it is actually negative (0). Type II error occurs when we classify a value\\nas negative (0) when it is actually positive(1).<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from \ndata/output/Machine_Learning_Interview_Questions_5.txt...vLLM STDOUT: INFO 11-10 10:49:03 [engine.py:317] Added request chatcmpl-db46633f12154b6da8bda04ef36cf05c.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_5.txt...vLLM STDOUT: INFO:     127.0.0.1:34596 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:49:06 [logger.py:43] Received request chatcmpl-35f2139e77d24d898d142d19674d5d57: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nWhat do you understand by Type I vs Type II error?\\n\\nAnswer: Type I error is committed when the null hypothesis is true and we reject it, also\\nknown as a ‘False Positive’. Type II error is committed when the null hypothesis is false and\\nwe accept it, also known as ‘False Negative’.\\n\\nIn the context of confusion matrix, we can say Type I error occurs when we classify a value\\nas positive (1) when it is actually negative (0). Type II error occurs when we classify a value\\nas negative (0) when it is actually positive(1).\\n\\n \\n \\n \\n \\n\\x0cQ31. You are working on a classification problem. For validation purposes, you’ve\\nrandomly sampled the training data set into train and validation. You are confident that\\nyour model will work incredibly well on unseen data since your validation accuracy is\\nhigh. However, you get shocked after getting poor test accuracy. What went wrong?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from \ndata/output/Machine_Learning_Interview_Questions_5.txt...vLLM STDOUT: INFO 11-10 10:49:06 [engine.py:317] Added request chatcmpl-35f2139e77d24d898d142d19674d5d57.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_5.txt...vLLM STDOUT: INFO:     127.0.0.1:34612 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from \ndata/output/Machine_Learning_Interview_Questions_5.txt...vLLM STDOUT: INFO 11-10 10:49:11 [logger.py:43] Received request chatcmpl-56353956508649f281190398cdf29245: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nYou are confident that\\nyour model will work incredibly well on unseen data since your validation accuracy is\\nhigh. However, you get shocked after getting poor test accuracy. What went wrong?\\n\\nAnswer: In case of classification problem, we should always use stratified sampling instead\\nof random sampling. A random sampling doesn’t takes into consideration the proportion of\\ntarget classes. On the contrary, stratified sampling helps to maintain the distribution of target\\nvariable in the resultant distributed samples also.\\n\\nQ32. You have been asked to evaluate a regression model based on R², adjusted R² and\\ntolerance. What will be your criteria?\\n\\nAnswer: Tolerance (1 / VIF) is used as an indicator of multicollinearity. It is an indicator of\\npercent of variance in a predictor which cannot be accounted by other predictors. Large\\nvalues of tolerance is desirable.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:49:11 [engine.py:317] Added request chatcmpl-56353956508649f281190398cdf29245.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_5.txt...vLLM STDOUT: INFO:     127.0.0.1:38816 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:49:14 [logger.py:43] Received request chatcmpl-706fcd0136c245929aa3283cb5f4bcbc: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nWhat will be your criteria?\\n\\nAnswer: Tolerance (1 / VIF) is used as an indicator of multicollinearity. It is an indicator of\\npercent of variance in a predictor which cannot be accounted by other predictors. Large\\nvalues of tolerance is desirable.\\n\\nWe will consider adjusted R² as opposed to R² to evaluate model fit because R² increases\\nirrespective of improvement in prediction accuracy as we add more variables. But, adjusted\\nR² would only increase if an additional variable improves the accuracy of model, otherwise\\nstays same. It is difficult to commit a general threshold value for adjusted R² because it\\nvaries between data sets. For example: a gene mutation data set might result in lower adjusted\\nR² and still provide fairly good predictions, as compared to a stock market data\\nwhere lower adjusted R² implies that model is not good.\\n\\nQ33. In k-means or kNN, we use euclidean distance to calculate the distance between\\nnearest neighbors. Why not manhattan distance?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:49:14 [engine.py:317] Added request chatcmpl-706fcd0136c245929aa3283cb5f4bcbc.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_5.txt...vLLM STDOUT: INFO:     127.0.0.1:38818 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:49:17 [logger.py:43] Received request chatcmpl-d5c4f41ff7cb4534a8cf69abfe5ebd49: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nFor example: a gene mutation data set might result in lower adjusted\\nR² and still provide fairly good predictions, as compared to a stock market data\\nwhere lower adjusted R² implies that model is not good.\\n\\nQ33. In k-means or kNN, we use euclidean distance to calculate the distance between\\nnearest neighbors. Why not manhattan distance?\\n\\nAnswer: We don’t use manhattan distance because it calculates distance horizontally or\\nvertically only. It has dimension restrictions. On the other hand, euclidean metric can be used\\nin any space to calculate distance. Since, the data points can be present in any dimension,\\neuclidean distance is a more viable option.\\n\\nExample: Think of a chess board, the movement made by a bishop or a rook is calculated by\\nmanhattan distance because of their respective vertical & horizontal movements.\\n\\nQ34. Explain machine learning to me like a 5 year old.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from \ndata/output/Machine_Learning_Interview_Questions_5.txt...vLLM STDOUT: INFO 11-10 10:49:17 [engine.py:317] Added request chatcmpl-d5c4f41ff7cb4534a8cf69abfe5ebd49.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_5.txt...vLLM STDOUT: INFO:     127.0.0.1:38822 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:49:21 [logger.py:43] Received request chatcmpl-1205c6bfc76248d1b5769ea044579dc4: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nOn the other hand, euclidean metric can be used\\nin any space to calculate distance. Since, the data points can be present in any dimension,\\neuclidean distance is a more viable option.\\n\\nExample: Think of a chess board, the movement made by a bishop or a rook is calculated by\\nmanhattan distance because of their respective vertical & horizontal movements.\\n\\nQ34. Explain machine learning to me like a 5 year old.\\n\\nAnswer: It’s simple. It’s just like how babies learn to walk. Every time they fall down, they\\nlearn (unconsciously) & realize that their legs should be straight and not in a bend position.\\nThe next time they fall down, they feel pain. They cry. But, they learn ‘not to stand like that\\nagain’. In order to avoid that pain, they try harder. To succeed, they even seek support from\\nthe door or wall or anything near them, which helps them stand<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:49:21 [engine.py:317] Added request chatcmpl-1205c6bfc76248d1b5769ea044579dc4.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_5.txt...vLLM STDOUT: INFO:     127.0.0.1:48074 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2KBatch processing complete._Questions_5.txt...\n\u001b[32m⠹\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KGenerated 17 QA pairs totalQuestions_5.txt...\n\u001b[32m⠹\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KSaving result to Interview_Questions_5.txt...\ndata/generated/Machine_Learning_Interview_Questions_5_qa_pairs.json\n\u001b[32m⠹\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n\u001b[32m⠹\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KSuccessfully wrote result to estions_5.txt...\ndata/generated/Machine_Learning_Interview_Questions_5_qa_pairs.json\n\u001b[32m⠹\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_5.txt...\n\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\n\u001b[1;32mdata/generated/Machine_Learning_Interview_Questions_5_qa_pairs.json\u001b[0m\nvLLM STDOUT: INFO:     127.0.0.1:48090 - \"GET /v1/models HTTP/1.1\" 200 OK\n\u001b[?25lvLLM STDOUT: INFO:     127.0.0.1:48102 - \"GET /v1/models HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:49:26 [logger.py:43] Received request chatcmpl-c8b1c2a3e8de41cc8005b5cbd8bd5783: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\na bend position.\\nThe next time they fall down, they feel pain. They cry. But, they learn ‘not to stand like that\\nagain’. In order to avoid that pain, they try harder. To succeed, they even seek support from\\nthe door or wall or anything near them, which helps them stand firm.\\n\\n \\n \\n \\n\\x0cThis is how a machine works & develops intuition from its environment.\\n\\nNote: The interview is only trying to test if have the ability of explain complex concepts in\\nsimple terms.\\n\\nQ35. I know that a linear regression model is generally evaluated using Adjusted R² or\\nF value. How would you evaluate a logistic regression model?\\n\\nAnswer: We can use the following methods:\\n\\n1. Since logistic regression is used to predict probabilities, we can use AUC-ROC curve\\n\\nalong with confusion matrix to determine its performance.\\n\\n2. Also, the analogous metric of adjusted R² in logistic regression is AIC. AIC is the\\nmeasure of fit which penalizes model for the number of model coefficients. Therefore,\\nwe always prefer model with minimum AIC value.\\n\\n3. Null Deviance indicates the response predicted by a model with nothing but an\\nintercept. Lower the value, better the model. Residual deviance indicates the response\\npredicted by a model on adding independent variables. Lower the value, better the\\nmodel.\\n\\nKnow more: Logistic Regression\\n\\nQ36. Considering the long list of machine learning algorithm, given a data set, how do\\nyou decide which one to use?\\n\\nAnswer: You should say, the choice of machine learning algorithm solely depends of the type\\nof data. If you are given a data set which is exhibits linearity, then linear regression would be\\nthe best algorithm to use. If you given to work on images, audios, then neural network would\\nhelp you to build a robust model.\\n\\nIf the data comprises of non linear interactions, then a boosting or bagging algorithm should\\nbe the choice. If the business requirement is to build a model which can be deployed, then\\nwe’ll use regression or a decision tree model (easy to interpret and explain) instead of black\\nbox algorithms like SVM, GBM etc.\\n\\nIn short, there is no one master algorithm for all situations. We must be scrupulous enough to\\nunderstand which algorithm to use.\\n\\nQ37. Do you suggest that treating a categorical variable as continuous variable would\\nresult in a better predictive model?\\n\\nAnswer: For better predictions, categorical variable can be considered as a continuous\\nvariable only when the variable is ordinal in nature.\\n\\n \\n \\n \\n\\x0cQ38. When does regularization becomes necessary in Machine Learning?\\n\\nAnswer: Regularization becomes necessary when the model begins to ovefit / underfit. This\\ntechnique introduces a cost term for bringing in more features with the objective function.\\nHence, it tries to push the coefficients for many variables to zero and hence reduce cost term.\\nThis helps to reduce model complexity so that the model can become better at predicting\\n(generalizing).\\n\\nQ39. What do you understand by Bias Variance trade off?\\n\\nAnswer:  The error emerging from any model can be broken down into three components\\nmathematically. Following are these component :\\n\\nBias error is useful to quantify how much on an average are the predicted values different\\nfrom the actual value. A high bias error means we have a under-performing model which\\nkeeps on missing important\\ntrends. Variance on the other side quantifies how are the\\nprediction made on same observation different from each other. A high variance model will\\nover-fit on your training population and perform badly on any observation beyond training.\\n\\nQ40. OLS is to linear regression. Maximum likelihood is to logistic regression. Explain\\nthe statement.\\n\\nAnswer: OLS and Maximum likelihood are the methods used by the respective regression\\nmethods to approximate the unknown parameter (coefficient) value. In simple words,\\n\\nOrdinary least square(OLS) is a method used in linear regression which approximates the\\nparameters resulting in minimum distance between actual and predicted values. Maximum\\nLikelihood helps in choosing the the values of parameters which maximizes the likelihood\\nthat the parameters are most likely to produce observed data.\\n\\nEnd Notes\\n\\nYou might have been able to answer all the questions, but the real value is in understanding\\nthem and generalizing your knowledge on similar questions. If you have struggled at these\\n\\n \\n \\n \\n \\n\\x0cquestions, no worries, now is the time to learn and not perform. You should right now focus\\non learning these topics scrupulously.\\n\\nThese questions are meant to give you a wide exposure on the types of questions asked at\\nstartups in machine learning. I’m sure these questions would leave you curious enough to do\\ndeeper topic research at your end. If you are planning for it, that’s a good sign.\\n\\nJoin for more: telegram.me/datasciencefree<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:49:26 [engine.py:317] Added request chatcmpl-c8b1c2a3e8de41cc8005b5cbd8bd5783.\n\u001b[32m⠋\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_6.txt...vLLM STDOUT: INFO:     127.0.0.1:48110 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:49:31 [logger.py:43] Received request chatcmpl-7922a8fce6f342fc8ee17df3dda466af: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n a bend position.\\nThe next time they fall down, they feel pain. They cry. But, they learn ‘not to stand like that\\nagain’. In order to avoid that pain, they try harder. To succeed, they even seek support from\\nthe door or wall or anything near them, which helps them stand firm.\\n\\n \\n \\n \\n\\x0cThis is how a machine works & develops intuition from its environment.\\n\\nNote: The interview is only trying to test if have the ability of explain complex concepts in\\nsimple terms.\\n\\nQ35. I know that a linear regression model is generally evaluated using Adjusted R² or\\nF value. How would you evaluate a logistic regression model?\\n\\nAnswer: We can use the following methods:\\n\\n1. Since logistic regression is used to predict probabilities, we can use AUC-ROC curve\\n\\nalong with confusion matrix to determine its performance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:49:31 [engine.py:317] Added request chatcmpl-7922a8fce6f342fc8ee17df3dda466af.\n\u001b[2K\u001b[1A\u001b[2KProcessing 9 chunks to generate QA pairs...\n\u001b[32m⠧\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_6.txt...vLLM STDOUT: INFO:     127.0.0.1:46948 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:49:34 [logger.py:43] Received request chatcmpl-8dcb2f0297bd41a29025123013c5851c: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nI know that a linear regression model is generally evaluated using Adjusted R² or\\nF value. How would you evaluate a logistic regression model?\\n\\nAnswer: We can use the following methods:\\n\\n1. Since logistic regression is used to predict probabilities, we can use AUC-ROC curve\\n\\nalong with confusion matrix to determine its performance.\\n\\n2. Also, the analogous metric of adjusted R² in logistic regression is AIC. AIC is the\\nmeasure of fit which penalizes model for the number of model coefficients. Therefore,\\nwe always prefer model with minimum AIC value.\\n\\n3. Null Deviance indicates the response predicted by a model with nothing but an\\nintercept. Lower the value, better the model. Residual deviance indicates the response\\npredicted by a model on adding independent variables. Lower the value, better the\\nmodel.\\n\\nKnow more: Logistic Regression\\n\\nQ36. Considering the long list of machine learning algorithm, given a data set, how do\\nyou decide which one to use?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:49:34 [engine.py:317] Added request chatcmpl-8dcb2f0297bd41a29025123013c5851c.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_6.txt...vLLM STDOUT: INFO:     127.0.0.1:46964 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:49:36 [logger.py:43] Received request chatcmpl-667cbd73ca2e4b56acb58b70b827c590: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nResidual deviance indicates the response\\npredicted by a model on adding independent variables. Lower the value, better the\\nmodel.\\n\\nKnow more: Logistic Regression\\n\\nQ36. Considering the long list of machine learning algorithm, given a data set, how do\\nyou decide which one to use?\\n\\nAnswer: You should say, the choice of machine learning algorithm solely depends of the type\\nof data. If you are given a data set which is exhibits linearity, then linear regression would be\\nthe best algorithm to use. If you given to work on images, audios, then neural network would\\nhelp you to build a robust model.\\n\\nIf the data comprises of non linear interactions, then a boosting or bagging algorithm should\\nbe the choice. If the business requirement is to build a model which can be deployed, then\\nwe’ll use regression or a decision tree model (easy to interpret and explain) instead of black\\nbox algorithms like SVM, GBM etc.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:49:36 [engine.py:317] Added request chatcmpl-667cbd73ca2e4b56acb58b70b827c590.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_6.txt...vLLM STDOUT: INFO:     127.0.0.1:46980 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:49:39 [logger.py:43] Received request chatcmpl-388023837b2a4c59ba2d0868141cdb7c: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nIf you are given a data set which is exhibits linearity, then linear regression would be\\nthe best algorithm to use. If you given to work on images, audios, then neural network would\\nhelp you to build a robust model.\\n\\nIf the data comprises of non linear interactions, then a boosting or bagging algorithm should\\nbe the choice. If the business requirement is to build a model which can be deployed, then\\nwe’ll use regression or a decision tree model (easy to interpret and explain) instead of black\\nbox algorithms like SVM, GBM etc.\\n\\nIn short, there is no one master algorithm for all situations. We must be scrupulous enough to\\nunderstand which algorithm to use.\\n\\nQ37. Do you suggest that treating a categorical variable as continuous variable would\\nresult in a better predictive model?\\n\\nAnswer: For better predictions, categorical variable can be considered as a continuous\\nvariable only when the variable is ordinal in nature.\\n\\n \\n \\n \\n\\x0cQ38. When does regularization becomes necessary in Machine Learning?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:49:39 [engine.py:317] Added request chatcmpl-388023837b2a4c59ba2d0868141cdb7c.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_6.txt...vLLM STDOUT: INFO:     127.0.0.1:46988 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:49:41 [logger.py:43] Received request chatcmpl-a84f78ef65054920a97b1fa5ee0b618a: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nWe must be scrupulous enough to\\nunderstand which algorithm to use.\\n\\nQ37. Do you suggest that treating a categorical variable as continuous variable would\\nresult in a better predictive model?\\n\\nAnswer: For better predictions, categorical variable can be considered as a continuous\\nvariable only when the variable is ordinal in nature.\\n\\n \\n \\n \\n\\x0cQ38. When does regularization becomes necessary in Machine Learning?\\n\\nAnswer: Regularization becomes necessary when the model begins to ovefit / underfit. This\\ntechnique introduces a cost term for bringing in more features with the objective function.\\nHence, it tries to push the coefficients for many variables to zero and hence reduce cost term.\\nThis helps to reduce model complexity so that the model can become better at predicting\\n(generalizing).\\n\\nQ39. What do you understand by Bias Variance trade off?\\n\\nAnswer:  The error emerging from any model can be broken down into three components\\nmathematically. Following are these component :<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:49:41 [engine.py:317] Added request chatcmpl-a84f78ef65054920a97b1fa5ee0b618a.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_6.txt...vLLM STDOUT: INFO:     127.0.0.1:47120 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:49:46 [logger.py:43] Received request chatcmpl-261eaad7be5a48d1af09f131f902bd83: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nThis\\ntechnique introduces a cost term for bringing in more features with the objective function.\\nHence, it tries to push the coefficients for many variables to zero and hence reduce cost term.\\nThis helps to reduce model complexity so that the model can become better at predicting\\n(generalizing).\\n\\nQ39. What do you understand by Bias Variance trade off?\\n\\nAnswer:  The error emerging from any model can be broken down into three components\\nmathematically. Following are these component :\\n\\nBias error is useful to quantify how much on an average are the predicted values different\\nfrom the actual value. A high bias error means we have a under-performing model which\\nkeeps on missing important\\ntrends. Variance on the other side quantifies how are the\\nprediction made on same observation different from each other. A high variance model will\\nover-fit on your training population and perform badly on any observation beyond training.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:49:46 [engine.py:317] Added request chatcmpl-261eaad7be5a48d1af09f131f902bd83.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_6.txt...vLLM STDOUT: INFO:     127.0.0.1:47124 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:49:48 [logger.py:43] Received request chatcmpl-52898def5a514b46b3172c39b076aafc: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nA high bias error means we have a under-performing model which\\nkeeps on missing important\\ntrends. Variance on the other side quantifies how are the\\nprediction made on same observation different from each other. A high variance model will\\nover-fit on your training population and perform badly on any observation beyond training.\\n\\nQ40. OLS is to linear regression. Maximum likelihood is to logistic regression. Explain\\nthe statement.\\n\\nAnswer: OLS and Maximum likelihood are the methods used by the respective regression\\nmethods to approximate the unknown parameter (coefficient) value. In simple words,\\n\\nOrdinary least square(OLS) is a method used in linear regression which approximates the\\nparameters resulting in minimum distance between actual and predicted values. Maximum\\nLikelihood helps in choosing the the values of parameters which maximizes the likelihood\\nthat the parameters are most likely to produce observed data.\\n\\nEnd Notes<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from \ndata/output/Machine_Learning_Interview_Questions_6.txt...vLLM STDOUT: INFO 11-10 10:49:48 [engine.py:317] Added request chatcmpl-52898def5a514b46b3172c39b076aafc.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_6.txt...vLLM STDOUT: INFO:     127.0.0.1:47134 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:49:52 [logger.py:43] Received request chatcmpl-672b94542b6b4afcb8e9e9348fe8b7df: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nExplain\\nthe statement.\\n\\nAnswer: OLS and Maximum likelihood are the methods used by the respective regression\\nmethods to approximate the unknown parameter (coefficient) value. In simple words,\\n\\nOrdinary least square(OLS) is a method used in linear regression which approximates the\\nparameters resulting in minimum distance between actual and predicted values. Maximum\\nLikelihood helps in choosing the the values of parameters which maximizes the likelihood\\nthat the parameters are most likely to produce observed data.\\n\\nEnd Notes\\n\\nYou might have been able to answer all the questions, but the real value is in understanding\\nthem and generalizing your knowledge on similar questions. If you have struggled at these\\n\\n \\n \\n \\n \\n\\x0cquestions, no worries, now is the time to learn and not perform. You should right now focus\\non learning these topics scrupulously.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 10:49:52 [engine.py:317] Added request chatcmpl-672b94542b6b4afcb8e9e9348fe8b7df.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_6.txt...vLLM STDOUT: INFO:     127.0.0.1:54810 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 10:49:55 [logger.py:43] Received request chatcmpl-acacd3fb1bd84008bec4aaef05e753dd: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 1 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nMaximum\\nLikelihood helps in choosing the the values of parameters which maximizes the likelihood\\nthat the parameters are most likely to produce observed data.\\n\\nEnd Notes\\n\\nYou might have been able to answer all the questions, but the real value is in understanding\\nthem and generalizing your knowledge on similar questions. If you have struggled at these\\n\\n \\n \\n \\n \\n\\x0cquestions, no worries, now is the time to learn and not perform. You should right now focus\\non learning these topics scrupulously.\\n\\nThese questions are meant to give you a wide exposure on the types of questions asked at\\nstartups in machine learning. I’m sure these questions would leave you curious enough to do\\ndeeper topic research at your end. If you are planning for it, that’s a good sign.\\n\\nJoin for more: telegram.me/datasciencefree<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from \ndata/output/Machine_Learning_Interview_Questions_6.txt...vLLM STDOUT: INFO 11-10 10:49:55 [engine.py:317] Added request chatcmpl-acacd3fb1bd84008bec4aaef05e753dd.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_6.txt...vLLM STDOUT: INFO:     127.0.0.1:54812 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from .txt...\n\u001b[2K\u001b[1A\u001b[2KBatch processing complete._Questions_6.txt...\n\u001b[32m⠏\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KGenerated 19 QA pairs totalQuestions_6.txt...\n\u001b[32m⠏\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KSaving result to Interview_Questions_6.txt...\ndata/generated/Machine_Learning_Interview_Questions_6_qa_pairs.json\n\u001b[32m⠏\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n\u001b[32m⠏\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2KSuccessfully wrote result to estions_6.txt...\ndata/generated/Machine_Learning_Interview_Questions_6_qa_pairs.json\n\u001b[32m⠏\u001b[0m Generating qa content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from .txt...\ndata/output/Machine_Learning_Interview_Questions_6.txt...\n\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\n\u001b[1;32mdata/generated/Machine_Learning_Interview_Questions_6_qa_pairs.json\u001b[0m\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"Optionally, you can clean up the data via pruning \"bad\" or low quality examples and convert the rest to JSON format for finetuning!","metadata":{"id":"cNkxxvBx7Csp"}},{"cell_type":"code","source":"import os\nfile_path = \"data/generated/Machine_Learning_Interview_Questions.json\"\nprint(\"Current directory:\", os.getcwd())\nprint(\"Directory contents:\", os.listdir('.'))\nprint(\"Does data/generated/ exist?\", os.path.exists(\"data/generated\"))\nif os.path.exists(\"data/generated\"):\n    print(\"Contents of data/generated:\", os.listdir(\"data/generated\"))\nprint(f\"Does {file_path} exist?\", os.path.exists(file_path))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T10:58:50.831942Z","iopub.execute_input":"2025-11-10T10:58:50.832585Z","iopub.status.idle":"2025-11-10T10:58:50.839833Z","shell.execute_reply.started":"2025-11-10T10:58:50.832553Z","shell.execute_reply":"2025-11-10T10:58:50.838981Z"}},"outputs":[{"name":"stdout","text":"Current directory: /kaggle/working\nDirectory contents: ['data', '.virtual_documents', 'synthetic_data_kit_config.yaml', 'unsloth_compiled_cache']\nDoes data/generated/ exist? True\nContents of data/generated: ['test_write.json', 'Machine_Learning_Interview_Questions_6_qa_pairs.json', 'Machine_Learning_Interview_Questions_2_qa_pairs.json', 'Machine_Learning_Interview_Questions_0_qa_pairs.json', 'Machine_Learning_Interview_Questions_1_qa_pairs.json', 'Machine_Learning_Interview_Questions_5_qa_pairs.json', 'Machine_Learning_Interview_Questions_3_qa_pairs.json', 'Machine_Learning_Interview_Questions_4_qa_pairs.json']\nDoes data/generated/Machine_Learning_Interview_Questions.json exist? False\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"!synthetic-data-kit \\\n    -c synthetic_data_kit_config.yaml \\\n    curate --threshold 0.0 \\\n    \"data/generated/Machine_Learning_Interview_Questions_0_qa_pairs.json\"","metadata":{"id":"HMD-izj5OiAK","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:01:30.499727Z","iopub.execute_input":"2025-11-10T11:01:30.500053Z","iopub.status.idle":"2025-11-10T11:01:58.411130Z","shell.execute_reply.started":"2025-11-10T11:01:30.500030Z","shell.execute_reply":"2025-11-10T11:01:58.410398Z"}},"outputs":[{"name":"stdout","text":"vLLM STDOUT: INFO:     127.0.0.1:56532 - \"GET /v1/models HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO:     127.0.0.1:56536 - \"GET /v1/models HTTP/1.1\" 200 OK\n\u001b[?25lvLLM STDOUT: INFO 11-10 11:01:30 [logger.py:43] Received request chatcmpl-5ff924c8de7f40628537f2b6442ac2f4: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What would you do to reduce the dimension of a high-dimensional data set on a limited memory machine?\",\\n    \"answer\": \"You can either close all other applications on the machine or randomly sample the data set to create a smaller data set.\"\\n  },\\n  {\\n    \"question\": \"How many variables and rows would you use if you randomly sample the data set?\",\\n    \"answer\": \"You would use 1000 variables and 300000 rows.\"\\n  },\\n  {\\n    \"question\": \"What should we do with lower RAM to make more memory available?\",\\n    \"answer\": \"Close all other applications in our machine, including the web browser.\"\\n  },\\n  {\\n    \"question\": \"How can we create a smaller data set?\",\\n    \"answer\": \"We can randomly sample the data set, for example, with 1000 variables and 300000 rows.\"\\n  },\\n  {\\n    \"question\": \"What are some possible options for using online learning algorithms?\",\\n    \"answer\": \"Using online learning algorithms like Vowpal Wabbit (available in Python) is a possible option.\"\\n  },\\n  {\\n    \"question\": \"Is rotation necessary in PCA?\",\\n    \"answer\": \"Yes, rotation (orthogonal) is necessary because it maximizes the difference between variance captured by the component.\"\\n  },\\n  {\\n    \"question\": \"What is the main purpose of PCA?\",\\n    \"answer\": \"To select fewer components (than features) which can explain the maximum variance in the data set.\"\\n  },\\n  {\\n    \"question\": \"If PCA components are not rotated, what will happen to the data?\",\\n    \"answer\": \"The effect of PCA will diminish and we\\'ll have to select more number of components to explain variance in the data set.\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 11:01:30 [engine.py:317] Added request chatcmpl-5ff924c8de7f40628537f2b6442ac2f4.\nProcessing 3 batches of QA pairs...\n\u001b[32m⠋\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\ndata/generated/Machine_Learning_Interview_Questions_0_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:56548 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 11:01:43 [logger.py:43] Received request chatcmpl-65c7a65c7a0043e287598fc1dac1995f: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What percentage of data will remain unaffected if there are missing values which spread along 1 standard deviation from the median?\",\\n    \"answer\": \"50%\"\\n  },\\n  {\\n    \"question\": \"What percentage of data remains unaffected in a normal distribution spread along 1 standard deviation from the median?\",\\n    \"answer\": \"~32% of the data would remain unaffected.\"\\n  },\\n  {\\n    \"question\": \"Why shouldn\\'t a model with an accuracy of 96% be happy with its performance?\",\\n    \"answer\": \"Because a model can be overfitting or not generalizing well to unseen data.\"\\n  },\\n  {\\n    \"question\": \"What is a limitation of using accuracy as a measure of model performance in imbalanced data?\",\\n    \"answer\": \"It only predicts the majority class correctly, not the minority class of interest.\"\\n  },\\n  {\\n    \"question\": \"What metrics should be used to evaluate model performance in an imbalanced dataset?\",\\n    \"answer\": \"Sensitivity (True Positive Rate), Specificity (True Negative Rate), and F measure.\"\\n  },\\n  {\\n    \"question\": \"Why is 96% accuracy in an imbalanced data set considered misleading?\",\\n    \"answer\": \"96% accuracy might only be predicting the majority class correctly, not the minority class of interest.\"\\n  },\\n  {\\n    \"question\": \"What performance metrics should be used to evaluate model performance in an imbalanced data set?\",\\n    \"answer\": \"Sensitivity, Specificity, and F measure to determine class-wise performance of the classifier.\"\\n  },\\n  {\\n    \"question\": \"What assumption does naive Bayes make about its features?\",\\n    \"answer\": \"naive Bayes assumes that all of the features in a data set are equally important and independent.\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 11:01:43 [engine.py:317] Added request chatcmpl-65c7a65c7a0043e287598fc1dac1995f.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\ndata/generated/Machine_Learning_Interview_Questions_0_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:40628 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 11:01:56 [logger.py:43] Received request chatcmpl-16c7aa23d34945d9851ffc7da7788c9c: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"Why is naive Bayes called \\'naive\\'?\",\\n    \"answer\": \"naive Bayes is so \\'naive\\' because it assumes that all of the features in a data set are equally important and independent.\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 11:01:56 [engine.py:317] Added request chatcmpl-16c7aa23d34945d9851ffc7da7788c9c.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_0_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_0_qa_pairs.json...\ndata/generated/Machine_Learning_Interview_Questions_0_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:55920 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2KBatch processing complete.iew_Questions_0_qa_pairs.json...\n\u001b[32m⠸\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2KRated 17 QA pairsng_Interview_Questions_0_qa_pairs.json...\n\u001b[32m⠸\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2KRetained 17 pairs (threshold: 0.0)tions_0_qa_pairs.json...\n\u001b[32m⠸\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2KAverage score: 8.4g_Interview_Questions_0_qa_pairs.json...\n\u001b[32m⠸\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_0_qa_pairs.json...\ndata/generated/Machine_Learning_Interview_Questions_0_qa_pairs.json...\n\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\n\u001b[1;32mdata/cleaned/Machine_Learning_Interview_Questions_0_qa_pairs_cleaned.json\u001b[0m\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"We now convert the generated datasets into QA formats so we can load it for finetuning:","metadata":{"id":"AScJ5-vAOjYj"}},{"cell_type":"code","source":"!synthetic-data-kit \\\n    -c synthetic_data_kit_config.yaml \\\n    curate --threshold 0.0 \\\n    \"data/generated/Machine_Learning_Interview_Questions_1_qa_pairs.json\"\n!synthetic-data-kit \\\n    -c synthetic_data_kit_config.yaml \\\n    curate --threshold 0.0 \\\n    \"data/generated/Machine_Learning_Interview_Questions_2_qa_pairs.json\"\n!synthetic-data-kit \\\n    -c synthetic_data_kit_config.yaml \\\n    curate --threshold 0.0 \\\n    \"data/generated/Machine_Learning_Interview_Questions_3_qa_pairs.json\"\n!synthetic-data-kit \\\n    -c synthetic_data_kit_config.yaml \\\n    curate --threshold 0.0 \\\n    \"data/generated/Machine_Learning_Interview_Questions_4_qa_pairs.json\"\n!synthetic-data-kit \\\n    -c synthetic_data_kit_config.yaml \\\n    curate --threshold 0.0 \\\n    \"data/generated/Machine_Learning_Interview_Questions_5_qa_pairs.json\"\n!synthetic-data-kit \\\n    -c synthetic_data_kit_config.yaml \\\n    curate --threshold 0.0 \\\n    \"data/generated/Machine_Learning_Interview_Questions_6_qa_pairs.json\"\n\nprint(\"ALL SET\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:06:13.651167Z","iopub.execute_input":"2025-11-10T11:06:13.651948Z","iopub.status.idle":"2025-11-10T11:09:14.042163Z","shell.execute_reply.started":"2025-11-10T11:06:13.651917Z","shell.execute_reply":"2025-11-10T11:09:14.041177Z"}},"outputs":[{"name":"stdout","text":"vLLM STDOUT: INFO:     127.0.0.1:40394 - \"GET /v1/models HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO:     127.0.0.1:40396 - \"GET /v1/models HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 11:06:13 [logger.py:43] Received request chatcmpl-99865987f0c64862897396a19225ad2f: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What is the assumption of the naive Bayes algorithm?\",\\n    \"answer\": \"the naive Bayes algorithm assumes that all of the features in a data set are equally important and independent\"\\n  },\\n  {\\n    \"question\": \"What is the proportion of dependent (binary) variable in the data set called?\",\\n    \"answer\": \"Prior probability\"\\n  },\\n  {\\n    \"question\": \"What percentage of chances there are that a new email would be classified as spam?\",\\n    \"answer\": \"70%\"\\n  },\\n  {\\n    \"question\": \"Can a decision tree algorithm achieve higher accuracy than a time series regression model?\",\\n    \"answer\": \"Yes, this can happen\"\\n  },\\n  {\\n    \"question\": \"What type of data is known to possess linearity?\",\\n    \"answer\": \"Time series data\"\\n  },\\n  {\\n    \"question\": \"Why did the decision tree algorithm fail to provide robust predictions?\",\\n    \"answer\": \"It couldn\\\\u2019t map the linear relationship as good as a regression model did.\"\\n  },\\n  {\\n    \"question\": \"What is the primary reason companies offer free food to keep customers happy?\",\\n    \"answer\": \"Their customers get unhappy.\"\\n  },\\n  {\\n    \"question\": \"What are the three key factors to determine if machine learning can solve a problem?\",\\n    \"answer\": \"A pattern exists, it cannot be solved mathematically, and you have data on it.\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 11:06:13 [engine.py:317] Added request chatcmpl-99865987f0c64862897396a19225ad2f.\n\u001b[?25lProcessing 2 batches of QA pairs...\n\u001b[32m⠋\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_1_qa_pairs.json...\ndata/generated/Machine_Learning_Interview_Questions_1_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:40412 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 11:06:24 [logger.py:43] Received request chatcmpl-cef28c0d7ccf49eeafd08064f752c439: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"Can machine learning solve a problem mathematically?\",\\n    \"answer\": \"No, it cannot be solved mathematically\"\\n  },\\n  {\\n    \"question\": \"What are the three factors to decide if machine learning is a tool to solve a problem?\",\\n    \"answer\": \"Data on the problem, low bias, and high variance\"\\n  },\\n  {\\n    \"question\": \"What are the symptoms of a model with low bias and high variance?\",\\n    \"answer\": \"A model with low bias has predicted values near to actual values, while a model with high variance gives disappointing results when tested on unseen data.\"\\n  },\\n  {\\n    \"question\": \"What algorithm can be used to tackle high variance in a model?\",\\n    \"answer\": \"Bagging algorithms, such as random forest, can be used to tackle high variance by combining model predictions using voting or averaging.\"\\n  },\\n  {\\n    \"question\": \"What is the purpose of generating models using a single learning algorithm?\",\\n    \"answer\": \"To generate a set of models\"\\n  },\\n  {\\n    \"question\": \"Why should correlated variables be removed before running PCA?\",\\n    \"answer\": \"Because, in presence of correlated variables, the variance explained by a particular component gets inflated\"\\n  },\\n  {\\n    \"question\": \"Should correlated variables be removed before running PCA?\",\\n    \"answer\": \"No, because removing them can inflate the variance explained by each component.\"\\n  },\\n  {\\n    \"question\": \"Why does running PCA on correlated variables have a substantial effect?\",\\n    \"answer\": \"Because the variance explained by a particular component gets inflated in their presence.\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 11:06:24 [engine.py:317] Added request chatcmpl-cef28c0d7ccf49eeafd08064f752c439.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_1_qa_pairs.json...\ndata/generated/Machine_Learning_Interview_Questions_1_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:44326 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2KBatch processing complete.iew_Questions_1_qa_pairs.json...\n\u001b[32m⠴\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2KRated 16 QA pairsng_Interview_Questions_1_qa_pairs.json...\n\u001b[32m⠴\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2KRetained 16 pairs (threshold: 0.0)tions_1_qa_pairs.json...\n\u001b[32m⠴\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2KAverage score: 7.9g_Interview_Questions_1_qa_pairs.json...\n\u001b[32m⠴\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_1_qa_pairs.json...\ndata/generated/Machine_Learning_Interview_Questions_1_qa_pairs.json...\n\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\n\u001b[1;32mdata/cleaned/Machine_Learning_Interview_Questions_1_qa_pairs_cleaned.json\u001b[0m\nvLLM STDOUT: INFO:     127.0.0.1:54918 - \"GET /v1/models HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO:     127.0.0.1:54922 - \"GET /v1/models HTTP/1.1\" 200 OK\n\u001b[?25lvLLM STDOUT: INFO 11-10 11:06:37 [logger.py:43] Received request chatcmpl-ac3b096da20242f993033c477b65285d: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What is the issue with using PCA on correlated variables?\",\\n    \"answer\": \"PCA puts more importance on those variable, which is misleading.\"\\n  },\\n  {\\n    \"question\": \"Why did the ensemble of 5 GBM models not perform better than the benchmark score?\",\\n    \"answer\": \"You are unfortunate, where \\'you\\' likely missed hyperparameter tuning, as it is a common reason why ensemble models do not perform better than the benchmark score.\"\\n  },\\n  {\\n    \"question\": \"What is the problem with using ensemble learners with correlated models?\",\\n    \"answer\": \"All the models provide the same information, leading to same predictions.\"\\n  },\\n  {\\n    \"question\": \"What is the main issue with correlated models?\",\\n    \"answer\": \"All the models provide the same information.\"\\n  },\\n  {\\n    \"question\": \"What type of algorithm is kNN?\",\\n    \"answer\": \"kNN is a classification (or regression) algorithm.\"\\n  },\\n  {\\n    \"question\": \"What is the primary goal of the kmeans algorithm?\",\\n    \"answer\": \"The algorithm tries to maintain enough separability between these clusters.\"\\n  },\\n  {\\n    \"question\": \"How does kNN algorithm classify an unlabeled observation?\",\\n    \"answer\": \"It is based on its k (can be any number) surrounding neighbors.\"\\n  },\\n  {\\n    \"question\": \"Can the intercept term be removed without affecting the model\\'s performance?\",\\n    \"answer\": \"No, the intercept term is necessary for the model\\'s evaluation, and removing it can result in a higher R\\\\u00b2 value.\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 11:06:37 [engine.py:317] Added request chatcmpl-ac3b096da20242f993033c477b65285d.\nProcessing 2 batches of QA pairs...\n\u001b[32m⠋\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_2_qa_pairs.json...\ndata/generated/Machine_Learning_Interview_Questions_2_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:54934 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 11:06:49 [logger.py:43] Received request chatcmpl-4a662c917ae94faeba46c0b24d05ab42: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"Can a regression model with multicollinearity be improved without losing any information?\",\\n    \"answer\": \"Yes, without losing any information, a better model can still be built by removing multicollinearity.\"\\n  },\\n  {\\n    \"question\": \"What happens to the R\\\\u00b2 value when the model lacks an intercept term?\",\\n    \"answer\": \"The R\\\\u00b2 value becomes smaller than actual, resulting in a higher R\\\\u00b2.\"\\n  },\\n  {\\n    \"question\": \"How can you check for multicollinearity without losing any information?\",\\n    \"answer\": \"You can create a correlation matrix to identify and remove variables with correlation above 75% or use VIF (variance inflation factor) and tolerance as indicators.\"\\n  },\\n  {\\n    \"question\": \"What is the threshold for no multicollinearity according to VIF value?\",\\n    \"answer\": \"VIF value <= 4\"\\n  },\\n  {\\n    \"question\": \"How do you determine the presence of multicollinearity?\",\\n    \"answer\": \"By checking the VIF\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from \ndata/generated/Machine_Learning_Interview_Questions_2_qa_pairs.json...vLLM STDOUT: INFO 11-10 11:06:49 [engine.py:317] Added request chatcmpl-4a662c917ae94faeba46c0b24d05ab42.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_2_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_2_qa_pairs.json...\ndata/generated/Machine_Learning_Interview_Questions_2_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:49804 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2KBatch processing complete.iew_Questions_2_qa_pairs.json...\n\u001b[32m⠸\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2KRated 13 QA pairsng_Interview_Questions_2_qa_pairs.json...\n\u001b[32m⠸\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2KRetained 13 pairs (threshold: 0.0)tions_2_qa_pairs.json...\n\u001b[32m⠸\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2KAverage score: 8.1g_Interview_Questions_2_qa_pairs.json...\n\u001b[32m⠸\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_2_qa_pairs.json...\ndata/generated/Machine_Learning_Interview_Questions_2_qa_pairs.json...\n\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\n\u001b[1;32mdata/cleaned/Machine_Learning_Interview_Questions_2_qa_pairs_cleaned.json\u001b[0m\nvLLM STDOUT: INFO:     127.0.0.1:60964 - \"GET /v1/models HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO:     127.0.0.1:60972 - \"GET /v1/models HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 11:06:59 [logger.py:43] Received request chatcmpl-62ea582dcf2e4322ac520afe1ad248ee: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"When is Ridge regression favorable over Lasso regression?\",\\n    \"answer\": \"In presence of many variables with small / medium sized effect.\"\\n  },\\n  {\\n    \"question\": \"Why is adding noise in correlated variables to Ridge regression used?\",\\n    \"answer\": \"To make the variables different from each other.\"\\n  },\\n  {\\n    \"question\": \"When is Ridge regression favorable over Lasso regression?\",\\n    \"answer\": \"In presence of many variables with small / medium sized effect\"\\n  },\\n  {\\n    \"question\": \"What does Ridge regression do differently than Lasso regression?\",\\n    \"answer\": \"Ridge regression only does parameter shrinkage, whereas Lasso regression does both variable selection and parameter shrinkage\"\\n  },\\n  {\\n    \"question\": \"In what situations does ridge regression work best?\",\\n    \"answer\": \"Ridge regression works best in situations where the least square estimates have higher variance.\"\\n  },\\n  {\\n    \"question\": \"Why is ridge regression preferred over other methods in the presence of correlated variables?\",\\n    \"answer\": \"Ridge regression might be the preferred choice in presence of correlated variables.\"\\n  },\\n  {\\n    \"question\": \"What is the relationship between global average temperature and the number of pirates?\",\\n    \"answer\": \"There is a correlation, but it cannot be concluded that a decrease in the number of pirates caused the climate change.\"\\n  },\\n  {\\n    \"question\": \"What methods can be used to select important variables while working on a data set?\",\\n    \"answer\": \"Variable selection methods include identifying the methods of variable selection.\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n\u001b[?25lProcessing 3 batches of QA pairs...\n\u001b[32m⠋\u001b[0m Cleaning content from \ndata/generated/Machine_Learning_Interview_Questions_3_qa_pairs.json...vLLM STDOUT: INFO 11-10 11:06:59 [engine.py:317] Added request chatcmpl-62ea582dcf2e4322ac520afe1ad248ee.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\ndata/generated/Machine_Learning_Interview_Questions_3_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:60974 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 11:07:15 [logger.py:43] Received request chatcmpl-0ba3e1ddf9624eac8e59686a9e766ad2: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What is the relationship between the decrease in the number of pirates and climate change?\",\\n    \"answer\": \"There is a correlation, but we cannot conclude that the decrease in pirates caused the climate change due to potential confounding variables.\"\\n  },\\n  {\\n    \"question\": \"How do you select important variables while working on a data set?\",\\n    \"answer\": \"Methods include removing correlated variables prior to selection and using linear regression to select variables based on p values.\"\\n  },\\n  {\\n    \"question\": \"What methods can be used to select important variables?\",\\n    \"answer\": \"You can use: Remove correlated variables, linear regression with p values, Forward Selection, Backward Selection, Stepwise Selection, Random Forest, Xgboost, Lasso Regression, and Measure information gain.\"\\n  },\\n  {\\n    \"question\": \"What is the difference between covariance and correlation?\",\\n    \"answer\": \"Correlation is the standardized form of covariance, making it easier to compare values across different scales.\"\\n  },\\n  {\\n    \"question\": \"What is the difference between covariance and correlation?\",\\n    \"answer\": \"Correlation is the standardized form of covariance.\"\\n  },\\n  {\\n    \"question\": \"Can we capture the correlation between continuous and categorical variables?\",\\n    \"answer\": \"Yes, we can use ANCOVA technique to capture association between continuous and categorical variables.\"\\n  },\\n  {\\n    \"question\": \"Can we capture the correlation between continuous and categorical variables?\",\\n    \"answer\": \"Yes, we can use ANCOVA (analysis of covariance) technique to capture association\"\\n  },\\n  {\\n    \"question\": \"How are random forests different from Gradient Boosting algorithms (GBM)?\",\\n    \"answer\": \"Random forests use bagging, while GBM uses boosting techniques\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from \ndata/generated/Machine_Learning_Interview_Questions_3_qa_pairs.json...vLLM STDOUT: INFO 11-10 11:07:15 [engine.py:317] Added request chatcmpl-0ba3e1ddf9624eac8e59686a9e766ad2.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\ndata/generated/Machine_Learning_Interview_Questions_3_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:53858 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 11:07:32 [logger.py:43] Received request chatcmpl-a38bf4bf286941cf95be7f55bb232a17: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What is the main way that random forests improve model accuracy?\",\\n    \"answer\": \"Random forests improve model accuracy by reducing variance (mainly).\"\\n  },\\n  {\\n    \"question\": \"What is the stopping criterion in boosting?\",\\n    \"answer\": \"A stopping criterion is reached.\"\\n  },\\n  {\\n    \"question\": \"How does a classification tree make a decision?\",\\n    \"answer\": \"A classification tree makes a decision based on the Gini Index and Node Entropy.\"\\n  },\\n  {\\n    \"question\": \"What is the Gini Index used for?\",\\n    \"answer\": \"The Gini Index is used to calculate the probability of two items being of the same class if selected from a population at random.\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 11:07:32 [engine.py:317] Added request chatcmpl-a38bf4bf286941cf95be7f55bb232a17.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_3_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_3_qa_pairs.json...\ndata/generated/Machine_Learning_Interview_Questions_3_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:39122 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2KBatch processing complete.iew_Questions_3_qa_pairs.json...\n\u001b[32m⠹\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2KRated 20 QA pairsng_Interview_Questions_3_qa_pairs.json...\n\u001b[32m⠹\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2KRetained 20 pairs (threshold: 0.0)tions_3_qa_pairs.json...\n\u001b[32m⠹\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2KAverage score: 7.4g_Interview_Questions_3_qa_pairs.json...\n\u001b[32m⠹\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_3_qa_pairs.json...\ndata/generated/Machine_Learning_Interview_Questions_3_qa_pairs.json...\n\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\n\u001b[1;32mdata/cleaned/Machine_Learning_Interview_Questions_3_qa_pairs_cleaned.json\u001b[0m\nvLLM STDOUT: INFO:     127.0.0.1:38346 - \"GET /v1/models HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO:     127.0.0.1:38354 - \"GET /v1/models HTTP/1.1\" 200 OK\n\u001b[?25lvLLM STDOUT: INFO 11-10 11:07:42 [logger.py:43] Received request chatcmpl-372921b4f87548be8f935fe407b1aa9e: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What does a pure population mean in the context of the Gini index?\",\\n    \"answer\": \"a population where all items are of the same class\"\\n  },\\n  {\\n    \"question\": \"What is the Gini index formula for calculating impurity in a node?\",\\n    \"answer\": \"sum of square of probability for success and failure (p^2+q^2)\"\\n  },\\n  {\\n    \"question\": \"What does a training error of 0.00 indicate in a model?\",\\n    \"answer\": \"The classifier has mimiced the training data patterns to an extent, that they are not available in the unseen data.\"\\n  },\\n  {\\n    \"question\": \"Why is OLS a bad option to work with a dataset having p > n?\",\\n    \"answer\": \"OLS is a bad option to work with a dataset having p > n because it\\'s highly likely to be multicollinearity and non-unique solutions, making it ineffective.\"\\n  },\\n  {\\n    \"question\": \"What is OLS and why is it not suitable for high-dimensional data sets?\",\\n    \"answer\": \"OLS (Ordinary Least Squares) is a regression technique that fails to meet its assumptions when p > n. In such cases, OLS cannot be used due to infinite variances and non-unique least square coefficient estimates.\"\\n  },\\n  {\\n    \"question\": \"Which methods can be used to reduce variance in high-dimensional data sets?\",\\n    \"answer\": \"Penalized regression methods like Lasso, LARS, and Ridge regression can shrink coefficients to reduce variance in high-dimensional data sets.\"\\n  },\\n  {\\n    \"question\": \"When does OLS not work?\",\\n    \"answer\": \"When p > n, the variances become infinite, so OLS cannot be used at all.\"\\n  },\\n  {\\n    \"question\": \"What is a method used to reduce variance in regression?\",\\n    \"answer\": \"Penalized regression methods like lasso, LARS, ridge regression can shrink the coefficients to reduce variance.\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 11:07:42 [engine.py:317] Added request chatcmpl-372921b4f87548be8f935fe407b1aa9e.\nProcessing 2 batches of QA pairs...\n\u001b[32m⠋\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\ndata/generated/Machine_Learning_Interview_Questions_4_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:38358 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 11:07:57 [logger.py:43] Received request chatcmpl-d6b03632e64544f69a55c750f4793d9c: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What is the purpose of convex hull in machine learning?\",\\n    \"answer\": \"In case of linearly separable data, convex hull represents the outer boundaries of the two group of data points.\"\\n  },\\n  {\\n    \"question\": \"How does label encoding affect a dataset?\",\\n    \"answer\": \"Label encoding does not increase the dimensionality of a dataset.\"\\n  },\\n  {\\n    \"question\": \"What is the effect of one hot encoding on a data set?\",\\n    \"answer\": \"One hot encoding increases the dimensionality of a data set.\"\\n  },\\n  {\\n    \"question\": \"How does label encoding work?\",\\n    \"answer\": \"Label encoding encodes the levels of a categorical variable as 0 and 1, without creating new variables.\"\\n  },\\n  {\\n    \"question\": \"What happens when one hot encoding is applied to a categorical variable?\",\\n    \"answer\": \"It generates three new variables as Color.Red, Color.Blue and Color.Green containing 0 and 1 value.\"\\n  },\\n  {\\n    \"question\": \"What cross-validation technique should be used on time series data?\",\\n    \"answer\": \"Neither k-fold nor LOOCV, but forward chaining strategy with 5-fold.\"\\n  },\\n  {\\n    \"question\": \"What cross-validation technique should not be used on time series data?\",\\n    \"answer\": \"Neither k-fold nor LOOCV.\"\\n  },\\n  {\\n    \"question\": \"Why is k-fold troublesome in time series data?\",\\n    \"answer\": \"Because resampling the data might separate trends in different years, leading to incorrect validation.\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from \ndata/generated/Machine_Learning_Interview_Questions_4_qa_pairs.json...vLLM STDOUT: INFO 11-10 11:07:57 [engine.py:317] Added request chatcmpl-d6b03632e64544f69a55c750f4793d9c.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_4_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_4_qa_pairs.json...\ndata/generated/Machine_Learning_Interview_Questions_4_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:51066 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2KBatch processing complete.iew_Questions_4_qa_pairs.json...\n\u001b[32m⠹\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2KRated 16 QA pairsng_Interview_Questions_4_qa_pairs.json...\n\u001b[32m⠹\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2KRetained 16 pairs (threshold: 0.0)tions_4_qa_pairs.json...\n\u001b[32m⠹\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2KAverage score: 8.3g_Interview_Questions_4_qa_pairs.json...\n\u001b[32m⠹\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_4_qa_pairs.json...\ndata/generated/Machine_Learning_Interview_Questions_4_qa_pairs.json...\n\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\n\u001b[1;32mdata/cleaned/Machine_Learning_Interview_Questions_4_qa_pairs_cleaned.json\u001b[0m\nvLLM STDOUT: INFO:     127.0.0.1:47284 - \"GET /v1/models HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO:     127.0.0.1:47300 - \"GET /v1/models HTTP/1.1\" 200 OK\n\u001b[?25lvLLM STDOUT: INFO 11-10 11:08:11 [logger.py:43] Received request chatcmpl-1200ae7815004df991c4de9bf0bc5665: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"How many variables have missing values higher than 30% in a dataset of 50 variables?\",\\n    \"answer\": \"8 variables\"\\n  },\\n  {\\n    \"question\": \"What is the basic idea behind the \\'People who bought this, also bought\\\\u2026\\' recommendations seen on Amazon?\",\\n    \"answer\": \"Collaborative filtering\"\\n  },\\n  {\\n    \"question\": \"What is the purpose of removing items blatantly?\",\\n    \"answer\": \"We can remove them blatantly or sensibly check their distribution with the target variable.\"\\n  },\\n  {\\n    \"question\": \"Which algorithm is behind \\'People who bought this, also bought\\\\u2026\\' recommendations?\",\\n    \"answer\": \"The basic idea for this kind of recommendation engine comes from collaborative filtering.\"\\n  },\\n  {\\n    \"question\": \"What is the primary method used for recommending items to new users?\",\\n    \"answer\": \"Other users\\' behaviour and preferences\"\\n  },\\n  {\\n    \"question\": \"What are Type I and Type II errors in the context of confusion matrix?\",\\n    \"answer\": \"Type I error: rejecting a true null hypothesis, Type II error: accepting a false null hypothesis\"\\n  },\\n  {\\n    \"question\": \"What are Type I and Type II errors in the context of hypothesis testing?\",\\n    \"answer\": \"Type I error is committed when the null hypothesis is true and we reject it, also known as a \\\\u2018False Positive\\\\u2019. Type II error is committed when the null hypothesis is false and we accept it, also known as \\\\u2018False Negative\\\\u2019.\"\\n  },\\n  {\\n    \"question\": \"Why might a high validation accuracy on a classification problem lead to poor test accuracy?\",\\n    \"answer\": \"You may have overfit your model to the training data, and it is not generalizing well to unseen data, leading to poor test accuracy.\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 11:08:11 [engine.py:317] Added request chatcmpl-1200ae7815004df991c4de9bf0bc5665.\nProcessing 3 batches of QA pairs...\n\u001b[32m⠋\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\ndata/generated/Machine_Learning_Interview_Questions_5_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:47306 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 11:08:27 [logger.py:43] Received request chatcmpl-d36b314bdd7241ba849932b766b0e811: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"Why poor test accuracy occurred despite high validation accuracy?\",\\n    \"answer\": \"In case of classification problem, we should always use stratified sampling instead of random sampling.\"\\n  },\\n  {\\n    \"question\": \"What is the criteria for evaluating a regression model?\",\\n    \"answer\": \"Tolerance (1 / VIF) is used as an indicator of multicollinearity.\"\\n  },\\n  {\\n    \"question\": \"What is the criteria for model fit evaluation?\",\\n    \"answer\": \"Tolerance (1 / VIF) is used as an indicator of multicollinearity.\"\\n  },\\n  {\\n    \"question\": \"Why is adjusted R\\\\u00b2 used instead of R\\\\u00b2 to evaluate model fit?\",\\n    \"answer\": \"Because R\\\\u00b2 increases irrespective of improvement in prediction accuracy as we add more variables.\"\\n  },\\n  {\\n    \"question\": \"What type of distance is used in k-means or kNN?\",\\n    \"answer\": \"euclidean distance\"\\n  },\\n  {\\n    \"question\": \"Why we can\\'t use manhattan distance?\",\\n    \"answer\": \"Because it has dimension restrictions\"\\n  },\\n  {\\n    \"question\": \"How do you explain machine learning to a 5-year-old?\",\\n    \"answer\": \"Machine learning is when computers learn from data, just like you learn from your parents or teachers.\"\\n  },\\n  {\\n    \"question\": \"What is euclidean distance used for?\",\\n    \"answer\": \"to calculate distance in any space\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 11:08:27 [engine.py:317] Added request chatcmpl-d36b314bdd7241ba849932b766b0e811.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\ndata/generated/Machine_Learning_Interview_Questions_5_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:36692 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 11:08:41 [logger.py:43] Received request chatcmpl-ceb78a6256bd42f4886d69b11692a136: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What is an example of when Manhattan distance is used?\",\\n    \"answer\": \"in calculating the movement made by a bishop or a rook on a chess board\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 11:08:41 [engine.py:317] Added request chatcmpl-ceb78a6256bd42f4886d69b11692a136.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_5_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_5_qa_pairs.json...\ndata/generated/Machine_Learning_Interview_Questions_5_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:54404 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2KBatch processing complete.iew_Questions_5_qa_pairs.json...\n\u001b[32m⠏\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2KRated 17 QA pairsng_Interview_Questions_5_qa_pairs.json...\n\u001b[32m⠏\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2KRetained 17 pairs (threshold: 0.0)tions_5_qa_pairs.json...\n\u001b[32m⠏\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2KAverage score: 8.1g_Interview_Questions_5_qa_pairs.json...\n\u001b[32m⠏\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_5_qa_pairs.json...\ndata/generated/Machine_Learning_Interview_Questions_5_qa_pairs.json...\n\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\n\u001b[1;32mdata/cleaned/Machine_Learning_Interview_Questions_5_qa_pairs_cleaned.json\u001b[0m\nvLLM STDOUT: INFO:     127.0.0.1:54420 - \"GET /v1/models HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO:     127.0.0.1:54430 - \"GET /v1/models HTTP/1.1\" 200 OK\n\u001b[?25lvLLM STDOUT: INFO 11-10 11:08:44 [logger.py:43] Received request chatcmpl-c82a78dca38f4046a418eedffc0a8970: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What is the primary goal of a machine when it falls down and tries to stand up again?\",\\n    \"answer\": \"To avoid pain and learn not to stand in a bend position again.\"\\n  },\\n  {\\n    \"question\": \"How is the performance of a logistic regression model typically evaluated?\",\\n    \"answer\": \"Using AUC-ROC curve along with a confusion matrix.\"\\n  },\\n  {\\n    \"question\": \"What is the analogous metric of adjusted R\\\\u00b2 in logistic regression?\",\\n    \"answer\": \"AIC\"\\n  },\\n  {\\n    \"question\": \"What indicates the response predicted by a model with nothing but an intercept?\",\\n    \"answer\": \"Null Deviance\"\\n  },\\n  {\\n    \"question\": \"What does residual deviance indicate in a model?\",\\n    \"answer\": \"the response predicted by a model on adding independent variables\"\\n  },\\n  {\\n    \"question\": \"How does the choice of machine learning algorithm depend?\",\\n    \"answer\": \"the type of data\"\\n  },\\n  {\\n    \"question\": \"When does a boosting or bagging algorithm be chosen?\",\\n    \"answer\": \"When the data comprises of non-linear interactions\"\\n  },\\n  {\\n    \"question\": \"When does regularization become necessary in Machine Learning?\",\\n    \"answer\": \"When building a model which can be deployed\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 11:08:44 [engine.py:317] Added request chatcmpl-c82a78dca38f4046a418eedffc0a8970.\nProcessing 3 batches of QA pairs...\n\u001b[32m⠋\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\ndata/generated/Machine_Learning_Interview_Questions_6_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:54446 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 11:08:55 [logger.py:43] Received request chatcmpl-c826cca0f325458287aeb5f42b68ff31: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What type of variable can be considered as continuous for better predictions?\",\\n    \"answer\": \"For better predictions, categorical variable can be considered as a continuous variable only when the variable is ordinal in nature.\"\\n  },\\n  {\\n    \"question\": \"When does regularization become necessary in Machine Learning?\",\\n    \"answer\": \"Regularization becomes necessary when the model begins to overfit / underfit.\"\\n  },\\n  {\\n    \"question\": \"What does the Bias Variance trade off refer to?\",\\n    \"answer\": \"The error emerging from any model can be broken down into three components mathematically.\"\\n  },\\n  {\\n    \"question\": \"What technique introduces a cost term for bringing in more features with the objective function?\",\\n    \"answer\": \"This technique\"\\n  },\\n  {\\n    \"question\": \"What is the purpose of reducing model complexity?\",\\n    \"answer\": \"To become better at predicting and generalizing\"\\n  },\\n  {\\n    \"question\": \"What is a high bias error in machine learning?\",\\n    \"answer\": \"A high bias error means we have a under-performing model which keeps on missing important trends.\"\\n  },\\n  {\\n    \"question\": \"What is the relationship between variance and a model\\'s performance?\",\\n    \"answer\": \"A high variance model will over-fit on your training population and perform badly on any observation beyond training.\"\\n  },\\n  {\\n    \"question\": \"What methods are used by linear regression to approximate the unknown parameter (coefficient) value?\",\\n    \"answer\": \"OLS and Maximum likelihood\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 11:08:55 [engine.py:317] Added request chatcmpl-c826cca0f325458287aeb5f42b68ff31.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\ndata/generated/Machine_Learning_Interview_Questions_6_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:52800 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nvLLM STDOUT: INFO 11-10 11:09:08 [logger.py:43] Received request chatcmpl-2f32162f1d524863a819a3b73469d280: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What is the main goal of Ordinary least square (OLS) in linear regression?\",\\n    \"answer\": \"to approximate the parameters resulting in minimum distance between actual and predicted values\"\\n  },\\n  {\\n    \"question\": \"What does Maximum Likelihood help in?\",\\n    \"answer\": \"choosing the values of parameters which maximizes the likelihood that the parameters are most likely to produce observed data.\"\\n  },\\n  {\\n    \"question\": \"What should you focus on if you struggled with the questions?\",\\n    \"answer\": \"learning these topics scrupulously\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nvLLM STDOUT: INFO 11-10 11:09:08 [engine.py:317] Added request chatcmpl-2f32162f1d524863a819a3b73469d280.\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from stions_6_qa_pairs.json...\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from stions_6_qa_pairs.json...\ndata/generated/Machine_Learning_Interview_Questions_6_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:51880 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2KBatch processing complete.iew_Questions_6_qa_pairs.json...\n\u001b[32m⠧\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2KRated 19 QA pairsng_Interview_Questions_6_qa_pairs.json...\n\u001b[32m⠧\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2KRetained 19 pairs (threshold: 0.0)tions_6_qa_pairs.json...\n\u001b[32m⠧\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2KAverage score: 8.1g_Interview_Questions_6_qa_pairs.json...\n\u001b[32m⠧\u001b[0m Cleaning content from \n\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from stions_6_qa_pairs.json...\ndata/generated/Machine_Learning_Interview_Questions_6_qa_pairs.json...\n\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\n\u001b[1;32mdata/cleaned/Machine_Learning_Interview_Questions_6_qa_pairs_cleaned.json\u001b[0m\nALL SET\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"qa_pairs_filenames = [\n    f\"data/generated/Machine_Learning_Interview_Questions_{i}_qa_pairs.json\"\n    for i in range(len(filenames[:7]))\n]\nfor filename in qa_pairs_filenames:\n    !synthetic-data-kit \\\n        -c synthetic_data_kit_config.yaml \\\n        save-as {filename} -f ft","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J9Um4Z8SqUTB","outputId":"d7880fb6-7548-45f0-9112-9837094285b9","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:16:10.918678Z","iopub.execute_input":"2025-11-10T11:16:10.919514Z","iopub.status.idle":"2025-11-10T11:16:13.930602Z","shell.execute_reply.started":"2025-11-10T11:16:10.919479Z","shell.execute_reply":"2025-11-10T11:16:13.929659Z"}},"outputs":[{"name":"stdout","text":"\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/Machine_Learning_Interview_Questions_0_qa_pairs.json\nto ft format with json storage...\n\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\n\u001b[1;32mdata/final/Machine_Learning_Interview_Questions_0_qa_pairs_ft.json\u001b[0m\n\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/Machine_Learning_Interview_Questions_1_qa_pairs.json\nto ft format with json storage...\n\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\n\u001b[1;32mdata/final/Machine_Learning_Interview_Questions_1_qa_pairs_ft.json\u001b[0m\n\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/Machine_Learning_Interview_Questions_2_qa_pairs.json\nto ft format with json storage...\n\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\n\u001b[1;32mdata/final/Machine_Learning_Interview_Questions_2_qa_pairs_ft.json\u001b[0m\n\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/Machine_Learning_Interview_Questions_3_qa_pairs.json\nto ft format with json storage...\n\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\n\u001b[1;32mdata/final/Machine_Learning_Interview_Questions_3_qa_pairs_ft.json\u001b[0m\n\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/Machine_Learning_Interview_Questions_4_qa_pairs.json\nto ft format with json storage...\n\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\n\u001b[1;32mdata/final/Machine_Learning_Interview_Questions_4_qa_pairs_ft.json\u001b[0m\n\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/Machine_Learning_Interview_Questions_5_qa_pairs.json\nto ft format with json storage...\n\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\n\u001b[1;32mdata/final/Machine_Learning_Interview_Questions_5_qa_pairs_ft.json\u001b[0m\n\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/Machine_Learning_Interview_Questions_6_qa_pairs.json\nto ft format with json storage...\n\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\n\u001b[1;32mdata/final/Machine_Learning_Interview_Questions_6_qa_pairs_ft.json\u001b[0m\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"Let's load up the data and see what the synthetic data looks like!","metadata":{"id":"6dVK-qza7rPB"}},{"cell_type":"code","source":"from datasets import Dataset\nimport pandas as pd\nfinal_filenames = [\n    f\"data/final/Machine_Learning_Interview_Questions_{i}_qa_pairs_ft.json\"\n    for i in range(len(filenames[:7]))\n]\nconversations = pd.concat([\n    pd.read_json(name) for name in final_filenames\n]).reset_index(drop = True)\n\ndataset = Dataset.from_pandas(conversations)","metadata":{"id":"VrBwG2KT7dam","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:16:52.755411Z","iopub.execute_input":"2025-11-10T11:16:52.755682Z","iopub.status.idle":"2025-11-10T11:16:52.880110Z","shell.execute_reply.started":"2025-11-10T11:16:52.755666Z","shell.execute_reply":"2025-11-10T11:16:52.879488Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"dataset[0]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jaZ3tRP8frSn","outputId":"c1ec0e11-0a8e-4969-bb9d-c4496822ba9d","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:17:02.043848Z","iopub.execute_input":"2025-11-10T11:17:02.044671Z","iopub.status.idle":"2025-11-10T11:17:02.064332Z","shell.execute_reply.started":"2025-11-10T11:17:02.044642Z","shell.execute_reply":"2025-11-10T11:17:02.063696Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"{'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'},\n  {'content': 'What would you do to reduce the dimension of a high-dimensional data set on a limited memory machine?',\n   'role': 'user'},\n  {'content': 'You can either close all other applications on the machine or randomly sample the data set to create a smaller data set.',\n   'role': 'assistant'}]}"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"dataset[1]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"504n46Sxfruu","outputId":"815928be-e07f-4257-ddd7-751cf8008cdb","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:17:12.932801Z","iopub.execute_input":"2025-11-10T11:17:12.933475Z","iopub.status.idle":"2025-11-10T11:17:12.939000Z","shell.execute_reply.started":"2025-11-10T11:17:12.933448Z","shell.execute_reply":"2025-11-10T11:17:12.938010Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"{'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'},\n  {'content': 'How many variables and rows would you use if you randomly sample the data set?',\n   'role': 'user'},\n  {'content': 'You would use 1000 variables and 300000 rows.',\n   'role': 'assistant'}]}"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"dataset[-1]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1BVBp9YXRw_o","outputId":"6c7fcf69-7de3-4bf7-8037-cb0477bce3b7","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:17:13.125137Z","iopub.execute_input":"2025-11-10T11:17:13.125748Z","iopub.status.idle":"2025-11-10T11:17:13.131392Z","shell.execute_reply.started":"2025-11-10T11:17:13.125728Z","shell.execute_reply":"2025-11-10T11:17:13.130530Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"{'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'},\n  {'content': 'What should you focus on if you struggled with the questions?',\n   'role': 'user'},\n  {'content': 'learning these topics scrupulously', 'role': 'assistant'}]}"},"metadata":{}}],"execution_count":30},{"cell_type":"markdown","source":"Finally free vLLM process to save memory and to allow for finetuning!","metadata":{"id":"aO9qePmP7yaY"}},{"cell_type":"code","source":"generator.cleanup()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F8qgTjywzgl6","outputId":"f54b04cd-9586-42ff-8276-c4c54496560b","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:17:33.850629Z","iopub.execute_input":"2025-11-10T11:17:33.851220Z","iopub.status.idle":"2025-11-10T11:17:48.055522Z","shell.execute_reply.started":"2025-11-10T11:17:33.851196Z","shell.execute_reply":"2025-11-10T11:17:48.054891Z"}},"outputs":[{"name":"stdout","text":"Attempting to terminate the VLLM server gracefully...\nvLLM STDOUT: INFO 11-10 11:17:33 [launcher.py:80] Shutting down FastAPI HTTP server.\nServer terminated gracefully.\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"### Fine-tuning Synthetic Dataset with Unsloth","metadata":{"id":"EQo2PR7oqDQE"}},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\n\nfourbit_models = [\n    # 4bit dynamic quants for superior accuracy and low memory use\n    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n    # Qwen3 new models\n    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n    # Other very popular models!\n    \"unsloth/Llama-3.1-8B\",\n    \"unsloth/Llama-3.2-3B\",\n    \"unsloth/Llama-3.3-70B\",\n    \"unsloth/mistral-7b-instruct-v0.3\",\n    \"unsloth/Phi-4\",\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n    max_seq_length = 2048, # Choose any for long context!\n    load_in_4bit = True,  # 4 bit quantization to reduce memory\n    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n    full_finetuning = False, # [NEW!] We have full finetuning now!\n    # token = \"hf_...\", # use one if using gated models\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282,"referenced_widgets":["fc9c4d66a2fa41a393525a04ced011f3","0365c9b5e9a44318a70109069135b7fe","5759ecdb02714a9f82892e12c6b9ae46","85ff0a11606e466ebd7f7b3b6ed3048e","cae045b72aca476a9c16c39778be53a0","c0cf2ea32d7e42d2b6cb00ba5b128e6d","5856dc5484e54deaae2c0ea0703885cb","f229d574614945029666403a10bb761e","b6da3300ba7e42888add16741bafd614","5d6cb85f619b439db3544414b732607c","9f083cc2ddb64b6196cc2d170affabae","3ca561741e5f4b59b796b074cac64d80","15c3082a57e04b0198a8232f135290ef","7cd6c8ef7ec9451c90560208277975c5","f631738fd3014d13a7e6a50a66eaf92a","f8b4855c3f0146dd8d3d0876f3a12c08","a87d29f844c449119a95f5548c0af0c7","abe93b4cbe524be08d08ff75f22d0c08","9f60d2b4aa054b82bcef0b49cd4014f3","27be088be2de4517964a8e5217641fd9","7f616bf83de54cfeaabda52b3f592e4c","2f822636140c45e8a9713b711d057005","d944a4b11b614896bdb6af583faed134","83657ceb17f1454ca7e33a5fcc5ad5f7","c6f86936f86347819002d6bbf50ec484","cf6e3f6267cb49d5ac4575d1d1e41d10","2d0a363ce9c54665811b8cd0a8fdb42b","bbc163a35ec54087bf0c56cc332df0b4","c901f4517b474511b6dd880e63f25b58","74806db2db6e4085a28dceca4fbeee28","e2f989df5f494351a5b242d2430c2917","ef715f56f3824a64949be1f2bb05296b","cd3f2a8158124c6b87be72e959a455d5","62e6190f733844319221ce6eb7a7928c","acd616b0a9574855a3d4525e72a6513a","9b27fcca9ec64b81952560f38080abc6","2f88451af2cd48aa93da32431e5de139","55e82538888940c1b754fa02d29e965c","2b15c1d19cf84911bee54a6d113eed1a","cfdfb65ccad04899881976d93945bf0d","e3ab144489504058b7c9009d867bef95","943c7280c19948999aa663ca6736d698","47d0de835e7d48e39161a7f038167653","6956008e9def426b82617536b015dd38","4952048f852a4593af48573a163a052f","a6e9f1533f83416c8b512f31b099254c","937a90e4c4dc41b195d24fa1694c9157","b82aae8041fa462d88467a02fa252af7","e7d6be17a7bb4d5e8b248d7f5dee9647","fc915d911ee144bea96b1885f5eb7191","caed1189f10140549fe2b445dd7dbd16","8c6845e9bd81418e8c02d66c32046129","2fb3b417c2404e8a9ad83b1a4114174c","e22b473dd97446298b42987e36d945ec","9a18c25e8d694fff905917e7ce2931dd"]},"id":"QmUBVEnvCDJv","outputId":"b094477b-ebf4-4980-934b-686f135e1347","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:18:03.369956Z","iopub.execute_input":"2025-11-10T11:18:03.370658Z","iopub.status.idle":"2025-11-10T11:18:19.536526Z","shell.execute_reply.started":"2025-11-10T11:18:03.370629Z","shell.execute_reply":"2025-11-10T11:18:19.535544Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.11.2: Fast Llama patching. Transformers: 4.56.2. vLLM: 0.9.2.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.35G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e61592061aa41d48100283ba0eaf912"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e182a8d139b4c6c8389b5b29d1e153b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab6ddb90a50b430498e43906dd37d385"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf33ff5b7c034b80be8c731bb6e59934"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f267db32c5745429dd9204406951cb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"737cdae65df14067b47e7e4fb7ec0e73"}},"metadata":{}}],"execution_count":32},{"cell_type":"markdown","source":"We now add LoRA adapters so we only need to update 1 to 10% of all parameters!","metadata":{"id":"SXd9bTZd1aaL"}},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6bZsfBuZDeCL","outputId":"a8a510bf-226e-4baf-b333-0cbc58cf93a7","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:18:19.538267Z","iopub.execute_input":"2025-11-10T11:18:19.538574Z","iopub.status.idle":"2025-11-10T11:18:27.581937Z","shell.execute_reply.started":"2025-11-10T11:18:19.538549Z","shell.execute_reply":"2025-11-10T11:18:27.581072Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.11.2 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"<a name=\"Data\"></a>\n### Data Prep\nWe now use the `Llama-3.2` format for conversation style finetunes. The chat template renders conversations like below: (Cutting Knowledge Date is by default there!)\n\n```\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 01 May 2025\n\nYou are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nWhat is 1+1?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n2<|eot_id|>\n```","metadata":{"id":"vITh0KVJ10qX"}},{"cell_type":"code","source":"def formatting_prompts_func(examples):\n    convos = examples[\"messages\"]\n    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n    return { \"text\" : texts, }\n\n# Get our previous dataset and format it:\ndataset = dataset.map(formatting_prompts_func, batched = True,)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["c7787082fecf4df0ae58e67f46e42589","60c5e826831c4afbb869a1429d3a372e","3fdc25451ed543d288d89df799883b3c","079705095aa14fa0bd5af53459ad5bfb","1082d64463a94f6282c1754720c702cd","137ca939b43b4bc7b41da9005e28b646","390ed406aa524d0c91e11ef74d01d700","21ba911bfeb2439aa0ca135c299de1ba","91ae4592c2d545bf9df31983257b6907","1e5ffbbd4d954281af45f8d0f2488de7","6751a852357447259e610430fdbb444b"]},"id":"LjY75GoYUCB8","outputId":"a8b657b0-e534-44a1-89a7-c7962e764ad2","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:18:27.582865Z","iopub.execute_input":"2025-11-10T11:18:27.583181Z","iopub.status.idle":"2025-11-10T11:18:27.853865Z","shell.execute_reply.started":"2025-11-10T11:18:27.583154Z","shell.execute_reply":"2025-11-10T11:18:27.852895Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/118 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ea39bd5620e4240810d5905db19871a"}},"metadata":{}}],"execution_count":34},{"cell_type":"markdown","source":"See result of the first row:","metadata":{"id":"YKA0VEF4CfCB"}},{"cell_type":"code","source":"dataset[0]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0usAI0M40hpT","outputId":"d9debc89-361b-4fb8-cf56-77acf55195cd","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:18:27.855600Z","iopub.execute_input":"2025-11-10T11:18:27.856078Z","iopub.status.idle":"2025-11-10T11:18:30.764819Z","shell.execute_reply.started":"2025-11-10T11:18:27.856057Z","shell.execute_reply":"2025-11-10T11:18:30.764020Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"{'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'},\n  {'content': 'What would you do to reduce the dimension of a high-dimensional data set on a limited memory machine?',\n   'role': 'user'},\n  {'content': 'You can either close all other applications on the machine or randomly sample the data set to create a smaller data set.',\n   'role': 'assistant'}],\n 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nYou are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat would you do to reduce the dimension of a high-dimensional data set on a limited memory machine?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nYou can either close all other applications on the machine or randomly sample the data set to create a smaller data set.<|eot_id|>'}"},"metadata":{}}],"execution_count":35},{"cell_type":"markdown","source":"<a name=\"Train\"></a>\n### Train the model\nNow let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`.","metadata":{"id":"idAEIeSQ3xdS"}},{"cell_type":"code","source":"from trl import SFTTrainer, SFTConfig\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    eval_dataset = None, # Can set up evaluation!\n    args = SFTConfig(\n        dataset_text_field = \"text\",\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n        warmup_steps = 5,\n        num_train_epochs = 3,\n        learning_rate = 2e-4,\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.001,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        report_to = \"none\", # Use TrackIO/WandB etc\n    ),\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["cfd9f69db23f4b86a42179ac0ac91a8c","e624589b592d43239e812e02fd3ab244","e0311985f098497e821f7c8720457384","7c05acca6aed46d49663afe10ebf6d9b","8d38463677324bd79c193798157a1568","b554950c61ae436dbb7772e7de173a81","ab2bdd97242243d7aadfe932c72c39a2","7be62d010b564972924fea1d8343398c","14b6b0568a074f8eb20cb57ffbcbb0f3","644e720f44d24d159133bfa515482054","50066adf2c2d4f6abe16456cb3cee737"]},"id":"95_Nn-89DhsL","outputId":"6ee189f7-91bf-4064-faf5-c6a8a96823f5","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:52:42.964300Z","iopub.execute_input":"2025-11-10T11:52:42.965039Z","iopub.status.idle":"2025-11-10T11:52:47.609791Z","shell.execute_reply.started":"2025-11-10T11:52:42.965012Z","shell.execute_reply":"2025-11-10T11:52:47.609110Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/118 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72ecbeb77c1741eab2d22e5741bf5fe8"}},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"# @title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"2ejIt2xSNKKp","outputId":"11f7102e-4aed-4389-aab4-35aa346c9df6","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:53:05.606296Z","iopub.execute_input":"2025-11-10T11:53:05.607029Z","iopub.status.idle":"2025-11-10T11:53:05.612789Z","shell.execute_reply.started":"2025-11-10T11:53:05.606998Z","shell.execute_reply":"2025-11-10T11:53:05.611996Z"}},"outputs":[{"name":"stdout","text":"GPU = Tesla T4. Max memory = 14.741 GB.\n3.07 GB of memory reserved.\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"yqxqAZ7KJ4oL","outputId":"1965bdc3-3caa-4d5d-8dab-aa93fc64fa39","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:53:05.999876Z","iopub.execute_input":"2025-11-10T11:53:06.000420Z","iopub.status.idle":"2025-11-10T11:54:25.932504Z","shell.execute_reply.started":"2025-11-10T11:53:06.000393Z","shell.execute_reply":"2025-11-10T11:54:25.931843Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 118 | Num Epochs = 3 | Total steps = 45\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n \"-____-\"     Trainable parameters = 24,313,856 of 3,237,063,680 (0.75% trained)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [45/45 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.849500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.988500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.004200</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.888000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.919500</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.006500</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.767700</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.901000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.819400</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.786500</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.822100</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.854100</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.759400</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.710100</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.815300</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.575900</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.554400</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.708600</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.580300</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.592800</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.497500</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.540800</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.587600</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.568900</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.431200</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.503500</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.494900</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.487400</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.623600</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.353100</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.401500</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.306900</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.361200</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.305500</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.309500</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.296400</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.310000</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.297800</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.350300</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.373400</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.362800</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.331800</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.431600</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.379300</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.367900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":61},{"cell_type":"code","source":"# @title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory / max_memory * 100, 3)\nlora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(\n    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n)\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"pCqnaKmlO1U9","outputId":"7d2e126d-42d9-4175-d909-31fe15a4193e","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:54:25.933477Z","iopub.execute_input":"2025-11-10T11:54:25.933668Z","iopub.status.idle":"2025-11-10T11:54:25.939628Z","shell.execute_reply.started":"2025-11-10T11:54:25.933654Z","shell.execute_reply":"2025-11-10T11:54:25.938835Z"}},"outputs":[{"name":"stdout","text":"77.1208 seconds used for training.\n1.29 minutes used for training.\nPeak reserved memory = 3.07 GB.\nPeak reserved memory for training = 0.0 GB.\nPeak reserved memory % of max memory = 20.826 %.\nPeak reserved memory for training % of max memory = 0.0 %.\n","output_type":"stream"}],"execution_count":62},{"cell_type":"markdown","source":"<a name=\"Inference\"></a>\n### Inference\nLet's run the model! Use `apply_chat_template` with `add_generation_prompt` set to `True` for inference.","metadata":{"id":"ekOmTR1hSNcr"}},{"cell_type":"code","source":"messages = [\n    {\"role\": \"user\", \"content\": \"What is the Byte Latent Transformer?\"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize = True,\n    add_generation_prompt = True, # Must add for generation\n    return_tensors = \"pt\",\n).to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer, skip_prompt = True)\n_ = model.generate(input_ids = inputs, streamer = text_streamer,\n                   max_new_tokens = 256, temperature = 0.1)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kR3gIAX-SM2q","outputId":"76e23150-16b3-4ef9-9cf8-8c494bc0f1ad","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:54:25.940734Z","iopub.execute_input":"2025-11-10T11:54:25.941044Z","iopub.status.idle":"2025-11-10T11:54:27.413768Z","shell.execute_reply.started":"2025-11-10T11:54:25.941020Z","shell.execute_reply":"2025-11-10T11:54:27.412959Z"}},"outputs":[{"name":"stdout","text":"The Byte Latent Transformer is a model that predicts the probability of a byte being a non-alphanumeric character.<|eot_id|>\n","output_type":"stream"}],"execution_count":63},{"cell_type":"markdown","source":"The model learns about the research paper!!","metadata":{"id":"bRrr--20Udm9"}},{"cell_type":"code","source":"messages = [\n    {\"role\": \"user\", \"content\": \"How should i prepare for an interview?\"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize = True,\n    add_generation_prompt = True, # Must add for generation\n    return_tensors = \"pt\",\n).to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer, skip_prompt = True)\n_ = model.generate(input_ids = inputs, streamer = text_streamer,\n                   max_new_tokens = 256, temperature = 0.1)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2strt31SUc5W","outputId":"3359dc6a-f533-4960-a875-400af205e20d","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:54:27.415435Z","iopub.execute_input":"2025-11-10T11:54:27.415653Z","iopub.status.idle":"2025-11-10T11:54:28.686451Z","shell.execute_reply.started":"2025-11-10T11:54:27.415637Z","shell.execute_reply":"2025-11-10T11:54:28.685909Z"}},"outputs":[{"name":"stdout","text":"Review the basics of your subject, practice common questions, and prepare questions to ask the interviewer.<|eot_id|>\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"messages = [\n    {\"role\": \"user\", \"content\": \" You are given a data set consisting of variables having more than 30% missing values? Let’s say, out of 50 variables, 8 variables have missing values higher than 30%. How will you deal with them?\"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize = True,\n    add_generation_prompt = True, # Must add for generation\n    return_tensors = \"pt\",\n).to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer, skip_prompt = True)\n_ = model.generate(input_ids = inputs, streamer = text_streamer,\n                   max_new_tokens = 256, temperature = 0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:54:28.687447Z","iopub.execute_input":"2025-11-10T11:54:28.687752Z","iopub.status.idle":"2025-11-10T11:54:29.671926Z","shell.execute_reply.started":"2025-11-10T11:54:28.687734Z","shell.execute_reply":"2025-11-10T11:54:29.671388Z"}},"outputs":[{"name":"stdout","text":"You can remove those variables as they are not helpful in any way.<|eot_id|>\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"messages = [\n    {\"role\": \"user\", \"content\": \"‘People who bought this, also bought...’ recommendations seen on amazon is a result of which algorithm?\"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize = True,\n    add_generation_prompt = True, # Must add for generation\n    return_tensors = \"pt\",\n).to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer, skip_prompt = True)\n_ = model.generate(input_ids = inputs, streamer = text_streamer,\n                   max_new_tokens = 256, temperature = 0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:54:29.672618Z","iopub.execute_input":"2025-11-10T11:54:29.672819Z","iopub.status.idle":"2025-11-10T11:54:30.076067Z","shell.execute_reply.started":"2025-11-10T11:54:29.672797Z","shell.execute_reply":"2025-11-10T11:54:30.075328Z"}},"outputs":[{"name":"stdout","text":"Collaborative Filtering<|eot_id|>\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"messages = [\n    {\"role\": \"user\", \"content\": \"What cross validation technique would you use on time series data set? Is it k-fold or LOOCV?\"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize = True,\n    add_generation_prompt = True, # Must add for generation\n    return_tensors = \"pt\",\n).to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer, skip_prompt = True)\n_ = model.generate(input_ids = inputs, streamer = text_streamer,\n                   max_new_tokens = 256, temperature = 0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:54:30.076987Z","iopub.execute_input":"2025-11-10T11:54:30.077423Z","iopub.status.idle":"2025-11-10T11:54:31.196462Z","shell.execute_reply.started":"2025-11-10T11:54:30.077394Z","shell.execute_reply":"2025-11-10T11:54:31.195866Z"}},"outputs":[{"name":"stdout","text":"Neither k-fold nor LOOCV can be used on time series data set.<|eot_id|>\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"messages = [\n    {\"role\": \"user\", \"content\": \"You’ve built a random forest model with 10000 trees. You got delighted after getting training error as 0.00. But, the validation error is 34.23. What is going on? Haven’t you trained your model perfectly?\"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize = True,\n    add_generation_prompt = True, # Must add for generation\n    return_tensors = \"pt\",\n).to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer, skip_prompt = True)\n_ = model.generate(input_ids = inputs, streamer = text_streamer,\n                   max_new_tokens = 512, temperature = 0.9)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:54:31.197226Z","iopub.execute_input":"2025-11-10T11:54:31.197473Z","iopub.status.idle":"2025-11-10T11:54:32.005669Z","shell.execute_reply.started":"2025-11-10T11:54:31.197455Z","shell.execute_reply":"2025-11-10T11:54:32.004871Z"}},"outputs":[{"name":"stdout","text":"Not at all. You have over-fit your model.<|eot_id|>\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"messages = [\n    {\"role\": \"user\", \"content\": \"How is True Positive Rate and Recall related? Write the equation.\"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize = True,\n    add_generation_prompt = True, # Must add for generation\n    return_tensors = \"pt\",\n).to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer, skip_prompt = True)\n_ = model.generate(input_ids = inputs, streamer = text_streamer,\n                   max_new_tokens = 1024, temperature = 0.9)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:54:32.006587Z","iopub.execute_input":"2025-11-10T11:54:32.006863Z","iopub.status.idle":"2025-11-10T11:54:32.768107Z","shell.execute_reply.started":"2025-11-10T11:54:32.006845Z","shell.execute_reply":"2025-11-10T11:54:32.767505Z"}},"outputs":[{"name":"stdout","text":"TPR = (Sensitivity * 100)<|eot_id|>\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"messages = [\n    {\"role\": \"system\", \"content\": \"You are a career coach providing detailed, actionable advice. Respond in a structured format with specific examples.\"},\n    {\"role\": \"user\", \"content\": \"How should I prepare for a technical interview for a machine learning engineer role? Provide a detailed step-by-step guide with examples of questions to practice, research strategies, and tips for success.\"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n).to(\"cuda\")\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer, skip_prompt=True)\n_ = model.generate(input_ids=inputs, streamer=text_streamer,\n                   max_new_tokens=1024, temperature=0.5, top_p=0.9)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:54:32.770289Z","iopub.execute_input":"2025-11-10T11:54:32.770573Z","iopub.status.idle":"2025-11-10T11:54:43.428592Z","shell.execute_reply.started":"2025-11-10T11:54:32.770555Z","shell.execute_reply":"2025-11-10T11:54:43.427935Z"}},"outputs":[{"name":"stdout","text":"Here's a structured plan to help you prepare:\n\n1.  Review the basics of machine learning and deep learning.\n2.  Learn about different algorithms, including their strengths, weaknesses, and applications.\n3.  Practice solving problems using online platforms such as LeetCode, HackerRank, or Kaggle.\n4.  Develop a portfolio of projects showcasing your skills in data science and machine learning.\n5.  Stay up-to-date with industry trends and advancements in machine learning.\n\nSome key topics to focus on include:\n\n*   Linear regression and its limitations\n*   Ridge regression and Lasso regression\n*   Gradient boosting and random forests\n*   Support vector machines (SVMs) and k-nearest neighbors (kNN)\n*   Convolutional neural networks (CNNs) and recurrent neural networks (RNNs)\n*   Natural language processing (NLP) and time series forecasting<|eot_id|>\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"messages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful Machine Learning Tutor helping me prepare to ace my Machine Learning Interview. Respond in a structured format with specific examples.\"},\n    {\"role\": \"user\", \"content\": \"what is categorical cross entropy?\"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n).to(\"cuda\")\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer, skip_prompt=True)\n_ = model.generate(input_ids=inputs, streamer=text_streamer,\n                   max_new_tokens=1024, temperature=0.5, top_p=0.9)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:54:43.429264Z","iopub.execute_input":"2025-11-10T11:54:43.429490Z","iopub.status.idle":"2025-11-10T11:54:44.678279Z","shell.execute_reply.started":"2025-11-10T11:54:43.429474Z","shell.execute_reply":"2025-11-10T11:54:44.677636Z"}},"outputs":[{"name":"stdout","text":"Categorical cross entropy is used to calculate the probability of selecting a particular class of items.<|eot_id|>\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"messages = [\n    {\"role\": \"system\", \"content\": \"You are a Machine Learning Tutor helping prepare for a machine learning engineer interview. Provide detailed, accurate explanations in a structured format (e.g., bullet points or numbered steps). Include mathematical formulations, practical examples (e.g., code or scenarios), and interview tips where relevant.\"},\n    {\"role\": \"user\", \"content\": \"Explain categorical cross entropy for a machine learning interview. Include its definition, mathematical formula, how it’s used in practice, and an example with code or a scenario.\"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n).to(\"cuda\")\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer, skip_prompt=True)\n_ = model.generate(input_ids=inputs, streamer=text_streamer,\n                   max_new_tokens=512, temperature=0.7, top_p=0.9)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:54:44.678994Z","iopub.execute_input":"2025-11-10T11:54:44.679213Z","iopub.status.idle":"2025-11-10T11:54:49.507893Z","shell.execute_reply.started":"2025-11-10T11:54:44.679189Z","shell.execute_reply":"2025-11-10T11:54:49.507308Z"}},"outputs":[{"name":"stdout","text":"Categorical cross-entropy is a measure of the difference between the predicted probabilities and the actual outcome in a classification problem. It’s defined as:\n\nLog likelihood of correct classification (y * log(p(y|X))) + (1-y) * log(1-p(y|X))\n\nwhere p(y|X) is the probability of the correct outcome given the data X, and y is the actual outcome.<|eot_id|>\n","output_type":"stream"}],"execution_count":72},{"cell_type":"markdown","source":"<a name=\"Save\"></a>\n### Saving, loading finetuned models\nTo save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n\n**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!","metadata":{"id":"uMuVrWbjAzhc"}},{"cell_type":"code","source":"model.save_pretrained(\"lora_model\")  # Local saving\ntokenizer.save_pretrained(\"lora_model\")\n# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"upcOlWe7A1vc","outputId":"b74c98e2-3909-46ff-f4c2-9ad838c7021b","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:56:05.496978Z","iopub.execute_input":"2025-11-10T11:56:05.497309Z","iopub.status.idle":"2025-11-10T11:56:06.505253Z","shell.execute_reply.started":"2025-11-10T11:56:05.497288Z","shell.execute_reply":"2025-11-10T11:56:06.504426Z"}},"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"('lora_model/tokenizer_config.json',\n 'lora_model/special_tokens_map.json',\n 'lora_model/chat_template.jinja',\n 'lora_model/tokenizer.json')"},"metadata":{}}],"execution_count":73},{"cell_type":"code","source":"!zip -r /kaggle/working/lora_model.zip /kaggle/working/lora_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:59:39.730273Z","iopub.execute_input":"2025-11-10T11:59:39.731133Z","iopub.status.idle":"2025-11-10T11:59:45.528566Z","shell.execute_reply.started":"2025-11-10T11:59:39.731111Z","shell.execute_reply":"2025-11-10T11:59:45.527745Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/lora_model/ (stored 0%)\n  adding: kaggle/working/lora_model/chat_template.jinja (deflated 71%)\n  adding: kaggle/working/lora_model/tokenizer_config.json (deflated 96%)\n  adding: kaggle/working/lora_model/tokenizer.json (deflated 85%)\n  adding: kaggle/working/lora_model/special_tokens_map.json (deflated 71%)\n  adding: kaggle/working/lora_model/adapter_model.safetensors (deflated 7%)\n  adding: kaggle/working/lora_model/README.md (deflated 65%)\n  adding: kaggle/working/lora_model/adapter_config.json (deflated 57%)\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"!zip -r /kaggle/working/lora_model_novten.zip /kaggle/working/lora_model\nprint(\"COMPLETED\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:02:59.009674Z","iopub.execute_input":"2025-11-10T12:02:59.010018Z","iopub.status.idle":"2025-11-10T12:03:04.727271Z","shell.execute_reply.started":"2025-11-10T12:02:59.009989Z","shell.execute_reply":"2025-11-10T12:03:04.726399Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/lora_model/ (stored 0%)\n  adding: kaggle/working/lora_model/chat_template.jinja (deflated 71%)\n  adding: kaggle/working/lora_model/tokenizer_config.json (deflated 96%)\n  adding: kaggle/working/lora_model/tokenizer.json (deflated 85%)\n  adding: kaggle/working/lora_model/special_tokens_map.json (deflated 71%)\n  adding: kaggle/working/lora_model/adapter_model.safetensors (deflated 7%)\n  adding: kaggle/working/lora_model/README.md (deflated 65%)\n  adding: kaggle/working/lora_model/adapter_config.json (deflated 57%)\nCOMPLETED\n","output_type":"stream"}],"execution_count":75},{"cell_type":"markdown","source":"Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:","metadata":{"id":"AEEcJ4qfC7Lp"}},{"cell_type":"code","source":"if False:\n    from unsloth import FastLanguageModel\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n        max_seq_length = max_seq_length,\n        dtype = dtype,\n        load_in_4bit = load_in_4bit,\n    )\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is so special about BLT's tokenization?\"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize = True,\n    add_generation_prompt = True, # Must add for generation\n    return_tensors = \"pt\",\n).to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer, skip_prompt = True)\n_ = model.generate(input_ids = inputs, streamer = text_streamer,\n                   max_new_tokens = 256, temperature = 0.1)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MKX_XKs_BNZR","outputId":"93cafd03-c9f7-4fdf-d031-f81e3497a839","trusted":true,"execution":{"execution_failed":"2025-11-10T12:12:35.733Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Saving to float16 for VLLM\n\nWe also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens.","metadata":{"id":"f422JgM9sdVT"}},{"cell_type":"code","source":"# Merge to 16bit\nif False:\n    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\nif False: # Change to True to upload finetune\n    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n\n# Merge to 4bit\nif False:\n    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\nif False: # Change to True to upload finetune\n    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n\n# Just LoRA adapters\nif False:\n    model.save_pretrained(\"model\")\n    tokenizer.save_pretrained(\"model\")\nif False: # Change to True to upload finetune\n    model.push_to_hub(\"hf/model\", token = \"\")\n    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n","metadata":{"id":"iHjt_SMYsd3P","trusted":true,"execution":{"execution_failed":"2025-11-10T12:12:35.733Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### GGUF / llama.cpp Conversion\nTo save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n\nSome supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.","metadata":{"id":"TCv4vXHd61i7"}},{"cell_type":"code","source":"# Save to 8bit Q8_0\nif False:\n    model.save_pretrained_gguf(\"model\", tokenizer,)\nif False: # Change to True to upload finetune\n    model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n\n# Save to 16bit GGUF\nif False:\n    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\nif False: # Change to True to upload finetune\n    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n\n# Save to q4_k_m GGUF\nif False:\n    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\nif False: # Change to True to upload finetune\n    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")","metadata":{"id":"FqfebeAdT073","trusted":true,"execution":{"execution_failed":"2025-11-10T12:12:35.733Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp.\n\nAnd we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n\nSome other links:\n1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n\n<div class=\"align-center\">\n  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n\n  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n\n  This notebook and all Unsloth notebooks are licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n</div>\n","metadata":{"id":"JKbf8poiMRml"}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}